{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('data/responses.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 documents\n",
      "2 classes ['goodbye', 'greeting']\n",
      "14 unique stemmed words ['anyon', 'ar', 'bye', 'day', 'good', 'goodby', 'hello', 'hi', 'how', 'is', 'lat', 'see', 'ther', 'you']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-9d149b3413ff>:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create our training data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: IM3Z05\n",
      "Log directory: tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 8\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.105s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 2  | total loss: \u001B[1m\u001B[32m0.62383\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 002 | loss: 0.62383 - acc: 0.3375 -- iter: 8/8\n",
      "--\n",
      "Training Step: 3  | total loss: \u001B[1m\u001B[32m0.68030\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 003 | loss: 0.68030 - acc: 0.5727 -- iter: 8/8\n",
      "--\n",
      "Training Step: 4  | total loss: \u001B[1m\u001B[32m0.68949\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 004 | loss: 0.68949 - acc: 0.6119 -- iter: 8/8\n",
      "--\n",
      "Training Step: 5  | total loss: \u001B[1m\u001B[32m0.69141\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 005 | loss: 0.69141 - acc: 0.6210 -- iter: 8/8\n",
      "--\n",
      "Training Step: 6  | total loss: \u001B[1m\u001B[32m0.69176\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 006 | loss: 0.69176 - acc: 0.6236 -- iter: 8/8\n",
      "--\n",
      "Training Step: 7  | total loss: \u001B[1m\u001B[32m0.69169\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 007 | loss: 0.69169 - acc: 0.6244 -- iter: 8/8\n",
      "--\n",
      "Training Step: 8  | total loss: \u001B[1m\u001B[32m0.69149\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 008 | loss: 0.69149 - acc: 0.6247 -- iter: 8/8\n",
      "--\n",
      "Training Step: 9  | total loss: \u001B[1m\u001B[32m0.69123\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 009 | loss: 0.69123 - acc: 0.6249 -- iter: 8/8\n",
      "--\n",
      "Training Step: 10  | total loss: \u001B[1m\u001B[32m0.69094\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 010 | loss: 0.69094 - acc: 0.6249 -- iter: 8/8\n",
      "--\n",
      "Training Step: 11  | total loss: \u001B[1m\u001B[32m0.69064\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 011 | loss: 0.69064 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 12  | total loss: \u001B[1m\u001B[32m0.69032\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 012 | loss: 0.69032 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 13  | total loss: \u001B[1m\u001B[32m0.69000\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 013 | loss: 0.69000 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 14  | total loss: \u001B[1m\u001B[32m0.68966\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 014 | loss: 0.68966 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 15  | total loss: \u001B[1m\u001B[32m0.68930\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 015 | loss: 0.68930 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 16  | total loss: \u001B[1m\u001B[32m0.68893\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 016 | loss: 0.68893 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 17  | total loss: \u001B[1m\u001B[32m0.68855\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 017 | loss: 0.68855 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 18  | total loss: \u001B[1m\u001B[32m0.68815\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 018 | loss: 0.68815 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 19  | total loss: \u001B[1m\u001B[32m0.68774\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 019 | loss: 0.68774 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 20  | total loss: \u001B[1m\u001B[32m0.68730\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 020 | loss: 0.68730 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 21  | total loss: \u001B[1m\u001B[32m0.68685\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 021 | loss: 0.68685 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 22  | total loss: \u001B[1m\u001B[32m0.68638\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 022 | loss: 0.68638 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 23  | total loss: \u001B[1m\u001B[32m0.68588\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 023 | loss: 0.68588 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 24  | total loss: \u001B[1m\u001B[32m0.68536\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 024 | loss: 0.68536 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 25  | total loss: \u001B[1m\u001B[32m0.68482\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 025 | loss: 0.68482 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 26  | total loss: \u001B[1m\u001B[32m0.68424\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 026 | loss: 0.68424 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 27  | total loss: \u001B[1m\u001B[32m0.68365\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 027 | loss: 0.68365 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 28  | total loss: \u001B[1m\u001B[32m0.68302\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 028 | loss: 0.68302 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 29  | total loss: \u001B[1m\u001B[32m0.68236\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 029 | loss: 0.68236 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 30  | total loss: \u001B[1m\u001B[32m0.68167\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 030 | loss: 0.68167 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 31  | total loss: \u001B[1m\u001B[32m0.68094\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 031 | loss: 0.68094 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 32  | total loss: \u001B[1m\u001B[32m0.68018\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 032 | loss: 0.68018 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 33  | total loss: \u001B[1m\u001B[32m0.67938\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 033 | loss: 0.67938 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 34  | total loss: \u001B[1m\u001B[32m0.67854\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 034 | loss: 0.67854 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 35  | total loss: \u001B[1m\u001B[32m0.67766\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 035 | loss: 0.67766 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 36  | total loss: \u001B[1m\u001B[32m0.67674\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 036 | loss: 0.67674 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 37  | total loss: \u001B[1m\u001B[32m0.67577\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 037 | loss: 0.67577 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 38  | total loss: \u001B[1m\u001B[32m0.67598\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 038 | loss: 0.67598 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 39  | total loss: \u001B[1m\u001B[32m0.67469\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 039 | loss: 0.67469 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 40  | total loss: \u001B[1m\u001B[32m0.67341\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 040 | loss: 0.67341 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 41  | total loss: \u001B[1m\u001B[32m0.67212\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 041 | loss: 0.67212 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 42  | total loss: \u001B[1m\u001B[32m0.67081\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 042 | loss: 0.67081 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 43  | total loss: \u001B[1m\u001B[32m0.66947\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 043 | loss: 0.66947 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 44  | total loss: \u001B[1m\u001B[32m0.66809\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 044 | loss: 0.66809 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 45  | total loss: \u001B[1m\u001B[32m0.66666\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 045 | loss: 0.66666 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 46  | total loss: \u001B[1m\u001B[32m0.66519\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 046 | loss: 0.66519 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 47  | total loss: \u001B[1m\u001B[32m0.66366\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 047 | loss: 0.66366 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 48  | total loss: \u001B[1m\u001B[32m0.66208\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 048 | loss: 0.66208 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 49  | total loss: \u001B[1m\u001B[32m0.66043\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 049 | loss: 0.66043 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 50  | total loss: \u001B[1m\u001B[32m0.65872\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 050 | loss: 0.65872 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 51  | total loss: \u001B[1m\u001B[32m0.65694\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 051 | loss: 0.65694 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 52  | total loss: \u001B[1m\u001B[32m0.65510\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 052 | loss: 0.65510 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 53  | total loss: \u001B[1m\u001B[32m0.65318\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 053 | loss: 0.65318 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 54  | total loss: \u001B[1m\u001B[32m0.65119\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 054 | loss: 0.65119 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 55  | total loss: \u001B[1m\u001B[32m0.64913\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 055 | loss: 0.64913 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 56  | total loss: \u001B[1m\u001B[32m0.64699\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 056 | loss: 0.64699 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 57  | total loss: \u001B[1m\u001B[32m0.64477\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 057 | loss: 0.64477 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 58  | total loss: \u001B[1m\u001B[32m0.64248\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 058 | loss: 0.64248 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 59  | total loss: \u001B[1m\u001B[32m0.64010\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 059 | loss: 0.64010 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 60  | total loss: \u001B[1m\u001B[32m0.63764\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 060 | loss: 0.63764 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 61  | total loss: \u001B[1m\u001B[32m0.63511\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 061 | loss: 0.63511 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 62  | total loss: \u001B[1m\u001B[32m0.63249\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 062 | loss: 0.63249 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 63  | total loss: \u001B[1m\u001B[32m0.62978\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 063 | loss: 0.62978 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 64  | total loss: \u001B[1m\u001B[32m0.63432\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 064 | loss: 0.63432 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 65  | total loss: \u001B[1m\u001B[32m0.63059\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 065 | loss: 0.63059 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 66  | total loss: \u001B[1m\u001B[32m0.62694\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 066 | loss: 0.62694 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 67  | total loss: \u001B[1m\u001B[32m0.62334\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 067 | loss: 0.62334 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 68  | total loss: \u001B[1m\u001B[32m0.61976\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 068 | loss: 0.61976 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 69  | total loss: \u001B[1m\u001B[32m0.61619\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 069 | loss: 0.61619 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 70  | total loss: \u001B[1m\u001B[32m0.61262\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 070 | loss: 0.61262 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 71  | total loss: \u001B[1m\u001B[32m0.60902\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 071 | loss: 0.60902 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 72  | total loss: \u001B[1m\u001B[32m0.60540\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 072 | loss: 0.60540 - acc: 0.6391 -- iter: 8/8\n",
      "--\n",
      "Training Step: 73  | total loss: \u001B[1m\u001B[32m0.60173\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 073 | loss: 0.60173 - acc: 0.6514 -- iter: 8/8\n",
      "--\n",
      "Training Step: 74  | total loss: \u001B[1m\u001B[32m0.59803\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 074 | loss: 0.59803 - acc: 0.6622 -- iter: 8/8\n",
      "--\n",
      "Training Step: 75  | total loss: \u001B[1m\u001B[32m0.59428\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 075 | loss: 0.59428 - acc: 0.6717 -- iter: 8/8\n",
      "--\n",
      "Training Step: 76  | total loss: \u001B[1m\u001B[32m0.59047\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 076 | loss: 0.59047 - acc: 0.6801 -- iter: 8/8\n",
      "--\n",
      "Training Step: 77  | total loss: \u001B[1m\u001B[32m0.58661\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 077 | loss: 0.58661 - acc: 0.6875 -- iter: 8/8\n",
      "--\n",
      "Training Step: 78  | total loss: \u001B[1m\u001B[32m0.58269\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 078 | loss: 0.58269 - acc: 0.6941 -- iter: 8/8\n",
      "--\n",
      "Training Step: 79  | total loss: \u001B[1m\u001B[32m0.57870\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 079 | loss: 0.57870 - acc: 0.6998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 80  | total loss: \u001B[1m\u001B[32m0.57466\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 080 | loss: 0.57466 - acc: 0.7050 -- iter: 8/8\n",
      "--\n",
      "Training Step: 81  | total loss: \u001B[1m\u001B[32m0.57056\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 081 | loss: 0.57056 - acc: 0.7095 -- iter: 8/8\n",
      "--\n",
      "Training Step: 82  | total loss: \u001B[1m\u001B[32m0.56639\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 082 | loss: 0.56639 - acc: 0.7136 -- iter: 8/8\n",
      "--\n",
      "Training Step: 83  | total loss: \u001B[1m\u001B[32m0.56212\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 083 | loss: 0.56212 - acc: 0.7172 -- iter: 8/8\n",
      "--\n",
      "Training Step: 84  | total loss: \u001B[1m\u001B[32m0.55774\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 084 | loss: 0.55774 - acc: 0.7205 -- iter: 8/8\n",
      "--\n",
      "Training Step: 85  | total loss: \u001B[1m\u001B[32m0.55325\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 085 | loss: 0.55325 - acc: 0.7234 -- iter: 8/8\n",
      "--\n",
      "Training Step: 86  | total loss: \u001B[1m\u001B[32m0.54867\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 086 | loss: 0.54867 - acc: 0.7261 -- iter: 8/8\n",
      "--\n",
      "Training Step: 87  | total loss: \u001B[1m\u001B[32m0.54399\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 087 | loss: 0.54399 - acc: 0.7285 -- iter: 8/8\n",
      "--\n",
      "Training Step: 88  | total loss: \u001B[1m\u001B[32m0.53921\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 088 | loss: 0.53921 - acc: 0.7306 -- iter: 8/8\n",
      "--\n",
      "Training Step: 89  | total loss: \u001B[1m\u001B[32m0.53434\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 089 | loss: 0.53434 - acc: 0.7326 -- iter: 8/8\n",
      "--\n",
      "Training Step: 90  | total loss: \u001B[1m\u001B[32m0.52938\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 090 | loss: 0.52938 - acc: 0.7343 -- iter: 8/8\n",
      "--\n",
      "Training Step: 91  | total loss: \u001B[1m\u001B[32m0.52434\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 091 | loss: 0.52434 - acc: 0.7359 -- iter: 8/8\n",
      "--\n",
      "Training Step: 92  | total loss: \u001B[1m\u001B[32m0.51920\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 092 | loss: 0.51920 - acc: 0.7373 -- iter: 8/8\n",
      "--\n",
      "Training Step: 93  | total loss: \u001B[1m\u001B[32m0.51399\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 093 | loss: 0.51399 - acc: 0.7386 -- iter: 8/8\n",
      "--\n",
      "Training Step: 94  | total loss: \u001B[1m\u001B[32m0.50869\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 094 | loss: 0.50869 - acc: 0.7397 -- iter: 8/8\n",
      "--\n",
      "Training Step: 95  | total loss: \u001B[1m\u001B[32m0.50331\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 095 | loss: 0.50331 - acc: 0.7407 -- iter: 8/8\n",
      "--\n",
      "Training Step: 96  | total loss: \u001B[1m\u001B[32m0.49785\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 096 | loss: 0.49785 - acc: 0.7417 -- iter: 8/8\n",
      "--\n",
      "Training Step: 97  | total loss: \u001B[1m\u001B[32m0.49232\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 097 | loss: 0.49232 - acc: 0.7425 -- iter: 8/8\n",
      "--\n",
      "Training Step: 98  | total loss: \u001B[1m\u001B[32m0.48671\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 098 | loss: 0.48671 - acc: 0.7433 -- iter: 8/8\n",
      "--\n",
      "Training Step: 99  | total loss: \u001B[1m\u001B[32m0.48103\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 099 | loss: 0.48103 - acc: 0.7439 -- iter: 8/8\n",
      "--\n",
      "Training Step: 100  | total loss: \u001B[1m\u001B[32m0.47528\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 100 | loss: 0.47528 - acc: 0.7445 -- iter: 8/8\n",
      "--\n",
      "Training Step: 101  | total loss: \u001B[1m\u001B[32m0.46947\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 101 | loss: 0.46947 - acc: 0.7451 -- iter: 8/8\n",
      "--\n",
      "Training Step: 102  | total loss: \u001B[1m\u001B[32m0.46359\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 102 | loss: 0.46359 - acc: 0.7456 -- iter: 8/8\n",
      "--\n",
      "Training Step: 103  | total loss: \u001B[1m\u001B[32m0.45765\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 103 | loss: 0.45765 - acc: 0.7460 -- iter: 8/8\n",
      "--\n",
      "Training Step: 104  | total loss: \u001B[1m\u001B[32m0.45165\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 104 | loss: 0.45165 - acc: 0.7464 -- iter: 8/8\n",
      "--\n",
      "Training Step: 105  | total loss: \u001B[1m\u001B[32m0.44560\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 105 | loss: 0.44560 - acc: 0.7468 -- iter: 8/8\n",
      "--\n",
      "Training Step: 106  | total loss: \u001B[1m\u001B[32m0.43951\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 106 | loss: 0.43951 - acc: 0.7471 -- iter: 8/8\n",
      "--\n",
      "Training Step: 107  | total loss: \u001B[1m\u001B[32m0.43336\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 107 | loss: 0.43336 - acc: 0.7474 -- iter: 8/8\n",
      "--\n",
      "Training Step: 108  | total loss: \u001B[1m\u001B[32m0.42718\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 108 | loss: 0.42718 - acc: 0.7476 -- iter: 8/8\n",
      "--\n",
      "Training Step: 109  | total loss: \u001B[1m\u001B[32m0.42096\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 109 | loss: 0.42096 - acc: 0.7479 -- iter: 8/8\n",
      "--\n",
      "Training Step: 110  | total loss: \u001B[1m\u001B[32m0.41470\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 110 | loss: 0.41470 - acc: 0.7481 -- iter: 8/8\n",
      "--\n",
      "Training Step: 111  | total loss: \u001B[1m\u001B[32m0.40842\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 111 | loss: 0.40842 - acc: 0.7483 -- iter: 8/8\n",
      "--\n",
      "Training Step: 112  | total loss: \u001B[1m\u001B[32m0.40212\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 112 | loss: 0.40212 - acc: 0.7610 -- iter: 8/8\n",
      "--\n",
      "Training Step: 113  | total loss: \u001B[1m\u001B[32m0.39579\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 113 | loss: 0.39579 - acc: 0.7849 -- iter: 8/8\n",
      "--\n",
      "Training Step: 114  | total loss: \u001B[1m\u001B[32m0.38946\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 114 | loss: 0.38946 - acc: 0.8064 -- iter: 8/8\n",
      "--\n",
      "Training Step: 115  | total loss: \u001B[1m\u001B[32m0.38311\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 115 | loss: 0.38311 - acc: 0.8257 -- iter: 8/8\n",
      "--\n",
      "Training Step: 116  | total loss: \u001B[1m\u001B[32m0.37675\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 116 | loss: 0.37675 - acc: 0.8432 -- iter: 8/8\n",
      "--\n",
      "Training Step: 117  | total loss: \u001B[1m\u001B[32m0.37040\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 117 | loss: 0.37040 - acc: 0.8588 -- iter: 8/8\n",
      "--\n",
      "Training Step: 118  | total loss: \u001B[1m\u001B[32m0.36405\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 118 | loss: 0.36405 - acc: 0.8730 -- iter: 8/8\n",
      "--\n",
      "Training Step: 119  | total loss: \u001B[1m\u001B[32m0.35770\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 119 | loss: 0.35770 - acc: 0.8857 -- iter: 8/8\n",
      "--\n",
      "Training Step: 120  | total loss: \u001B[1m\u001B[32m0.35137\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 120 | loss: 0.35137 - acc: 0.8971 -- iter: 8/8\n",
      "--\n",
      "Training Step: 121  | total loss: \u001B[1m\u001B[32m0.34506\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 121 | loss: 0.34506 - acc: 0.9074 -- iter: 8/8\n",
      "--\n",
      "Training Step: 122  | total loss: \u001B[1m\u001B[32m0.33876\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 122 | loss: 0.33876 - acc: 0.9167 -- iter: 8/8\n",
      "--\n",
      "Training Step: 123  | total loss: \u001B[1m\u001B[32m0.33248\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 123 | loss: 0.33248 - acc: 0.9250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 124  | total loss: \u001B[1m\u001B[32m0.32623\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 124 | loss: 0.32623 - acc: 0.9325 -- iter: 8/8\n",
      "--\n",
      "Training Step: 125  | total loss: \u001B[1m\u001B[32m0.32002\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 125 | loss: 0.32002 - acc: 0.9392 -- iter: 8/8\n",
      "--\n",
      "Training Step: 126  | total loss: \u001B[1m\u001B[32m0.31383\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 126 | loss: 0.31383 - acc: 0.9453 -- iter: 8/8\n",
      "--\n",
      "Training Step: 127  | total loss: \u001B[1m\u001B[32m0.30769\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 127 | loss: 0.30769 - acc: 0.9508 -- iter: 8/8\n",
      "--\n",
      "Training Step: 128  | total loss: \u001B[1m\u001B[32m0.30158\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 128 | loss: 0.30158 - acc: 0.9557 -- iter: 8/8\n",
      "--\n",
      "Training Step: 129  | total loss: \u001B[1m\u001B[32m0.29552\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 129 | loss: 0.29552 - acc: 0.9601 -- iter: 8/8\n",
      "--\n",
      "Training Step: 130  | total loss: \u001B[1m\u001B[32m0.28950\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 130 | loss: 0.28950 - acc: 0.9641 -- iter: 8/8\n",
      "--\n",
      "Training Step: 131  | total loss: \u001B[1m\u001B[32m0.28353\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 131 | loss: 0.28353 - acc: 0.9677 -- iter: 8/8\n",
      "--\n",
      "Training Step: 132  | total loss: \u001B[1m\u001B[32m0.27762\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 132 | loss: 0.27762 - acc: 0.9709 -- iter: 8/8\n",
      "--\n",
      "Training Step: 133  | total loss: \u001B[1m\u001B[32m0.27176\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 133 | loss: 0.27176 - acc: 0.9738 -- iter: 8/8\n",
      "--\n",
      "Training Step: 134  | total loss: \u001B[1m\u001B[32m0.26597\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 134 | loss: 0.26597 - acc: 0.9765 -- iter: 8/8\n",
      "--\n",
      "Training Step: 135  | total loss: \u001B[1m\u001B[32m0.26023\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 135 | loss: 0.26023 - acc: 0.9788 -- iter: 8/8\n",
      "--\n",
      "Training Step: 136  | total loss: \u001B[1m\u001B[32m0.25456\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 136 | loss: 0.25456 - acc: 0.9809 -- iter: 8/8\n",
      "--\n",
      "Training Step: 137  | total loss: \u001B[1m\u001B[32m0.24895\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 137 | loss: 0.24895 - acc: 0.9828 -- iter: 8/8\n",
      "--\n",
      "Training Step: 138  | total loss: \u001B[1m\u001B[32m0.24342\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 138 | loss: 0.24342 - acc: 0.9846 -- iter: 8/8\n",
      "--\n",
      "Training Step: 139  | total loss: \u001B[1m\u001B[32m0.23796\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 139 | loss: 0.23796 - acc: 0.9861 -- iter: 8/8\n",
      "--\n",
      "Training Step: 140  | total loss: \u001B[1m\u001B[32m0.23257\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 140 | loss: 0.23257 - acc: 0.9875 -- iter: 8/8\n",
      "--\n",
      "Training Step: 141  | total loss: \u001B[1m\u001B[32m0.22725\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 141 | loss: 0.22725 - acc: 0.9887 -- iter: 8/8\n",
      "--\n",
      "Training Step: 142  | total loss: \u001B[1m\u001B[32m0.33345\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 142 | loss: 0.33345 - acc: 0.9399 -- iter: 8/8\n",
      "--\n",
      "Training Step: 143  | total loss: \u001B[1m\u001B[32m0.31734\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 143 | loss: 0.31734 - acc: 0.9459 -- iter: 8/8\n",
      "--\n",
      "Training Step: 144  | total loss: \u001B[1m\u001B[32m0.28901\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 144 | loss: 0.28901 - acc: 0.9513 -- iter: 8/8\n",
      "--\n",
      "Training Step: 145  | total loss: \u001B[1m\u001B[32m0.28901\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 145 | loss: 0.28901 - acc: 0.9562 -- iter: 8/8\n",
      "--\n",
      "Training Step: 146  | total loss: \u001B[1m\u001B[32m0.27651\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 146 | loss: 0.27651 - acc: 0.9605 -- iter: 8/8\n",
      "--\n",
      "Training Step: 147  | total loss: \u001B[1m\u001B[32m0.26498\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 147 | loss: 0.26498 - acc: 0.9645 -- iter: 8/8\n",
      "--\n",
      "Training Step: 148  | total loss: \u001B[1m\u001B[32m0.25431\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 148 | loss: 0.25431 - acc: 0.9680 -- iter: 8/8\n",
      "--\n",
      "Training Step: 149  | total loss: \u001B[1m\u001B[32m0.24440\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 149 | loss: 0.24440 - acc: 0.9712 -- iter: 8/8\n",
      "--\n",
      "Training Step: 150  | total loss: \u001B[1m\u001B[32m0.23519\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 150 | loss: 0.23519 - acc: 0.9741 -- iter: 8/8\n",
      "--\n",
      "Training Step: 151  | total loss: \u001B[1m\u001B[32m0.22661\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 151 | loss: 0.22661 - acc: 0.9767 -- iter: 8/8\n",
      "--\n",
      "Training Step: 152  | total loss: \u001B[1m\u001B[32m0.21858\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 152 | loss: 0.21858 - acc: 0.9790 -- iter: 8/8\n",
      "--\n",
      "Training Step: 153  | total loss: \u001B[1m\u001B[32m0.21106\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 153 | loss: 0.21106 - acc: 0.9811 -- iter: 8/8\n",
      "--\n",
      "Training Step: 154  | total loss: \u001B[1m\u001B[32m0.20399\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 154 | loss: 0.20399 - acc: 0.9830 -- iter: 8/8\n",
      "--\n",
      "Training Step: 155  | total loss: \u001B[1m\u001B[32m0.19733\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 155 | loss: 0.19733 - acc: 0.9847 -- iter: 8/8\n",
      "--\n",
      "Training Step: 156  | total loss: \u001B[1m\u001B[32m0.19104\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 156 | loss: 0.19104 - acc: 0.9862 -- iter: 8/8\n",
      "--\n",
      "Training Step: 157  | total loss: \u001B[1m\u001B[32m0.18509\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 157 | loss: 0.18509 - acc: 0.9876 -- iter: 8/8\n",
      "--\n",
      "Training Step: 158  | total loss: \u001B[1m\u001B[32m0.17945\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 158 | loss: 0.17945 - acc: 0.9889 -- iter: 8/8\n",
      "--\n",
      "Training Step: 159  | total loss: \u001B[1m\u001B[32m0.17409\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 159 | loss: 0.17409 - acc: 0.9900 -- iter: 8/8\n",
      "--\n",
      "Training Step: 160  | total loss: \u001B[1m\u001B[32m0.16899\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 160 | loss: 0.16899 - acc: 0.9910 -- iter: 8/8\n",
      "--\n",
      "Training Step: 161  | total loss: \u001B[1m\u001B[32m0.16412\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 161 | loss: 0.16412 - acc: 0.9919 -- iter: 8/8\n",
      "--\n",
      "Training Step: 162  | total loss: \u001B[1m\u001B[32m0.15947\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 162 | loss: 0.15947 - acc: 0.9927 -- iter: 8/8\n",
      "--\n",
      "Training Step: 163  | total loss: \u001B[1m\u001B[32m0.15501\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 163 | loss: 0.15501 - acc: 0.9934 -- iter: 8/8\n",
      "--\n",
      "Training Step: 164  | total loss: \u001B[1m\u001B[32m0.15075\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 164 | loss: 0.15075 - acc: 0.9941 -- iter: 8/8\n",
      "--\n",
      "Training Step: 165  | total loss: \u001B[1m\u001B[32m0.14665\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 165 | loss: 0.14665 - acc: 0.9947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 166  | total loss: \u001B[1m\u001B[32m0.34211\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 166 | loss: 0.34211 - acc: 0.9202 -- iter: 8/8\n",
      "--\n",
      "Training Step: 167  | total loss: \u001B[1m\u001B[32m0.31858\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 167 | loss: 0.31858 - acc: 0.9282 -- iter: 8/8\n",
      "--\n",
      "Training Step: 168  | total loss: \u001B[1m\u001B[32m0.42217\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 168 | loss: 0.42217 - acc: 0.8854 -- iter: 8/8\n",
      "--\n",
      "Training Step: 169  | total loss: \u001B[1m\u001B[32m0.39060\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 169 | loss: 0.39060 - acc: 0.8968 -- iter: 8/8\n",
      "--\n",
      "Training Step: 170  | total loss: \u001B[1m\u001B[32m0.36220\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 170 | loss: 0.36220 - acc: 0.9071 -- iter: 8/8\n",
      "--\n",
      "Training Step: 171  | total loss: \u001B[1m\u001B[32m0.33663\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 171 | loss: 0.33663 - acc: 0.9164 -- iter: 8/8\n",
      "--\n",
      "Training Step: 172  | total loss: \u001B[1m\u001B[32m0.31360\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 172 | loss: 0.31360 - acc: 0.9248 -- iter: 8/8\n",
      "--\n",
      "Training Step: 173  | total loss: \u001B[1m\u001B[32m0.29282\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 173 | loss: 0.29282 - acc: 0.9323 -- iter: 8/8\n",
      "--\n",
      "Training Step: 174  | total loss: \u001B[1m\u001B[32m0.41816\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 174 | loss: 0.41816 - acc: 0.8891 -- iter: 8/8\n",
      "--\n",
      "Training Step: 175  | total loss: \u001B[1m\u001B[32m0.38690\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 175 | loss: 0.38690 - acc: 0.9002 -- iter: 8/8\n",
      "--\n",
      "Training Step: 176  | total loss: \u001B[1m\u001B[32m0.35879\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 176 | loss: 0.35879 - acc: 0.9102 -- iter: 8/8\n",
      "--\n",
      "Training Step: 177  | total loss: \u001B[1m\u001B[32m0.33348\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 177 | loss: 0.33348 - acc: 0.9191 -- iter: 8/8\n",
      "--\n",
      "Training Step: 178  | total loss: \u001B[1m\u001B[32m0.31068\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 178 | loss: 0.31068 - acc: 0.9272 -- iter: 8/8\n",
      "--\n",
      "Training Step: 179  | total loss: \u001B[1m\u001B[32m0.29012\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 179 | loss: 0.29012 - acc: 0.9345 -- iter: 8/8\n",
      "--\n",
      "Training Step: 180  | total loss: \u001B[1m\u001B[32m0.27157\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 180 | loss: 0.27157 - acc: 0.9411 -- iter: 8/8\n",
      "--\n",
      "Training Step: 181  | total loss: \u001B[1m\u001B[32m0.25481\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 181 | loss: 0.25481 - acc: 0.9469 -- iter: 8/8\n",
      "--\n",
      "Training Step: 182  | total loss: \u001B[1m\u001B[32m0.23965\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 182 | loss: 0.23965 - acc: 0.9523 -- iter: 8/8\n",
      "--\n",
      "Training Step: 183  | total loss: \u001B[1m\u001B[32m0.22593\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 183 | loss: 0.22593 - acc: 0.9570 -- iter: 8/8\n",
      "--\n",
      "Training Step: 184  | total loss: \u001B[1m\u001B[32m0.21348\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 184 | loss: 0.21348 - acc: 0.9613 -- iter: 8/8\n",
      "--\n",
      "Training Step: 185  | total loss: \u001B[1m\u001B[32m0.20217\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 185 | loss: 0.20217 - acc: 0.9652 -- iter: 8/8\n",
      "--\n",
      "Training Step: 186  | total loss: \u001B[1m\u001B[32m0.31373\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 186 | loss: 0.31373 - acc: 0.9187 -- iter: 8/8\n",
      "--\n",
      "Training Step: 187  | total loss: \u001B[1m\u001B[32m0.29228\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 187 | loss: 0.29228 - acc: 0.9268 -- iter: 8/8\n",
      "--\n",
      "Training Step: 188  | total loss: \u001B[1m\u001B[32m0.27294\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 188 | loss: 0.27294 - acc: 0.9341 -- iter: 8/8\n",
      "--\n",
      "Training Step: 189  | total loss: \u001B[1m\u001B[32m0.25549\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 189 | loss: 0.25549 - acc: 0.9407 -- iter: 8/8\n",
      "--\n",
      "Training Step: 190  | total loss: \u001B[1m\u001B[32m0.23973\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 190 | loss: 0.23973 - acc: 0.9466 -- iter: 8/8\n",
      "--\n",
      "Training Step: 191  | total loss: \u001B[1m\u001B[32m0.22548\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 191 | loss: 0.22548 - acc: 0.9520 -- iter: 8/8\n",
      "--\n",
      "Training Step: 192  | total loss: \u001B[1m\u001B[32m0.21259\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 192 | loss: 0.21259 - acc: 0.9568 -- iter: 8/8\n",
      "--\n",
      "Training Step: 193  | total loss: \u001B[1m\u001B[32m0.20089\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 193 | loss: 0.20089 - acc: 0.9611 -- iter: 8/8\n",
      "--\n",
      "Training Step: 194  | total loss: \u001B[1m\u001B[32m0.19028\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 194 | loss: 0.19028 - acc: 0.9650 -- iter: 8/8\n",
      "--\n",
      "Training Step: 195  | total loss: \u001B[1m\u001B[32m0.18063\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 195 | loss: 0.18063 - acc: 0.9685 -- iter: 8/8\n",
      "--\n",
      "Training Step: 196  | total loss: \u001B[1m\u001B[32m0.17184\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 196 | loss: 0.17184 - acc: 0.9716 -- iter: 8/8\n",
      "--\n",
      "Training Step: 197  | total loss: \u001B[1m\u001B[32m0.16383\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 197 | loss: 0.16383 - acc: 0.9745 -- iter: 8/8\n",
      "--\n",
      "Training Step: 198  | total loss: \u001B[1m\u001B[32m0.15651\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 198 | loss: 0.15651 - acc: 0.9770 -- iter: 8/8\n",
      "--\n",
      "Training Step: 199  | total loss: \u001B[1m\u001B[32m0.14980\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 199 | loss: 0.14980 - acc: 0.9793 -- iter: 8/8\n",
      "--\n",
      "Training Step: 200  | total loss: \u001B[1m\u001B[32m0.14365\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 200 | loss: 0.14365 - acc: 0.9814 -- iter: 8/8\n",
      "--\n",
      "Training Step: 201  | total loss: \u001B[1m\u001B[32m0.13799\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 201 | loss: 0.13799 - acc: 0.9833 -- iter: 8/8\n",
      "--\n",
      "Training Step: 202  | total loss: \u001B[1m\u001B[32m0.13278\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 202 | loss: 0.13278 - acc: 0.9849 -- iter: 8/8\n",
      "--\n",
      "Training Step: 203  | total loss: \u001B[1m\u001B[32m0.12797\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 203 | loss: 0.12797 - acc: 0.9864 -- iter: 8/8\n",
      "--\n",
      "Training Step: 204  | total loss: \u001B[1m\u001B[32m0.12352\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 204 | loss: 0.12352 - acc: 0.9878 -- iter: 8/8\n",
      "--\n",
      "Training Step: 205  | total loss: \u001B[1m\u001B[32m0.11939\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 205 | loss: 0.11939 - acc: 0.9890 -- iter: 8/8\n",
      "--\n",
      "Training Step: 206  | total loss: \u001B[1m\u001B[32m0.11555\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 206 | loss: 0.11555 - acc: 0.9901 -- iter: 8/8\n",
      "--\n",
      "Training Step: 207  | total loss: \u001B[1m\u001B[32m0.11197\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 207 | loss: 0.11197 - acc: 0.9911 -- iter: 8/8\n",
      "--\n",
      "Training Step: 208  | total loss: \u001B[1m\u001B[32m0.10862\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 208 | loss: 0.10862 - acc: 0.9920 -- iter: 8/8\n",
      "--\n",
      "Training Step: 209  | total loss: \u001B[1m\u001B[32m0.10549\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 209 | loss: 0.10549 - acc: 0.9928 -- iter: 8/8\n",
      "--\n",
      "Training Step: 210  | total loss: \u001B[1m\u001B[32m0.10255\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 210 | loss: 0.10255 - acc: 0.9935 -- iter: 8/8\n",
      "--\n",
      "Training Step: 211  | total loss: \u001B[1m\u001B[32m0.09978\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 211 | loss: 0.09978 - acc: 0.9942 -- iter: 8/8\n",
      "--\n",
      "Training Step: 212  | total loss: \u001B[1m\u001B[32m0.09717\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 212 | loss: 0.09717 - acc: 0.9947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 213  | total loss: \u001B[1m\u001B[32m0.09471\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 213 | loss: 0.09471 - acc: 0.9953 -- iter: 8/8\n",
      "--\n",
      "Training Step: 214  | total loss: \u001B[1m\u001B[32m0.09237\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 214 | loss: 0.09237 - acc: 0.9957 -- iter: 8/8\n",
      "--\n",
      "Training Step: 215  | total loss: \u001B[1m\u001B[32m0.09015\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 215 | loss: 0.09015 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 216  | total loss: \u001B[1m\u001B[32m0.08804\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 216 | loss: 0.08804 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 217  | total loss: \u001B[1m\u001B[32m0.08603\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 217 | loss: 0.08603 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 218  | total loss: \u001B[1m\u001B[32m0.08411\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 218 | loss: 0.08411 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 219  | total loss: \u001B[1m\u001B[32m0.08227\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 219 | loss: 0.08227 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 220  | total loss: \u001B[1m\u001B[32m0.08051\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 220 | loss: 0.08051 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 221  | total loss: \u001B[1m\u001B[32m0.07882\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 221 | loss: 0.07882 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 222  | total loss: \u001B[1m\u001B[32m0.07719\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 222 | loss: 0.07719 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 223  | total loss: \u001B[1m\u001B[32m0.07563\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 223 | loss: 0.07563 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 224  | total loss: \u001B[1m\u001B[32m0.07412\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 224 | loss: 0.07412 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 225  | total loss: \u001B[1m\u001B[32m0.07266\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 225 | loss: 0.07266 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 226  | total loss: \u001B[1m\u001B[32m0.07125\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 226 | loss: 0.07125 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 227  | total loss: \u001B[1m\u001B[32m0.06989\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 227 | loss: 0.06989 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 228  | total loss: \u001B[1m\u001B[32m0.06857\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 228 | loss: 0.06857 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 229  | total loss: \u001B[1m\u001B[32m0.06730\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 229 | loss: 0.06730 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 230  | total loss: \u001B[1m\u001B[32m0.06606\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 230 | loss: 0.06606 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 231  | total loss: \u001B[1m\u001B[32m0.06485\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 231 | loss: 0.06485 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 232  | total loss: \u001B[1m\u001B[32m0.06368\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 232 | loss: 0.06368 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 233  | total loss: \u001B[1m\u001B[32m0.06255\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 233 | loss: 0.06255 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 234  | total loss: \u001B[1m\u001B[32m0.06144\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 234 | loss: 0.06144 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 235  | total loss: \u001B[1m\u001B[32m0.06036\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 235 | loss: 0.06036 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 236  | total loss: \u001B[1m\u001B[32m0.05932\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 236 | loss: 0.05932 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 237  | total loss: \u001B[1m\u001B[32m0.05829\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 237 | loss: 0.05829 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 238  | total loss: \u001B[1m\u001B[32m0.05730\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 238 | loss: 0.05730 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 239  | total loss: \u001B[1m\u001B[32m0.05633\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 239 | loss: 0.05633 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 240  | total loss: \u001B[1m\u001B[32m0.05538\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 240 | loss: 0.05538 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 241  | total loss: \u001B[1m\u001B[32m0.05446\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 241 | loss: 0.05446 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 242  | total loss: \u001B[1m\u001B[32m0.05355\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 242 | loss: 0.05355 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 243  | total loss: \u001B[1m\u001B[32m0.05267\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 243 | loss: 0.05267 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 244  | total loss: \u001B[1m\u001B[32m0.05181\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 244 | loss: 0.05181 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 245  | total loss: \u001B[1m\u001B[32m0.05097\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 245 | loss: 0.05097 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 246  | total loss: \u001B[1m\u001B[32m0.05015\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 246 | loss: 0.05015 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 247  | total loss: \u001B[1m\u001B[32m0.04935\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 247 | loss: 0.04935 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 248  | total loss: \u001B[1m\u001B[32m0.04856\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 248 | loss: 0.04856 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 249  | total loss: \u001B[1m\u001B[32m0.04779\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 249 | loss: 0.04779 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 250  | total loss: \u001B[1m\u001B[32m0.04704\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 250 | loss: 0.04704 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 251  | total loss: \u001B[1m\u001B[32m0.04631\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 251 | loss: 0.04631 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 252  | total loss: \u001B[1m\u001B[32m0.04559\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 252 | loss: 0.04559 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 253  | total loss: \u001B[1m\u001B[32m0.04489\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 253 | loss: 0.04489 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 254  | total loss: \u001B[1m\u001B[32m0.04420\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 254 | loss: 0.04420 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 255  | total loss: \u001B[1m\u001B[32m0.04353\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 255 | loss: 0.04353 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 256  | total loss: \u001B[1m\u001B[32m0.04287\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 256 | loss: 0.04287 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 257  | total loss: \u001B[1m\u001B[32m0.04222\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 257 | loss: 0.04222 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 258  | total loss: \u001B[1m\u001B[32m0.11105\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 258 | loss: 0.11105 - acc: 0.9750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 259  | total loss: \u001B[1m\u001B[32m0.10351\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 259 | loss: 0.10351 - acc: 0.9775 -- iter: 8/8\n",
      "--\n",
      "Training Step: 260  | total loss: \u001B[1m\u001B[32m0.09671\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 260 | loss: 0.09671 - acc: 0.9797 -- iter: 8/8\n",
      "--\n",
      "Training Step: 261  | total loss: \u001B[1m\u001B[32m0.09057\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 261 | loss: 0.09057 - acc: 0.9817 -- iter: 8/8\n",
      "--\n",
      "Training Step: 262  | total loss: \u001B[1m\u001B[32m0.08501\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 262 | loss: 0.08501 - acc: 0.9836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 263  | total loss: \u001B[1m\u001B[32m0.07998\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 263 | loss: 0.07998 - acc: 0.9852 -- iter: 8/8\n",
      "--\n",
      "Training Step: 264  | total loss: \u001B[1m\u001B[32m0.07543\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 264 | loss: 0.07543 - acc: 0.9867 -- iter: 8/8\n",
      "--\n",
      "Training Step: 265  | total loss: \u001B[1m\u001B[32m0.07130\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 265 | loss: 0.07130 - acc: 0.9880 -- iter: 8/8\n",
      "--\n",
      "Training Step: 266  | total loss: \u001B[1m\u001B[32m0.06755\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 266 | loss: 0.06755 - acc: 0.9892 -- iter: 8/8\n",
      "--\n",
      "Training Step: 267  | total loss: \u001B[1m\u001B[32m0.06415\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 267 | loss: 0.06415 - acc: 0.9903 -- iter: 8/8\n",
      "--\n",
      "Training Step: 268  | total loss: \u001B[1m\u001B[32m0.06105\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 268 | loss: 0.06105 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 269  | total loss: \u001B[1m\u001B[32m0.05823\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 269 | loss: 0.05823 - acc: 0.9921 -- iter: 8/8\n",
      "--\n",
      "Training Step: 270  | total loss: \u001B[1m\u001B[32m0.05566\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 270 | loss: 0.05566 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 271  | total loss: \u001B[1m\u001B[32m0.05331\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 271 | loss: 0.05331 - acc: 0.9936 -- iter: 8/8\n",
      "--\n",
      "Training Step: 272  | total loss: \u001B[1m\u001B[32m0.05117\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 272 | loss: 0.05117 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 273  | total loss: \u001B[1m\u001B[32m0.04920\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 273 | loss: 0.04920 - acc: 0.9948 -- iter: 8/8\n",
      "--\n",
      "Training Step: 274  | total loss: \u001B[1m\u001B[32m0.04740\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 274 | loss: 0.04740 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 275  | total loss: \u001B[1m\u001B[32m0.04574\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 275 | loss: 0.04574 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 276  | total loss: \u001B[1m\u001B[32m0.04421\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 276 | loss: 0.04421 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 277  | total loss: \u001B[1m\u001B[32m0.04281\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 277 | loss: 0.04281 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 278  | total loss: \u001B[1m\u001B[32m0.04151\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 278 | loss: 0.04151 - acc: 0.9970 -- iter: 8/8\n",
      "--\n",
      "Training Step: 279  | total loss: \u001B[1m\u001B[32m0.04030\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 279 | loss: 0.04030 - acc: 0.9973 -- iter: 8/8\n",
      "--\n",
      "Training Step: 280  | total loss: \u001B[1m\u001B[32m0.11460\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 280 | loss: 0.11460 - acc: 0.9725 -- iter: 8/8\n",
      "--\n",
      "Training Step: 281  | total loss: \u001B[1m\u001B[32m0.10604\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 281 | loss: 0.10604 - acc: 0.9753 -- iter: 8/8\n",
      "--\n",
      "Training Step: 282  | total loss: \u001B[1m\u001B[32m0.09834\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 282 | loss: 0.09834 - acc: 0.9778 -- iter: 8/8\n",
      "--\n",
      "Training Step: 283  | total loss: \u001B[1m\u001B[32m0.09139\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 283 | loss: 0.09139 - acc: 0.9800 -- iter: 8/8\n",
      "--\n",
      "Training Step: 284  | total loss: \u001B[1m\u001B[32m0.20437\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 284 | loss: 0.20437 - acc: 0.9570 -- iter: 8/8\n",
      "--\n",
      "Training Step: 285  | total loss: \u001B[1m\u001B[32m0.18683\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 285 | loss: 0.18683 - acc: 0.9613 -- iter: 8/8\n",
      "--\n",
      "Training Step: 286  | total loss: \u001B[1m\u001B[32m0.17105\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 286 | loss: 0.17105 - acc: 0.9652 -- iter: 8/8\n",
      "--\n",
      "Training Step: 287  | total loss: \u001B[1m\u001B[32m0.15686\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 287 | loss: 0.15686 - acc: 0.9686 -- iter: 8/8\n",
      "--\n",
      "Training Step: 288  | total loss: \u001B[1m\u001B[32m0.14409\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 288 | loss: 0.14409 - acc: 0.9718 -- iter: 8/8\n",
      "--\n",
      "Training Step: 289  | total loss: \u001B[1m\u001B[32m0.13260\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 289 | loss: 0.13260 - acc: 0.9746 -- iter: 8/8\n",
      "--\n",
      "Training Step: 290  | total loss: \u001B[1m\u001B[32m0.12225\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 290 | loss: 0.12225 - acc: 0.9771 -- iter: 8/8\n",
      "--\n",
      "Training Step: 291  | total loss: \u001B[1m\u001B[32m0.11294\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 291 | loss: 0.11294 - acc: 0.9794 -- iter: 8/8\n",
      "--\n",
      "Training Step: 292  | total loss: \u001B[1m\u001B[32m0.10455\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 292 | loss: 0.10455 - acc: 0.9815 -- iter: 8/8\n",
      "--\n",
      "Training Step: 293  | total loss: \u001B[1m\u001B[32m0.09698\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 293 | loss: 0.09698 - acc: 0.9833 -- iter: 8/8\n",
      "--\n",
      "Training Step: 294  | total loss: \u001B[1m\u001B[32m0.09016\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 294 | loss: 0.09016 - acc: 0.9850 -- iter: 8/8\n",
      "--\n",
      "Training Step: 295  | total loss: \u001B[1m\u001B[32m0.08401\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 295 | loss: 0.08401 - acc: 0.9865 -- iter: 8/8\n",
      "--\n",
      "Training Step: 296  | total loss: \u001B[1m\u001B[32m0.07846\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 296 | loss: 0.07846 - acc: 0.9878 -- iter: 8/8\n",
      "--\n",
      "Training Step: 297  | total loss: \u001B[1m\u001B[32m0.07344\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 297 | loss: 0.07344 - acc: 0.9891 -- iter: 8/8\n",
      "--\n",
      "Training Step: 298  | total loss: \u001B[1m\u001B[32m0.06891\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 298 | loss: 0.06891 - acc: 0.9902 -- iter: 8/8\n",
      "--\n",
      "Training Step: 299  | total loss: \u001B[1m\u001B[32m0.06481\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 299 | loss: 0.06481 - acc: 0.9911 -- iter: 8/8\n",
      "--\n",
      "Training Step: 300  | total loss: \u001B[1m\u001B[32m0.06110\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 300 | loss: 0.06110 - acc: 0.9920 -- iter: 8/8\n",
      "--\n",
      "Training Step: 301  | total loss: \u001B[1m\u001B[32m0.05774\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 301 | loss: 0.05774 - acc: 0.9928 -- iter: 8/8\n",
      "--\n",
      "Training Step: 302  | total loss: \u001B[1m\u001B[32m0.05470\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 302 | loss: 0.05470 - acc: 0.9935 -- iter: 8/8\n",
      "--\n",
      "Training Step: 303  | total loss: \u001B[1m\u001B[32m0.05193\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 303 | loss: 0.05193 - acc: 0.9942 -- iter: 8/8\n",
      "--\n",
      "Training Step: 304  | total loss: \u001B[1m\u001B[32m0.04942\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 304 | loss: 0.04942 - acc: 0.9948 -- iter: 8/8\n",
      "--\n",
      "Training Step: 305  | total loss: \u001B[1m\u001B[32m0.04713\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 305 | loss: 0.04713 - acc: 0.9953 -- iter: 8/8\n",
      "--\n",
      "Training Step: 306  | total loss: \u001B[1m\u001B[32m0.04505\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 306 | loss: 0.04505 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 307  | total loss: \u001B[1m\u001B[32m0.04315\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 307 | loss: 0.04315 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 308  | total loss: \u001B[1m\u001B[32m0.04142\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 308 | loss: 0.04142 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 309  | total loss: \u001B[1m\u001B[32m0.03984\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 309 | loss: 0.03984 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 310  | total loss: \u001B[1m\u001B[32m0.03839\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 310 | loss: 0.03839 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 311  | total loss: \u001B[1m\u001B[32m0.03705\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 311 | loss: 0.03705 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 312  | total loss: \u001B[1m\u001B[32m0.03583\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 312 | loss: 0.03583 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 313  | total loss: \u001B[1m\u001B[32m0.03471\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 313 | loss: 0.03471 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 314  | total loss: \u001B[1m\u001B[32m0.03367\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 314 | loss: 0.03367 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 315  | total loss: \u001B[1m\u001B[32m0.03271\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 315 | loss: 0.03271 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 316  | total loss: \u001B[1m\u001B[32m0.03182\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 316 | loss: 0.03182 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 317  | total loss: \u001B[1m\u001B[32m0.03100\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 317 | loss: 0.03100 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 318  | total loss: \u001B[1m\u001B[32m0.03023\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 318 | loss: 0.03023 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 319  | total loss: \u001B[1m\u001B[32m0.02952\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 319 | loss: 0.02952 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 320  | total loss: \u001B[1m\u001B[32m0.02885\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 320 | loss: 0.02885 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 321  | total loss: \u001B[1m\u001B[32m0.02823\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 321 | loss: 0.02823 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 322  | total loss: \u001B[1m\u001B[32m0.21304\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 322 | loss: 0.21304 - acc: 0.9492 -- iter: 8/8\n",
      "--\n",
      "Training Step: 323  | total loss: \u001B[1m\u001B[32m0.19400\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 323 | loss: 0.19400 - acc: 0.9543 -- iter: 8/8\n",
      "--\n",
      "Training Step: 324  | total loss: \u001B[1m\u001B[32m0.17689\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 324 | loss: 0.17689 - acc: 0.9589 -- iter: 8/8\n",
      "--\n",
      "Training Step: 325  | total loss: \u001B[1m\u001B[32m0.16150\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 325 | loss: 0.16150 - acc: 0.9630 -- iter: 8/8\n",
      "--\n",
      "Training Step: 326  | total loss: \u001B[1m\u001B[32m0.14767\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 326 | loss: 0.14767 - acc: 0.9667 -- iter: 8/8\n",
      "--\n",
      "Training Step: 327  | total loss: \u001B[1m\u001B[32m0.13523\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 327 | loss: 0.13523 - acc: 0.9700 -- iter: 8/8\n",
      "--\n",
      "Training Step: 328  | total loss: \u001B[1m\u001B[32m0.12404\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 328 | loss: 0.12404 - acc: 0.9730 -- iter: 8/8\n",
      "--\n",
      "Training Step: 329  | total loss: \u001B[1m\u001B[32m0.11398\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 329 | loss: 0.11398 - acc: 0.9757 -- iter: 8/8\n",
      "--\n",
      "Training Step: 330  | total loss: \u001B[1m\u001B[32m0.10492\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 330 | loss: 0.10492 - acc: 0.9781 -- iter: 8/8\n",
      "--\n",
      "Training Step: 331  | total loss: \u001B[1m\u001B[32m0.09677\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 331 | loss: 0.09677 - acc: 0.9803 -- iter: 8/8\n",
      "--\n",
      "Training Step: 332  | total loss: \u001B[1m\u001B[32m0.37095\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 332 | loss: 0.37095 - acc: 0.9073 -- iter: 8/8\n",
      "--\n",
      "Training Step: 333  | total loss: \u001B[1m\u001B[32m0.33625\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 333 | loss: 0.33625 - acc: 0.9166 -- iter: 8/8\n",
      "--\n",
      "Training Step: 334  | total loss: \u001B[1m\u001B[32m0.30508\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 334 | loss: 0.30508 - acc: 0.9249 -- iter: 8/8\n",
      "--\n",
      "Training Step: 335  | total loss: \u001B[1m\u001B[32m0.27708\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 335 | loss: 0.27708 - acc: 0.9324 -- iter: 8/8\n",
      "--\n",
      "Training Step: 336  | total loss: \u001B[1m\u001B[32m0.25192\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 336 | loss: 0.25192 - acc: 0.9392 -- iter: 8/8\n",
      "--\n",
      "Training Step: 337  | total loss: \u001B[1m\u001B[32m0.22932\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 337 | loss: 0.22932 - acc: 0.9453 -- iter: 8/8\n",
      "--\n",
      "Training Step: 338  | total loss: \u001B[1m\u001B[32m0.20900\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 338 | loss: 0.20900 - acc: 0.9507 -- iter: 8/8\n",
      "--\n",
      "Training Step: 339  | total loss: \u001B[1m\u001B[32m0.19075\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 339 | loss: 0.19075 - acc: 0.9557 -- iter: 8/8\n",
      "--\n",
      "Training Step: 340  | total loss: \u001B[1m\u001B[32m0.17434\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 340 | loss: 0.17434 - acc: 0.9601 -- iter: 8/8\n",
      "--\n",
      "Training Step: 341  | total loss: \u001B[1m\u001B[32m0.15959\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 341 | loss: 0.15959 - acc: 0.9641 -- iter: 8/8\n",
      "--\n",
      "Training Step: 342  | total loss: \u001B[1m\u001B[32m0.14633\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 342 | loss: 0.14633 - acc: 0.9677 -- iter: 8/8\n",
      "--\n",
      "Training Step: 343  | total loss: \u001B[1m\u001B[32m0.13440\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 343 | loss: 0.13440 - acc: 0.9709 -- iter: 8/8\n",
      "--\n",
      "Training Step: 344  | total loss: \u001B[1m\u001B[32m0.12367\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 344 | loss: 0.12367 - acc: 0.9738 -- iter: 8/8\n",
      "--\n",
      "Training Step: 345  | total loss: \u001B[1m\u001B[32m0.11402\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 345 | loss: 0.11402 - acc: 0.9764 -- iter: 8/8\n",
      "--\n",
      "Training Step: 346  | total loss: \u001B[1m\u001B[32m0.10533\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 346 | loss: 0.10533 - acc: 0.9788 -- iter: 8/8\n",
      "--\n",
      "Training Step: 347  | total loss: \u001B[1m\u001B[32m0.09750\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 347 | loss: 0.09750 - acc: 0.9809 -- iter: 8/8\n",
      "--\n",
      "Training Step: 348  | total loss: \u001B[1m\u001B[32m0.09046\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 348 | loss: 0.09046 - acc: 0.9828 -- iter: 8/8\n",
      "--\n",
      "Training Step: 349  | total loss: \u001B[1m\u001B[32m0.08411\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 349 | loss: 0.08411 - acc: 0.9845 -- iter: 8/8\n",
      "--\n",
      "Training Step: 350  | total loss: \u001B[1m\u001B[32m0.07839\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 350 | loss: 0.07839 - acc: 0.9861 -- iter: 8/8\n",
      "--\n",
      "Training Step: 351  | total loss: \u001B[1m\u001B[32m0.07323\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 351 | loss: 0.07323 - acc: 0.9875 -- iter: 8/8\n",
      "--\n",
      "Training Step: 352  | total loss: \u001B[1m\u001B[32m0.24127\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 352 | loss: 0.24127 - acc: 0.9387 -- iter: 8/8\n",
      "--\n",
      "Training Step: 353  | total loss: \u001B[1m\u001B[32m0.21984\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 353 | loss: 0.21984 - acc: 0.9449 -- iter: 8/8\n",
      "--\n",
      "Training Step: 354  | total loss: \u001B[1m\u001B[32m0.20059\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 354 | loss: 0.20059 - acc: 0.9504 -- iter: 8/8\n",
      "--\n",
      "Training Step: 355  | total loss: \u001B[1m\u001B[32m0.18328\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 355 | loss: 0.18328 - acc: 0.9553 -- iter: 8/8\n",
      "--\n",
      "Training Step: 356  | total loss: \u001B[1m\u001B[32m0.16772\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 356 | loss: 0.16772 - acc: 0.9598 -- iter: 8/8\n",
      "--\n",
      "Training Step: 357  | total loss: \u001B[1m\u001B[32m0.15373\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 357 | loss: 0.15373 - acc: 0.9638 -- iter: 8/8\n",
      "--\n",
      "Training Step: 358  | total loss: \u001B[1m\u001B[32m0.14115\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 358 | loss: 0.14115 - acc: 0.9674 -- iter: 8/8\n",
      "--\n",
      "Training Step: 359  | total loss: \u001B[1m\u001B[32m0.12984\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 359 | loss: 0.12984 - acc: 0.9707 -- iter: 8/8\n",
      "--\n",
      "Training Step: 360  | total loss: \u001B[1m\u001B[32m0.11966\u001B[0m\u001B[0m | time: 0.012s\n",
      "| Adam | epoch: 360 | loss: 0.11966 - acc: 0.9736 -- iter: 8/8\n",
      "--\n",
      "Training Step: 361  | total loss: \u001B[1m\u001B[32m0.11050\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 361 | loss: 0.11050 - acc: 0.9763 -- iter: 8/8\n",
      "--\n",
      "Training Step: 362  | total loss: \u001B[1m\u001B[32m0.28839\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 362 | loss: 0.28839 - acc: 0.9286 -- iter: 8/8\n",
      "--\n",
      "Training Step: 363  | total loss: \u001B[1m\u001B[32m0.26240\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 363 | loss: 0.26240 - acc: 0.9358 -- iter: 8/8\n",
      "--\n",
      "Training Step: 364  | total loss: \u001B[1m\u001B[32m0.23904\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 364 | loss: 0.23904 - acc: 0.9422 -- iter: 8/8\n",
      "--\n",
      "Training Step: 365  | total loss: \u001B[1m\u001B[32m0.21806\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 365 | loss: 0.21806 - acc: 0.9480 -- iter: 8/8\n",
      "--\n",
      "Training Step: 366  | total loss: \u001B[1m\u001B[32m0.19920\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 366 | loss: 0.19920 - acc: 0.9532 -- iter: 8/8\n",
      "--\n",
      "Training Step: 367  | total loss: \u001B[1m\u001B[32m0.18225\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 367 | loss: 0.18225 - acc: 0.9579 -- iter: 8/8\n",
      "--\n",
      "Training Step: 368  | total loss: \u001B[1m\u001B[32m0.16701\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 368 | loss: 0.16701 - acc: 0.9621 -- iter: 8/8\n",
      "--\n",
      "Training Step: 369  | total loss: \u001B[1m\u001B[32m0.15331\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 369 | loss: 0.15331 - acc: 0.9659 -- iter: 8/8\n",
      "--\n",
      "Training Step: 370  | total loss: \u001B[1m\u001B[32m0.14098\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 370 | loss: 0.14098 - acc: 0.9693 -- iter: 8/8\n",
      "--\n",
      "Training Step: 371  | total loss: \u001B[1m\u001B[32m0.12990\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 371 | loss: 0.12990 - acc: 0.9724 -- iter: 8/8\n",
      "--\n",
      "Training Step: 372  | total loss: \u001B[1m\u001B[32m0.11992\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 372 | loss: 0.11992 - acc: 0.9751 -- iter: 8/8\n",
      "--\n",
      "Training Step: 373  | total loss: \u001B[1m\u001B[32m0.11094\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 373 | loss: 0.11094 - acc: 0.9776 -- iter: 8/8\n",
      "--\n",
      "Training Step: 374  | total loss: \u001B[1m\u001B[32m0.10285\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 374 | loss: 0.10285 - acc: 0.9798 -- iter: 8/8\n",
      "--\n",
      "Training Step: 375  | total loss: \u001B[1m\u001B[32m0.09557\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 375 | loss: 0.09557 - acc: 0.9819 -- iter: 8/8\n",
      "--\n",
      "Training Step: 376  | total loss: \u001B[1m\u001B[32m0.08900\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 376 | loss: 0.08900 - acc: 0.9837 -- iter: 8/8\n",
      "--\n",
      "Training Step: 377  | total loss: \u001B[1m\u001B[32m0.08308\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 377 | loss: 0.08308 - acc: 0.9853 -- iter: 8/8\n",
      "--\n",
      "Training Step: 378  | total loss: \u001B[1m\u001B[32m0.07774\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 378 | loss: 0.07774 - acc: 0.9868 -- iter: 8/8\n",
      "--\n",
      "Training Step: 379  | total loss: \u001B[1m\u001B[32m0.07292\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 379 | loss: 0.07292 - acc: 0.9881 -- iter: 8/8\n",
      "--\n",
      "Training Step: 380  | total loss: \u001B[1m\u001B[32m0.06856\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 380 | loss: 0.06856 - acc: 0.9893 -- iter: 8/8\n",
      "--\n",
      "Training Step: 381  | total loss: \u001B[1m\u001B[32m0.06462\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 381 | loss: 0.06462 - acc: 0.9904 -- iter: 8/8\n",
      "--\n",
      "Training Step: 382  | total loss: \u001B[1m\u001B[32m0.06106\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 382 | loss: 0.06106 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 383  | total loss: \u001B[1m\u001B[32m0.05784\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 383 | loss: 0.05784 - acc: 0.9922 -- iter: 8/8\n",
      "--\n",
      "Training Step: 384  | total loss: \u001B[1m\u001B[32m0.05491\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 384 | loss: 0.05491 - acc: 0.9930 -- iter: 8/8\n",
      "--\n",
      "Training Step: 385  | total loss: \u001B[1m\u001B[32m0.05226\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 385 | loss: 0.05226 - acc: 0.9937 -- iter: 8/8\n",
      "--\n",
      "Training Step: 386  | total loss: \u001B[1m\u001B[32m0.04985\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 386 | loss: 0.04985 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 387  | total loss: \u001B[1m\u001B[32m0.04767\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 387 | loss: 0.04767 - acc: 0.9949 -- iter: 8/8\n",
      "--\n",
      "Training Step: 388  | total loss: \u001B[1m\u001B[32m0.04567\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 388 | loss: 0.04567 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 389  | total loss: \u001B[1m\u001B[32m0.04386\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 389 | loss: 0.04386 - acc: 0.9959 -- iter: 8/8\n",
      "--\n",
      "Training Step: 390  | total loss: \u001B[1m\u001B[32m0.04220\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 390 | loss: 0.04220 - acc: 0.9963 -- iter: 8/8\n",
      "--\n",
      "Training Step: 391  | total loss: \u001B[1m\u001B[32m0.04069\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 391 | loss: 0.04069 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 392  | total loss: \u001B[1m\u001B[32m0.03930\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 392 | loss: 0.03930 - acc: 0.9970 -- iter: 8/8\n",
      "--\n",
      "Training Step: 393  | total loss: \u001B[1m\u001B[32m0.03803\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 393 | loss: 0.03803 - acc: 0.9973 -- iter: 8/8\n",
      "--\n",
      "Training Step: 394  | total loss: \u001B[1m\u001B[32m0.03687\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 394 | loss: 0.03687 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 395  | total loss: \u001B[1m\u001B[32m0.03579\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 395 | loss: 0.03579 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 396  | total loss: \u001B[1m\u001B[32m0.31691\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 396 | loss: 0.31691 - acc: 0.9230 -- iter: 8/8\n",
      "--\n",
      "Training Step: 397  | total loss: \u001B[1m\u001B[32m0.26175\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 397 | loss: 0.26175 - acc: 0.9307 -- iter: 8/8\n",
      "--\n",
      "Training Step: 398  | total loss: \u001B[1m\u001B[32m0.26175\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 398 | loss: 0.26175 - acc: 0.9376 -- iter: 8/8\n",
      "--\n",
      "Training Step: 399  | total loss: \u001B[1m\u001B[32m0.23829\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 399 | loss: 0.23829 - acc: 0.9439 -- iter: 8/8\n",
      "--\n",
      "Training Step: 400  | total loss: \u001B[1m\u001B[32m0.21720\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 400 | loss: 0.21720 - acc: 0.9495 -- iter: 8/8\n",
      "--\n",
      "Training Step: 401  | total loss: \u001B[1m\u001B[32m0.19825\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 401 | loss: 0.19825 - acc: 0.9545 -- iter: 8/8\n",
      "--\n",
      "Training Step: 402  | total loss: \u001B[1m\u001B[32m0.18122\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 402 | loss: 0.18122 - acc: 0.9591 -- iter: 8/8\n",
      "--\n",
      "Training Step: 403  | total loss: \u001B[1m\u001B[32m0.16591\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 403 | loss: 0.16591 - acc: 0.9632 -- iter: 8/8\n",
      "--\n",
      "Training Step: 404  | total loss: \u001B[1m\u001B[32m0.15213\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 404 | loss: 0.15213 - acc: 0.9669 -- iter: 8/8\n",
      "--\n",
      "Training Step: 405  | total loss: \u001B[1m\u001B[32m0.13975\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 405 | loss: 0.13975 - acc: 0.9702 -- iter: 8/8\n",
      "--\n",
      "Training Step: 406  | total loss: \u001B[1m\u001B[32m0.12860\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 406 | loss: 0.12860 - acc: 0.9732 -- iter: 8/8\n",
      "--\n",
      "Training Step: 407  | total loss: \u001B[1m\u001B[32m0.11858\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 407 | loss: 0.11858 - acc: 0.9758 -- iter: 8/8\n",
      "--\n",
      "Training Step: 408  | total loss: \u001B[1m\u001B[32m0.10955\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 408 | loss: 0.10955 - acc: 0.9783 -- iter: 8/8\n",
      "--\n",
      "Training Step: 409  | total loss: \u001B[1m\u001B[32m0.10143\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 409 | loss: 0.10143 - acc: 0.9804 -- iter: 8/8\n",
      "--\n",
      "Training Step: 410  | total loss: \u001B[1m\u001B[32m0.09411\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 410 | loss: 0.09411 - acc: 0.9824 -- iter: 8/8\n",
      "--\n",
      "Training Step: 411  | total loss: \u001B[1m\u001B[32m0.08751\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 411 | loss: 0.08751 - acc: 0.9841 -- iter: 8/8\n",
      "--\n",
      "Training Step: 412  | total loss: \u001B[1m\u001B[32m0.08157\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 412 | loss: 0.08157 - acc: 0.9857 -- iter: 8/8\n",
      "--\n",
      "Training Step: 413  | total loss: \u001B[1m\u001B[32m0.07620\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 413 | loss: 0.07620 - acc: 0.9872 -- iter: 8/8\n",
      "--\n",
      "Training Step: 414  | total loss: \u001B[1m\u001B[32m0.07136\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 414 | loss: 0.07136 - acc: 0.9884 -- iter: 8/8\n",
      "--\n",
      "Training Step: 415  | total loss: \u001B[1m\u001B[32m0.06700\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 415 | loss: 0.06700 - acc: 0.9896 -- iter: 8/8\n",
      "--\n",
      "Training Step: 416  | total loss: \u001B[1m\u001B[32m0.06305\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 416 | loss: 0.06305 - acc: 0.9906 -- iter: 8/8\n",
      "--\n",
      "Training Step: 417  | total loss: \u001B[1m\u001B[32m0.05948\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 417 | loss: 0.05948 - acc: 0.9916 -- iter: 8/8\n",
      "--\n",
      "Training Step: 418  | total loss: \u001B[1m\u001B[32m0.05625\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 418 | loss: 0.05625 - acc: 0.9924 -- iter: 8/8\n",
      "--\n",
      "Training Step: 419  | total loss: \u001B[1m\u001B[32m0.05333\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 419 | loss: 0.05333 - acc: 0.9932 -- iter: 8/8\n",
      "--\n",
      "Training Step: 420  | total loss: \u001B[1m\u001B[32m0.05068\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 420 | loss: 0.05068 - acc: 0.9939 -- iter: 8/8\n",
      "--\n",
      "Training Step: 421  | total loss: \u001B[1m\u001B[32m0.04827\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 421 | loss: 0.04827 - acc: 0.9945 -- iter: 8/8\n",
      "--\n",
      "Training Step: 422  | total loss: \u001B[1m\u001B[32m0.04609\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 422 | loss: 0.04609 - acc: 0.9950 -- iter: 8/8\n",
      "--\n",
      "Training Step: 423  | total loss: \u001B[1m\u001B[32m0.04411\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 423 | loss: 0.04411 - acc: 0.9955 -- iter: 8/8\n",
      "--\n",
      "Training Step: 424  | total loss: \u001B[1m\u001B[32m0.04230\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 424 | loss: 0.04230 - acc: 0.9960 -- iter: 8/8\n",
      "--\n",
      "Training Step: 425  | total loss: \u001B[1m\u001B[32m0.04066\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 425 | loss: 0.04066 - acc: 0.9964 -- iter: 8/8\n",
      "--\n",
      "Training Step: 426  | total loss: \u001B[1m\u001B[32m0.03916\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 426 | loss: 0.03916 - acc: 0.9967 -- iter: 8/8\n",
      "--\n",
      "Training Step: 427  | total loss: \u001B[1m\u001B[32m0.03778\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 427 | loss: 0.03778 - acc: 0.9971 -- iter: 8/8\n",
      "--\n",
      "Training Step: 428  | total loss: \u001B[1m\u001B[32m0.21980\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 428 | loss: 0.21980 - acc: 0.9474 -- iter: 8/8\n",
      "--\n",
      "Training Step: 429  | total loss: \u001B[1m\u001B[32m0.20037\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 429 | loss: 0.20037 - acc: 0.9526 -- iter: 8/8\n",
      "--\n",
      "Training Step: 430  | total loss: \u001B[1m\u001B[32m0.18290\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 430 | loss: 0.18290 - acc: 0.9574 -- iter: 8/8\n",
      "--\n",
      "Training Step: 431  | total loss: \u001B[1m\u001B[32m0.16720\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 431 | loss: 0.16720 - acc: 0.9616 -- iter: 8/8\n",
      "--\n",
      "Training Step: 432  | total loss: \u001B[1m\u001B[32m0.15308\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 432 | loss: 0.15308 - acc: 0.9655 -- iter: 8/8\n",
      "--\n",
      "Training Step: 433  | total loss: \u001B[1m\u001B[32m0.14038\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 433 | loss: 0.14038 - acc: 0.9689 -- iter: 8/8\n",
      "--\n",
      "Training Step: 434  | total loss: \u001B[1m\u001B[32m0.12896\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 434 | loss: 0.12896 - acc: 0.9720 -- iter: 8/8\n",
      "--\n",
      "Training Step: 435  | total loss: \u001B[1m\u001B[32m0.11868\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 435 | loss: 0.11868 - acc: 0.9748 -- iter: 8/8\n",
      "--\n",
      "Training Step: 436  | total loss: \u001B[1m\u001B[32m0.10943\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 436 | loss: 0.10943 - acc: 0.9773 -- iter: 8/8\n",
      "--\n",
      "Training Step: 437  | total loss: \u001B[1m\u001B[32m0.10111\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 437 | loss: 0.10111 - acc: 0.9796 -- iter: 8/8\n",
      "--\n",
      "Training Step: 438  | total loss: \u001B[1m\u001B[32m0.09361\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 438 | loss: 0.09361 - acc: 0.9816 -- iter: 8/8\n",
      "--\n",
      "Training Step: 439  | total loss: \u001B[1m\u001B[32m0.08686\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 439 | loss: 0.08686 - acc: 0.9835 -- iter: 8/8\n",
      "--\n",
      "Training Step: 440  | total loss: \u001B[1m\u001B[32m0.08078\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 440 | loss: 0.08078 - acc: 0.9851 -- iter: 8/8\n",
      "--\n",
      "Training Step: 441  | total loss: \u001B[1m\u001B[32m0.07529\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 441 | loss: 0.07529 - acc: 0.9866 -- iter: 8/8\n",
      "--\n",
      "Training Step: 442  | total loss: \u001B[1m\u001B[32m0.25587\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 442 | loss: 0.25587 - acc: 0.9380 -- iter: 8/8\n",
      "--\n",
      "Training Step: 443  | total loss: \u001B[1m\u001B[32m0.23290\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 443 | loss: 0.23290 - acc: 0.9442 -- iter: 8/8\n",
      "--\n",
      "Training Step: 444  | total loss: \u001B[1m\u001B[32m0.21226\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 444 | loss: 0.21226 - acc: 0.9497 -- iter: 8/8\n",
      "--\n",
      "Training Step: 445  | total loss: \u001B[1m\u001B[32m0.19370\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 445 | loss: 0.19370 - acc: 0.9548 -- iter: 8/8\n",
      "--\n",
      "Training Step: 446  | total loss: \u001B[1m\u001B[32m0.17702\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 446 | loss: 0.17702 - acc: 0.9593 -- iter: 8/8\n",
      "--\n",
      "Training Step: 447  | total loss: \u001B[1m\u001B[32m0.16202\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 447 | loss: 0.16202 - acc: 0.9634 -- iter: 8/8\n",
      "--\n",
      "Training Step: 448  | total loss: \u001B[1m\u001B[32m0.25202\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 448 | loss: 0.25202 - acc: 0.9420 -- iter: 8/8\n",
      "--\n",
      "Training Step: 449  | total loss: \u001B[1m\u001B[32m0.22957\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 449 | loss: 0.22957 - acc: 0.9478 -- iter: 8/8\n",
      "--\n",
      "Training Step: 450  | total loss: \u001B[1m\u001B[32m0.20938\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 450 | loss: 0.20938 - acc: 0.9530 -- iter: 8/8\n",
      "--\n",
      "Training Step: 451  | total loss: \u001B[1m\u001B[32m0.19124\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 451 | loss: 0.19124 - acc: 0.9577 -- iter: 8/8\n",
      "--\n",
      "Training Step: 452  | total loss: \u001B[1m\u001B[32m0.17492\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 452 | loss: 0.17492 - acc: 0.9620 -- iter: 8/8\n",
      "--\n",
      "Training Step: 453  | total loss: \u001B[1m\u001B[32m0.16025\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 453 | loss: 0.16025 - acc: 0.9658 -- iter: 8/8\n",
      "--\n",
      "Training Step: 454  | total loss: \u001B[1m\u001B[32m0.14705\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 454 | loss: 0.14705 - acc: 0.9692 -- iter: 8/8\n",
      "--\n",
      "Training Step: 455  | total loss: \u001B[1m\u001B[32m0.13518\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 455 | loss: 0.13518 - acc: 0.9723 -- iter: 8/8\n",
      "--\n",
      "Training Step: 456  | total loss: \u001B[1m\u001B[32m0.12450\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 456 | loss: 0.12450 - acc: 0.9750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 457  | total loss: \u001B[1m\u001B[32m0.11489\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 457 | loss: 0.11489 - acc: 0.9775 -- iter: 8/8\n",
      "--\n",
      "Training Step: 458  | total loss: \u001B[1m\u001B[32m0.10623\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 458 | loss: 0.10623 - acc: 0.9798 -- iter: 8/8\n",
      "--\n",
      "Training Step: 459  | total loss: \u001B[1m\u001B[32m0.09844\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 459 | loss: 0.09844 - acc: 0.9818 -- iter: 8/8\n",
      "--\n",
      "Training Step: 460  | total loss: \u001B[1m\u001B[32m0.09141\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 460 | loss: 0.09141 - acc: 0.9836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 461  | total loss: \u001B[1m\u001B[32m0.08508\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 461 | loss: 0.08508 - acc: 0.9853 -- iter: 8/8\n",
      "--\n",
      "Training Step: 462  | total loss: \u001B[1m\u001B[32m0.07937\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 462 | loss: 0.07937 - acc: 0.9867 -- iter: 8/8\n",
      "--\n",
      "Training Step: 463  | total loss: \u001B[1m\u001B[32m0.07422\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 463 | loss: 0.07422 - acc: 0.9881 -- iter: 8/8\n",
      "--\n",
      "Training Step: 464  | total loss: \u001B[1m\u001B[32m0.06957\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 464 | loss: 0.06957 - acc: 0.9893 -- iter: 8/8\n",
      "--\n",
      "Training Step: 465  | total loss: \u001B[1m\u001B[32m0.06537\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 465 | loss: 0.06537 - acc: 0.9903 -- iter: 8/8\n",
      "--\n",
      "Training Step: 466  | total loss: \u001B[1m\u001B[32m0.06158\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 466 | loss: 0.06158 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 467  | total loss: \u001B[1m\u001B[32m0.05814\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 467 | loss: 0.05814 - acc: 0.9922 -- iter: 8/8\n",
      "--\n",
      "Training Step: 468  | total loss: \u001B[1m\u001B[32m0.05504\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 468 | loss: 0.05504 - acc: 0.9930 -- iter: 8/8\n",
      "--\n",
      "Training Step: 469  | total loss: \u001B[1m\u001B[32m0.05222\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 469 | loss: 0.05222 - acc: 0.9937 -- iter: 8/8\n",
      "--\n",
      "Training Step: 470  | total loss: \u001B[1m\u001B[32m0.04967\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 470 | loss: 0.04967 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 471  | total loss: \u001B[1m\u001B[32m0.04736\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 471 | loss: 0.04736 - acc: 0.9949 -- iter: 8/8\n",
      "--\n",
      "Training Step: 472  | total loss: \u001B[1m\u001B[32m0.04525\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 472 | loss: 0.04525 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 473  | total loss: \u001B[1m\u001B[32m0.04334\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 473 | loss: 0.04334 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 474  | total loss: \u001B[1m\u001B[32m0.24503\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 474 | loss: 0.24503 - acc: 0.9463 -- iter: 8/8\n",
      "--\n",
      "Training Step: 475  | total loss: \u001B[1m\u001B[32m0.22315\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 475 | loss: 0.22315 - acc: 0.9516 -- iter: 8/8\n",
      "--\n",
      "Training Step: 476  | total loss: \u001B[1m\u001B[32m0.20348\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 476 | loss: 0.20348 - acc: 0.9565 -- iter: 8/8\n",
      "--\n",
      "Training Step: 477  | total loss: \u001B[1m\u001B[32m0.18579\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 477 | loss: 0.18579 - acc: 0.9608 -- iter: 8/8\n",
      "--\n",
      "Training Step: 478  | total loss: \u001B[1m\u001B[32m0.16989\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 478 | loss: 0.16989 - acc: 0.9647 -- iter: 8/8\n",
      "--\n",
      "Training Step: 479  | total loss: \u001B[1m\u001B[32m0.15559\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 479 | loss: 0.15559 - acc: 0.9683 -- iter: 8/8\n",
      "--\n",
      "Training Step: 480  | total loss: \u001B[1m\u001B[32m0.14273\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 480 | loss: 0.14273 - acc: 0.9714 -- iter: 8/8\n",
      "--\n",
      "Training Step: 481  | total loss: \u001B[1m\u001B[32m0.13116\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 481 | loss: 0.13116 - acc: 0.9743 -- iter: 8/8\n",
      "--\n",
      "Training Step: 482  | total loss: \u001B[1m\u001B[32m0.12075\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 482 | loss: 0.12075 - acc: 0.9769 -- iter: 8/8\n",
      "--\n",
      "Training Step: 483  | total loss: \u001B[1m\u001B[32m0.11138\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 483 | loss: 0.11138 - acc: 0.9792 -- iter: 8/8\n",
      "--\n",
      "Training Step: 484  | total loss: \u001B[1m\u001B[32m0.10294\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 484 | loss: 0.10294 - acc: 0.9813 -- iter: 8/8\n",
      "--\n",
      "Training Step: 485  | total loss: \u001B[1m\u001B[32m0.09534\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 485 | loss: 0.09534 - acc: 0.9831 -- iter: 8/8\n",
      "--\n",
      "Training Step: 486  | total loss: \u001B[1m\u001B[32m0.08849\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 486 | loss: 0.08849 - acc: 0.9848 -- iter: 8/8\n",
      "--\n",
      "Training Step: 487  | total loss: \u001B[1m\u001B[32m0.08232\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 487 | loss: 0.08232 - acc: 0.9863 -- iter: 8/8\n",
      "--\n",
      "Training Step: 488  | total loss: \u001B[1m\u001B[32m0.07676\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 488 | loss: 0.07676 - acc: 0.9877 -- iter: 8/8\n",
      "--\n",
      "Training Step: 489  | total loss: \u001B[1m\u001B[32m0.07174\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 489 | loss: 0.07174 - acc: 0.9889 -- iter: 8/8\n",
      "--\n",
      "Training Step: 490  | total loss: \u001B[1m\u001B[32m0.06721\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 490 | loss: 0.06721 - acc: 0.9900 -- iter: 8/8\n",
      "--\n",
      "Training Step: 491  | total loss: \u001B[1m\u001B[32m0.06312\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 491 | loss: 0.06312 - acc: 0.9910 -- iter: 8/8\n",
      "--\n",
      "Training Step: 492  | total loss: \u001B[1m\u001B[32m0.05943\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 492 | loss: 0.05943 - acc: 0.9919 -- iter: 8/8\n",
      "--\n",
      "Training Step: 493  | total loss: \u001B[1m\u001B[32m0.05608\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 493 | loss: 0.05608 - acc: 0.9927 -- iter: 8/8\n",
      "--\n",
      "Training Step: 494  | total loss: \u001B[1m\u001B[32m0.05306\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 494 | loss: 0.05306 - acc: 0.9935 -- iter: 8/8\n",
      "--\n",
      "Training Step: 495  | total loss: \u001B[1m\u001B[32m0.05032\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 495 | loss: 0.05032 - acc: 0.9941 -- iter: 8/8\n",
      "--\n",
      "Training Step: 496  | total loss: \u001B[1m\u001B[32m0.04784\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 496 | loss: 0.04784 - acc: 0.9947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 497  | total loss: \u001B[1m\u001B[32m0.04559\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 497 | loss: 0.04559 - acc: 0.9952 -- iter: 8/8\n",
      "--\n",
      "Training Step: 498  | total loss: \u001B[1m\u001B[32m0.04354\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 498 | loss: 0.04354 - acc: 0.9957 -- iter: 8/8\n",
      "--\n",
      "Training Step: 499  | total loss: \u001B[1m\u001B[32m0.04168\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 499 | loss: 0.04168 - acc: 0.9961 -- iter: 8/8\n",
      "--\n",
      "Training Step: 500  | total loss: \u001B[1m\u001B[32m0.03999\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 500 | loss: 0.03999 - acc: 0.9965 -- iter: 8/8\n",
      "--\n",
      "Training Step: 501  | total loss: \u001B[1m\u001B[32m0.03845\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 501 | loss: 0.03845 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 502  | total loss: \u001B[1m\u001B[32m0.03704\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 502 | loss: 0.03704 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 503  | total loss: \u001B[1m\u001B[32m0.03575\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 503 | loss: 0.03575 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 504  | total loss: \u001B[1m\u001B[32m0.03458\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 504 | loss: 0.03458 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 505  | total loss: \u001B[1m\u001B[32m0.03350\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 505 | loss: 0.03350 - acc: 0.9979 -- iter: 8/8\n",
      "--\n",
      "Training Step: 506  | total loss: \u001B[1m\u001B[32m0.03251\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 506 | loss: 0.03251 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 507  | total loss: \u001B[1m\u001B[32m0.03160\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 507 | loss: 0.03160 - acc: 0.9983 -- iter: 8/8\n",
      "--\n",
      "Training Step: 508  | total loss: \u001B[1m\u001B[32m0.03076\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 508 | loss: 0.03076 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 509  | total loss: \u001B[1m\u001B[32m0.02999\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 509 | loss: 0.02999 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 510  | total loss: \u001B[1m\u001B[32m0.02927\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 510 | loss: 0.02927 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 511  | total loss: \u001B[1m\u001B[32m0.02861\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 511 | loss: 0.02861 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 512  | total loss: \u001B[1m\u001B[32m0.02799\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 512 | loss: 0.02799 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 513  | total loss: \u001B[1m\u001B[32m0.02742\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 513 | loss: 0.02742 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 514  | total loss: \u001B[1m\u001B[32m0.02688\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 514 | loss: 0.02688 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 515  | total loss: \u001B[1m\u001B[32m0.02638\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 515 | loss: 0.02638 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 516  | total loss: \u001B[1m\u001B[32m0.02592\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 516 | loss: 0.02592 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 517  | total loss: \u001B[1m\u001B[32m0.02548\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 517 | loss: 0.02548 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 518  | total loss: \u001B[1m\u001B[32m0.02506\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 518 | loss: 0.02506 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 519  | total loss: \u001B[1m\u001B[32m0.02467\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 519 | loss: 0.02467 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 520  | total loss: \u001B[1m\u001B[32m0.02430\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 520 | loss: 0.02430 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 521  | total loss: \u001B[1m\u001B[32m0.02395\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 521 | loss: 0.02395 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 522  | total loss: \u001B[1m\u001B[32m0.02362\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 522 | loss: 0.02362 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 523  | total loss: \u001B[1m\u001B[32m0.02330\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 523 | loss: 0.02330 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 524  | total loss: \u001B[1m\u001B[32m0.02300\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 524 | loss: 0.02300 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 525  | total loss: \u001B[1m\u001B[32m0.02271\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 525 | loss: 0.02271 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 526  | total loss: \u001B[1m\u001B[32m0.02244\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 526 | loss: 0.02244 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 527  | total loss: \u001B[1m\u001B[32m0.02217\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 527 | loss: 0.02217 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 528  | total loss: \u001B[1m\u001B[32m0.02192\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 528 | loss: 0.02192 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 529  | total loss: \u001B[1m\u001B[32m0.02167\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 529 | loss: 0.02167 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 530  | total loss: \u001B[1m\u001B[32m0.02143\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 530 | loss: 0.02143 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 531  | total loss: \u001B[1m\u001B[32m0.02120\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 531 | loss: 0.02120 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 532  | total loss: \u001B[1m\u001B[32m0.02098\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 532 | loss: 0.02098 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 533  | total loss: \u001B[1m\u001B[32m0.02077\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 533 | loss: 0.02077 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 534  | total loss: \u001B[1m\u001B[32m0.02056\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 534 | loss: 0.02056 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 535  | total loss: \u001B[1m\u001B[32m0.02036\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 535 | loss: 0.02036 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 536  | total loss: \u001B[1m\u001B[32m0.02016\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 536 | loss: 0.02016 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 537  | total loss: \u001B[1m\u001B[32m0.01997\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 537 | loss: 0.01997 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 538  | total loss: \u001B[1m\u001B[32m0.01978\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 538 | loss: 0.01978 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 539  | total loss: \u001B[1m\u001B[32m0.01959\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 539 | loss: 0.01959 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 540  | total loss: \u001B[1m\u001B[32m0.01941\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 540 | loss: 0.01941 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 541  | total loss: \u001B[1m\u001B[32m0.01924\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 541 | loss: 0.01924 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 542  | total loss: \u001B[1m\u001B[32m0.01907\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 542 | loss: 0.01907 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 543  | total loss: \u001B[1m\u001B[32m0.01890\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 543 | loss: 0.01890 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 544  | total loss: \u001B[1m\u001B[32m0.01873\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 544 | loss: 0.01873 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 545  | total loss: \u001B[1m\u001B[32m0.01857\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 545 | loss: 0.01857 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 546  | total loss: \u001B[1m\u001B[32m0.01841\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 546 | loss: 0.01841 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 547  | total loss: \u001B[1m\u001B[32m0.01826\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 547 | loss: 0.01826 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 548  | total loss: \u001B[1m\u001B[32m0.01810\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 548 | loss: 0.01810 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 549  | total loss: \u001B[1m\u001B[32m0.01795\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 549 | loss: 0.01795 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 550  | total loss: \u001B[1m\u001B[32m0.01780\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 550 | loss: 0.01780 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 551  | total loss: \u001B[1m\u001B[32m0.01766\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 551 | loss: 0.01766 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 552  | total loss: \u001B[1m\u001B[32m0.01751\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 552 | loss: 0.01751 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 553  | total loss: \u001B[1m\u001B[32m0.01737\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 553 | loss: 0.01737 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 554  | total loss: \u001B[1m\u001B[32m0.01723\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 554 | loss: 0.01723 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 555  | total loss: \u001B[1m\u001B[32m0.01710\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 555 | loss: 0.01710 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 556  | total loss: \u001B[1m\u001B[32m0.01696\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 556 | loss: 0.01696 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 557  | total loss: \u001B[1m\u001B[32m0.01683\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 557 | loss: 0.01683 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 558  | total loss: \u001B[1m\u001B[32m0.01669\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 558 | loss: 0.01669 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 559  | total loss: \u001B[1m\u001B[32m0.01656\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 559 | loss: 0.01656 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 560  | total loss: \u001B[1m\u001B[32m0.01644\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 560 | loss: 0.01644 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 561  | total loss: \u001B[1m\u001B[32m0.01631\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 561 | loss: 0.01631 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 562  | total loss: \u001B[1m\u001B[32m0.01618\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 562 | loss: 0.01618 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 563  | total loss: \u001B[1m\u001B[32m0.01606\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 563 | loss: 0.01606 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 564  | total loss: \u001B[1m\u001B[32m0.01594\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 564 | loss: 0.01594 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 565  | total loss: \u001B[1m\u001B[32m0.01582\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 565 | loss: 0.01582 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 566  | total loss: \u001B[1m\u001B[32m0.01570\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 566 | loss: 0.01570 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 567  | total loss: \u001B[1m\u001B[32m0.01558\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 567 | loss: 0.01558 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 568  | total loss: \u001B[1m\u001B[32m0.01547\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 568 | loss: 0.01547 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 569  | total loss: \u001B[1m\u001B[32m0.01535\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 569 | loss: 0.01535 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 570  | total loss: \u001B[1m\u001B[32m0.01524\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 570 | loss: 0.01524 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 571  | total loss: \u001B[1m\u001B[32m0.01513\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 571 | loss: 0.01513 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 572  | total loss: \u001B[1m\u001B[32m0.01502\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 572 | loss: 0.01502 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 573  | total loss: \u001B[1m\u001B[32m0.01491\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 573 | loss: 0.01491 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 574  | total loss: \u001B[1m\u001B[32m0.01480\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 574 | loss: 0.01480 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 575  | total loss: \u001B[1m\u001B[32m0.01469\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 575 | loss: 0.01469 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 576  | total loss: \u001B[1m\u001B[32m0.01459\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 576 | loss: 0.01459 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 577  | total loss: \u001B[1m\u001B[32m0.01448\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 577 | loss: 0.01448 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 578  | total loss: \u001B[1m\u001B[32m0.01438\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 578 | loss: 0.01438 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 579  | total loss: \u001B[1m\u001B[32m0.01428\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 579 | loss: 0.01428 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 580  | total loss: \u001B[1m\u001B[32m0.34493\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 580 | loss: 0.34493 - acc: 0.9250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 581  | total loss: \u001B[1m\u001B[32m0.31180\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 581 | loss: 0.31180 - acc: 0.9325 -- iter: 8/8\n",
      "--\n",
      "Training Step: 582  | total loss: \u001B[1m\u001B[32m0.28202\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 582 | loss: 0.28202 - acc: 0.9392 -- iter: 8/8\n",
      "--\n",
      "Training Step: 583  | total loss: \u001B[1m\u001B[32m0.25525\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 583 | loss: 0.25525 - acc: 0.9453 -- iter: 8/8\n",
      "--\n",
      "Training Step: 584  | total loss: \u001B[1m\u001B[32m0.23118\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 584 | loss: 0.23118 - acc: 0.9508 -- iter: 8/8\n",
      "--\n",
      "Training Step: 585  | total loss: \u001B[1m\u001B[32m0.20954\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 585 | loss: 0.20954 - acc: 0.9557 -- iter: 8/8\n",
      "--\n",
      "Training Step: 586  | total loss: \u001B[1m\u001B[32m0.19008\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 586 | loss: 0.19008 - acc: 0.9601 -- iter: 8/8\n",
      "--\n",
      "Training Step: 587  | total loss: \u001B[1m\u001B[32m0.17259\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 587 | loss: 0.17259 - acc: 0.9641 -- iter: 8/8\n",
      "--\n",
      "Training Step: 588  | total loss: \u001B[1m\u001B[32m0.15686\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 588 | loss: 0.15686 - acc: 0.9677 -- iter: 8/8\n",
      "--\n",
      "Training Step: 589  | total loss: \u001B[1m\u001B[32m0.14272\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 589 | loss: 0.14272 - acc: 0.9709 -- iter: 8/8\n",
      "--\n",
      "Training Step: 590  | total loss: \u001B[1m\u001B[32m0.13000\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 590 | loss: 0.13000 - acc: 0.9738 -- iter: 8/8\n",
      "--\n",
      "Training Step: 591  | total loss: \u001B[1m\u001B[32m0.11857\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 591 | loss: 0.11857 - acc: 0.9765 -- iter: 8/8\n",
      "--\n",
      "Training Step: 592  | total loss: \u001B[1m\u001B[32m0.10828\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 592 | loss: 0.10828 - acc: 0.9788 -- iter: 8/8\n",
      "--\n",
      "Training Step: 593  | total loss: \u001B[1m\u001B[32m0.09903\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 593 | loss: 0.09903 - acc: 0.9809 -- iter: 8/8\n",
      "--\n",
      "Training Step: 594  | total loss: \u001B[1m\u001B[32m0.09071\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 594 | loss: 0.09071 - acc: 0.9828 -- iter: 8/8\n",
      "--\n",
      "Training Step: 595  | total loss: \u001B[1m\u001B[32m0.08322\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 595 | loss: 0.08322 - acc: 0.9846 -- iter: 8/8\n",
      "--\n",
      "Training Step: 596  | total loss: \u001B[1m\u001B[32m0.07648\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 596 | loss: 0.07648 - acc: 0.9861 -- iter: 8/8\n",
      "--\n",
      "Training Step: 597  | total loss: \u001B[1m\u001B[32m0.07041\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 597 | loss: 0.07041 - acc: 0.9875 -- iter: 8/8\n",
      "--\n",
      "Training Step: 598  | total loss: \u001B[1m\u001B[32m0.06495\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 598 | loss: 0.06495 - acc: 0.9887 -- iter: 8/8\n",
      "--\n",
      "Training Step: 599  | total loss: \u001B[1m\u001B[32m0.06003\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 599 | loss: 0.06003 - acc: 0.9899 -- iter: 8/8\n",
      "--\n",
      "Training Step: 600  | total loss: \u001B[1m\u001B[32m0.05560\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 600 | loss: 0.05560 - acc: 0.9909 -- iter: 8/8\n",
      "--\n",
      "Training Step: 601  | total loss: \u001B[1m\u001B[32m0.05162\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 601 | loss: 0.05162 - acc: 0.9918 -- iter: 8/8\n",
      "--\n",
      "Training Step: 602  | total loss: \u001B[1m\u001B[32m0.04802\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 602 | loss: 0.04802 - acc: 0.9926 -- iter: 8/8\n",
      "--\n",
      "Training Step: 603  | total loss: \u001B[1m\u001B[32m0.04478\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 603 | loss: 0.04478 - acc: 0.9934 -- iter: 8/8\n",
      "--\n",
      "Training Step: 604  | total loss: \u001B[1m\u001B[32m0.04186\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 604 | loss: 0.04186 - acc: 0.9940 -- iter: 8/8\n",
      "--\n",
      "Training Step: 605  | total loss: \u001B[1m\u001B[32m0.03922\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 605 | loss: 0.03922 - acc: 0.9946 -- iter: 8/8\n",
      "--\n",
      "Training Step: 606  | total loss: \u001B[1m\u001B[32m0.03684\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 606 | loss: 0.03684 - acc: 0.9952 -- iter: 8/8\n",
      "--\n",
      "Training Step: 607  | total loss: \u001B[1m\u001B[32m0.03469\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 607 | loss: 0.03469 - acc: 0.9956 -- iter: 8/8\n",
      "--\n",
      "Training Step: 608  | total loss: \u001B[1m\u001B[32m0.03275\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 608 | loss: 0.03275 - acc: 0.9961 -- iter: 8/8\n",
      "--\n",
      "Training Step: 609  | total loss: \u001B[1m\u001B[32m0.03100\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 609 | loss: 0.03100 - acc: 0.9965 -- iter: 8/8\n",
      "--\n",
      "Training Step: 610  | total loss: \u001B[1m\u001B[32m0.02941\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 610 | loss: 0.02941 - acc: 0.9968 -- iter: 8/8\n",
      "--\n",
      "Training Step: 611  | total loss: \u001B[1m\u001B[32m0.02798\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 611 | loss: 0.02798 - acc: 0.9971 -- iter: 8/8\n",
      "--\n",
      "Training Step: 612  | total loss: \u001B[1m\u001B[32m0.02668\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 612 | loss: 0.02668 - acc: 0.9974 -- iter: 8/8\n",
      "--\n",
      "Training Step: 613  | total loss: \u001B[1m\u001B[32m0.02550\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 613 | loss: 0.02550 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 614  | total loss: \u001B[1m\u001B[32m0.02443\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 614 | loss: 0.02443 - acc: 0.9979 -- iter: 8/8\n",
      "--\n",
      "Training Step: 615  | total loss: \u001B[1m\u001B[32m0.02346\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 615 | loss: 0.02346 - acc: 0.9981 -- iter: 8/8\n",
      "--\n",
      "Training Step: 616  | total loss: \u001B[1m\u001B[32m0.02257\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 616 | loss: 0.02257 - acc: 0.9983 -- iter: 8/8\n",
      "--\n",
      "Training Step: 617  | total loss: \u001B[1m\u001B[32m0.02177\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 617 | loss: 0.02177 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 618  | total loss: \u001B[1m\u001B[32m0.02104\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 618 | loss: 0.02104 - acc: 0.9986 -- iter: 8/8\n",
      "--\n",
      "Training Step: 619  | total loss: \u001B[1m\u001B[32m0.02037\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 619 | loss: 0.02037 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 620  | total loss: \u001B[1m\u001B[32m0.01976\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 620 | loss: 0.01976 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 621  | total loss: \u001B[1m\u001B[32m0.01921\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 621 | loss: 0.01921 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 622  | total loss: \u001B[1m\u001B[32m0.01870\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 622 | loss: 0.01870 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 623  | total loss: \u001B[1m\u001B[32m0.01823\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 623 | loss: 0.01823 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 624  | total loss: \u001B[1m\u001B[32m0.01780\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 624 | loss: 0.01780 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 625  | total loss: \u001B[1m\u001B[32m0.01740\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 625 | loss: 0.01740 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 626  | total loss: \u001B[1m\u001B[32m0.01704\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 626 | loss: 0.01704 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 627  | total loss: \u001B[1m\u001B[32m0.01670\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 627 | loss: 0.01670 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 628  | total loss: \u001B[1m\u001B[32m0.01639\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 628 | loss: 0.01639 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 629  | total loss: \u001B[1m\u001B[32m0.01610\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 629 | loss: 0.01610 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 630  | total loss: \u001B[1m\u001B[32m0.01583\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 630 | loss: 0.01583 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 631  | total loss: \u001B[1m\u001B[32m0.01558\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 631 | loss: 0.01558 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 632  | total loss: \u001B[1m\u001B[32m0.01535\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 632 | loss: 0.01535 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 633  | total loss: \u001B[1m\u001B[32m0.01513\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 633 | loss: 0.01513 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 634  | total loss: \u001B[1m\u001B[32m0.01492\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 634 | loss: 0.01492 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 635  | total loss: \u001B[1m\u001B[32m0.01473\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 635 | loss: 0.01473 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 636  | total loss: \u001B[1m\u001B[32m0.01454\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 636 | loss: 0.01454 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 637  | total loss: \u001B[1m\u001B[32m0.01437\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 637 | loss: 0.01437 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 638  | total loss: \u001B[1m\u001B[32m0.01421\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 638 | loss: 0.01421 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 639  | total loss: \u001B[1m\u001B[32m0.01405\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 639 | loss: 0.01405 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 640  | total loss: \u001B[1m\u001B[32m0.01391\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 640 | loss: 0.01391 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 641  | total loss: \u001B[1m\u001B[32m0.01377\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 641 | loss: 0.01377 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 642  | total loss: \u001B[1m\u001B[32m0.22649\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 642 | loss: 0.22649 - acc: 0.9499 -- iter: 8/8\n",
      "--\n",
      "Training Step: 643  | total loss: \u001B[1m\u001B[32m0.20510\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 643 | loss: 0.20510 - acc: 0.9549 -- iter: 8/8\n",
      "--\n",
      "Training Step: 644  | total loss: \u001B[1m\u001B[32m0.18587\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 644 | loss: 0.18587 - acc: 0.9594 -- iter: 8/8\n",
      "--\n",
      "Training Step: 645  | total loss: \u001B[1m\u001B[32m0.16858\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 645 | loss: 0.16858 - acc: 0.9635 -- iter: 8/8\n",
      "--\n",
      "Training Step: 646  | total loss: \u001B[1m\u001B[32m0.15304\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 646 | loss: 0.15304 - acc: 0.9671 -- iter: 8/8\n",
      "--\n",
      "Training Step: 647  | total loss: \u001B[1m\u001B[32m0.13906\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 647 | loss: 0.13906 - acc: 0.9704 -- iter: 8/8\n",
      "--\n",
      "Training Step: 648  | total loss: \u001B[1m\u001B[32m0.12648\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 648 | loss: 0.12648 - acc: 0.9734 -- iter: 8/8\n",
      "--\n",
      "Training Step: 649  | total loss: \u001B[1m\u001B[32m0.11518\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 649 | loss: 0.11518 - acc: 0.9760 -- iter: 8/8\n",
      "--\n",
      "Training Step: 650  | total loss: \u001B[1m\u001B[32m0.10501\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 650 | loss: 0.10501 - acc: 0.9784 -- iter: 8/8\n",
      "--\n",
      "Training Step: 651  | total loss: \u001B[1m\u001B[32m0.09586\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 651 | loss: 0.09586 - acc: 0.9806 -- iter: 8/8\n",
      "--\n",
      "Training Step: 652  | total loss: \u001B[1m\u001B[32m0.08763\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 652 | loss: 0.08763 - acc: 0.9825 -- iter: 8/8\n",
      "--\n",
      "Training Step: 653  | total loss: \u001B[1m\u001B[32m0.08023\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 653 | loss: 0.08023 - acc: 0.9843 -- iter: 8/8\n",
      "--\n",
      "Training Step: 654  | total loss: \u001B[1m\u001B[32m0.07357\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 654 | loss: 0.07357 - acc: 0.9858 -- iter: 8/8\n",
      "--\n",
      "Training Step: 655  | total loss: \u001B[1m\u001B[32m0.06757\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 655 | loss: 0.06757 - acc: 0.9873 -- iter: 8/8\n",
      "--\n",
      "Training Step: 656  | total loss: \u001B[1m\u001B[32m0.06217\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 656 | loss: 0.06217 - acc: 0.9885 -- iter: 8/8\n",
      "--\n",
      "Training Step: 657  | total loss: \u001B[1m\u001B[32m0.05732\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 657 | loss: 0.05732 - acc: 0.9897 -- iter: 8/8\n",
      "--\n",
      "Training Step: 658  | total loss: \u001B[1m\u001B[32m0.05294\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 658 | loss: 0.05294 - acc: 0.9907 -- iter: 8/8\n",
      "--\n",
      "Training Step: 659  | total loss: \u001B[1m\u001B[32m0.04901\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 659 | loss: 0.04901 - acc: 0.9916 -- iter: 8/8\n",
      "--\n",
      "Training Step: 660  | total loss: \u001B[1m\u001B[32m0.04546\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 660 | loss: 0.04546 - acc: 0.9925 -- iter: 8/8\n",
      "--\n",
      "Training Step: 661  | total loss: \u001B[1m\u001B[32m0.04226\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 661 | loss: 0.04226 - acc: 0.9932 -- iter: 8/8\n",
      "--\n",
      "Training Step: 662  | total loss: \u001B[1m\u001B[32m0.03938\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 662 | loss: 0.03938 - acc: 0.9939 -- iter: 8/8\n",
      "--\n",
      "Training Step: 663  | total loss: \u001B[1m\u001B[32m0.03678\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 663 | loss: 0.03678 - acc: 0.9945 -- iter: 8/8\n",
      "--\n",
      "Training Step: 664  | total loss: \u001B[1m\u001B[32m0.03444\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 664 | loss: 0.03444 - acc: 0.9951 -- iter: 8/8\n",
      "--\n",
      "Training Step: 665  | total loss: \u001B[1m\u001B[32m0.03233\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 665 | loss: 0.03233 - acc: 0.9956 -- iter: 8/8\n",
      "--\n",
      "Training Step: 666  | total loss: \u001B[1m\u001B[32m0.25035\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 666 | loss: 0.25035 - acc: 0.9460 -- iter: 8/8\n",
      "--\n",
      "Training Step: 667  | total loss: \u001B[1m\u001B[32m0.22667\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 667 | loss: 0.22667 - acc: 0.9514 -- iter: 8/8\n",
      "--\n",
      "Training Step: 668  | total loss: \u001B[1m\u001B[32m0.20537\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 668 | loss: 0.20537 - acc: 0.9563 -- iter: 8/8\n",
      "--\n",
      "Training Step: 669  | total loss: \u001B[1m\u001B[32m0.18623\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 669 | loss: 0.18623 - acc: 0.9606 -- iter: 8/8\n",
      "--\n",
      "Training Step: 670  | total loss: \u001B[1m\u001B[32m0.16902\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 670 | loss: 0.16902 - acc: 0.9646 -- iter: 8/8\n",
      "--\n",
      "Training Step: 671  | total loss: \u001B[1m\u001B[32m0.15354\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 671 | loss: 0.15354 - acc: 0.9681 -- iter: 8/8\n",
      "--\n",
      "Training Step: 672  | total loss: \u001B[1m\u001B[32m0.35761\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 672 | loss: 0.35761 - acc: 0.9213 -- iter: 8/8\n",
      "--\n",
      "Training Step: 673  | total loss: \u001B[1m\u001B[32m0.32334\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 673 | loss: 0.32334 - acc: 0.9292 -- iter: 8/8\n",
      "--\n",
      "Training Step: 674  | total loss: \u001B[1m\u001B[32m0.29252\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 674 | loss: 0.29252 - acc: 0.9363 -- iter: 8/8\n",
      "--\n",
      "Training Step: 675  | total loss: \u001B[1m\u001B[32m0.26483\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 675 | loss: 0.26483 - acc: 0.9426 -- iter: 8/8\n",
      "--\n",
      "Training Step: 676  | total loss: \u001B[1m\u001B[32m0.23993\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 676 | loss: 0.23993 - acc: 0.9484 -- iter: 8/8\n",
      "--\n",
      "Training Step: 677  | total loss: \u001B[1m\u001B[32m0.21755\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 677 | loss: 0.21755 - acc: 0.9535 -- iter: 8/8\n",
      "--\n",
      "Training Step: 678  | total loss: \u001B[1m\u001B[32m0.19744\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 678 | loss: 0.19744 - acc: 0.9582 -- iter: 8/8\n",
      "--\n",
      "Training Step: 679  | total loss: \u001B[1m\u001B[32m0.17936\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 679 | loss: 0.17936 - acc: 0.9624 -- iter: 8/8\n",
      "--\n",
      "Training Step: 680  | total loss: \u001B[1m\u001B[32m0.38047\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 680 | loss: 0.38047 - acc: 0.9161 -- iter: 8/8\n",
      "--\n",
      "Training Step: 681  | total loss: \u001B[1m\u001B[32m0.34416\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 681 | loss: 0.34416 - acc: 0.9245 -- iter: 8/8\n",
      "--\n",
      "Training Step: 682  | total loss: \u001B[1m\u001B[32m0.31152\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 682 | loss: 0.31152 - acc: 0.9321 -- iter: 8/8\n",
      "--\n",
      "Training Step: 683  | total loss: \u001B[1m\u001B[32m0.28218\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 683 | loss: 0.28218 - acc: 0.9389 -- iter: 8/8\n",
      "--\n",
      "Training Step: 684  | total loss: \u001B[1m\u001B[32m0.25582\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 684 | loss: 0.25582 - acc: 0.9450 -- iter: 8/8\n",
      "--\n",
      "Training Step: 685  | total loss: \u001B[1m\u001B[32m0.23213\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 685 | loss: 0.23213 - acc: 0.9505 -- iter: 8/8\n",
      "--\n",
      "Training Step: 686  | total loss: \u001B[1m\u001B[32m0.21083\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 686 | loss: 0.21083 - acc: 0.9554 -- iter: 8/8\n",
      "--\n",
      "Training Step: 687  | total loss: \u001B[1m\u001B[32m0.19169\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 687 | loss: 0.19169 - acc: 0.9599 -- iter: 8/8\n",
      "--\n",
      "Training Step: 688  | total loss: \u001B[1m\u001B[32m0.17449\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 688 | loss: 0.17449 - acc: 0.9639 -- iter: 8/8\n",
      "--\n",
      "Training Step: 689  | total loss: \u001B[1m\u001B[32m0.15902\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 689 | loss: 0.15902 - acc: 0.9675 -- iter: 8/8\n",
      "--\n",
      "Training Step: 690  | total loss: \u001B[1m\u001B[32m0.14511\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 690 | loss: 0.14511 - acc: 0.9708 -- iter: 8/8\n",
      "--\n",
      "Training Step: 691  | total loss: \u001B[1m\u001B[32m0.13261\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 691 | loss: 0.13261 - acc: 0.9737 -- iter: 8/8\n",
      "--\n",
      "Training Step: 692  | total loss: \u001B[1m\u001B[32m0.12137\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 692 | loss: 0.12137 - acc: 0.9763 -- iter: 8/8\n",
      "--\n",
      "Training Step: 693  | total loss: \u001B[1m\u001B[32m0.11125\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 693 | loss: 0.11125 - acc: 0.9787 -- iter: 8/8\n",
      "--\n",
      "Training Step: 694  | total loss: \u001B[1m\u001B[32m0.10215\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 694 | loss: 0.10215 - acc: 0.9808 -- iter: 8/8\n",
      "--\n",
      "Training Step: 695  | total loss: \u001B[1m\u001B[32m0.09397\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 695 | loss: 0.09397 - acc: 0.9827 -- iter: 8/8\n",
      "--\n",
      "Training Step: 696  | total loss: \u001B[1m\u001B[32m0.08660\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 696 | loss: 0.08660 - acc: 0.9845 -- iter: 8/8\n",
      "--\n",
      "Training Step: 697  | total loss: \u001B[1m\u001B[32m0.07997\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 697 | loss: 0.07997 - acc: 0.9860 -- iter: 8/8\n",
      "--\n",
      "Training Step: 698  | total loss: \u001B[1m\u001B[32m0.07400\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 698 | loss: 0.07400 - acc: 0.9874 -- iter: 8/8\n",
      "--\n",
      "Training Step: 699  | total loss: \u001B[1m\u001B[32m0.06863\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 699 | loss: 0.06863 - acc: 0.9887 -- iter: 8/8\n",
      "--\n",
      "Training Step: 700  | total loss: \u001B[1m\u001B[32m0.06378\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 700 | loss: 0.06378 - acc: 0.9898 -- iter: 8/8\n",
      "--\n",
      "Training Step: 701  | total loss: \u001B[1m\u001B[32m0.05942\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 701 | loss: 0.05942 - acc: 0.9908 -- iter: 8/8\n",
      "--\n",
      "Training Step: 702  | total loss: \u001B[1m\u001B[32m0.05548\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 702 | loss: 0.05548 - acc: 0.9917 -- iter: 8/8\n",
      "--\n",
      "Training Step: 703  | total loss: \u001B[1m\u001B[32m0.05193\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 703 | loss: 0.05193 - acc: 0.9926 -- iter: 8/8\n",
      "--\n",
      "Training Step: 704  | total loss: \u001B[1m\u001B[32m0.04872\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 704 | loss: 0.04872 - acc: 0.9933 -- iter: 8/8\n",
      "--\n",
      "Training Step: 705  | total loss: \u001B[1m\u001B[32m0.04583\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 705 | loss: 0.04583 - acc: 0.9940 -- iter: 8/8\n",
      "--\n",
      "Training Step: 706  | total loss: \u001B[1m\u001B[32m0.04322\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 706 | loss: 0.04322 - acc: 0.9946 -- iter: 8/8\n",
      "--\n",
      "Training Step: 707  | total loss: \u001B[1m\u001B[32m0.04085\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 707 | loss: 0.04085 - acc: 0.9951 -- iter: 8/8\n",
      "--\n",
      "Training Step: 708  | total loss: \u001B[1m\u001B[32m0.03872\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 708 | loss: 0.03872 - acc: 0.9956 -- iter: 8/8\n",
      "--\n",
      "Training Step: 709  | total loss: \u001B[1m\u001B[32m0.03678\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 709 | loss: 0.03678 - acc: 0.9960 -- iter: 8/8\n",
      "--\n",
      "Training Step: 710  | total loss: \u001B[1m\u001B[32m0.23996\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 710 | loss: 0.23996 - acc: 0.9464 -- iter: 8/8\n",
      "--\n",
      "Training Step: 711  | total loss: \u001B[1m\u001B[32m0.21792\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 711 | loss: 0.21792 - acc: 0.9518 -- iter: 8/8\n",
      "--\n",
      "Training Step: 712  | total loss: \u001B[1m\u001B[32m0.19810\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 712 | loss: 0.19810 - acc: 0.9566 -- iter: 8/8\n",
      "--\n",
      "Training Step: 713  | total loss: \u001B[1m\u001B[32m0.18029\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 713 | loss: 0.18029 - acc: 0.9610 -- iter: 8/8\n",
      "--\n",
      "Training Step: 714  | total loss: \u001B[1m\u001B[32m0.16428\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 714 | loss: 0.16428 - acc: 0.9649 -- iter: 8/8\n",
      "--\n",
      "Training Step: 715  | total loss: \u001B[1m\u001B[32m0.14989\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 715 | loss: 0.14989 - acc: 0.9684 -- iter: 8/8\n",
      "--\n",
      "Training Step: 716  | total loss: \u001B[1m\u001B[32m0.13694\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 716 | loss: 0.13694 - acc: 0.9715 -- iter: 8/8\n",
      "--\n",
      "Training Step: 717  | total loss: \u001B[1m\u001B[32m0.12530\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 717 | loss: 0.12530 - acc: 0.9744 -- iter: 8/8\n",
      "--\n",
      "Training Step: 718  | total loss: \u001B[1m\u001B[32m0.11483\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 718 | loss: 0.11483 - acc: 0.9769 -- iter: 8/8\n",
      "--\n",
      "Training Step: 719  | total loss: \u001B[1m\u001B[32m0.10542\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 719 | loss: 0.10542 - acc: 0.9793 -- iter: 8/8\n",
      "--\n",
      "Training Step: 720  | total loss: \u001B[1m\u001B[32m0.09694\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 720 | loss: 0.09694 - acc: 0.9813 -- iter: 8/8\n",
      "--\n",
      "Training Step: 721  | total loss: \u001B[1m\u001B[32m0.08932\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 721 | loss: 0.08932 - acc: 0.9832 -- iter: 8/8\n",
      "--\n",
      "Training Step: 722  | total loss: \u001B[1m\u001B[32m0.08245\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 722 | loss: 0.08245 - acc: 0.9849 -- iter: 8/8\n",
      "--\n",
      "Training Step: 723  | total loss: \u001B[1m\u001B[32m0.07627\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 723 | loss: 0.07627 - acc: 0.9864 -- iter: 8/8\n",
      "--\n",
      "Training Step: 724  | total loss: \u001B[1m\u001B[32m0.07070\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 724 | loss: 0.07070 - acc: 0.9877 -- iter: 8/8\n",
      "--\n",
      "Training Step: 725  | total loss: \u001B[1m\u001B[32m0.06569\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 725 | loss: 0.06569 - acc: 0.9890 -- iter: 8/8\n",
      "--\n",
      "Training Step: 726  | total loss: \u001B[1m\u001B[32m0.06117\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 726 | loss: 0.06117 - acc: 0.9901 -- iter: 8/8\n",
      "--\n",
      "Training Step: 727  | total loss: \u001B[1m\u001B[32m0.05709\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 727 | loss: 0.05709 - acc: 0.9911 -- iter: 8/8\n",
      "--\n",
      "Training Step: 728  | total loss: \u001B[1m\u001B[32m0.05342\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 728 | loss: 0.05342 - acc: 0.9920 -- iter: 8/8\n",
      "--\n",
      "Training Step: 729  | total loss: \u001B[1m\u001B[32m0.05010\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 729 | loss: 0.05010 - acc: 0.9928 -- iter: 8/8\n",
      "--\n",
      "Training Step: 730  | total loss: \u001B[1m\u001B[32m0.04710\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 730 | loss: 0.04710 - acc: 0.9935 -- iter: 8/8\n",
      "--\n",
      "Training Step: 731  | total loss: \u001B[1m\u001B[32m0.04440\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 731 | loss: 0.04440 - acc: 0.9941 -- iter: 8/8\n",
      "--\n",
      "Training Step: 732  | total loss: \u001B[1m\u001B[32m0.04195\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 732 | loss: 0.04195 - acc: 0.9947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 733  | total loss: \u001B[1m\u001B[32m0.03974\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 733 | loss: 0.03974 - acc: 0.9953 -- iter: 8/8\n",
      "--\n",
      "Training Step: 734  | total loss: \u001B[1m\u001B[32m0.03773\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 734 | loss: 0.03773 - acc: 0.9957 -- iter: 8/8\n",
      "--\n",
      "Training Step: 735  | total loss: \u001B[1m\u001B[32m0.03592\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 735 | loss: 0.03592 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 736  | total loss: \u001B[1m\u001B[32m0.03427\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 736 | loss: 0.03427 - acc: 0.9965 -- iter: 8/8\n",
      "--\n",
      "Training Step: 737  | total loss: \u001B[1m\u001B[32m0.03278\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 737 | loss: 0.03278 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 738  | total loss: \u001B[1m\u001B[32m0.03142\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 738 | loss: 0.03142 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 739  | total loss: \u001B[1m\u001B[32m0.03019\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 739 | loss: 0.03019 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 740  | total loss: \u001B[1m\u001B[32m0.02906\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 740 | loss: 0.02906 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 741  | total loss: \u001B[1m\u001B[32m0.02804\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 741 | loss: 0.02804 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 742  | total loss: \u001B[1m\u001B[32m0.02710\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 742 | loss: 0.02710 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 743  | total loss: \u001B[1m\u001B[32m0.02625\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 743 | loss: 0.02625 - acc: 0.9983 -- iter: 8/8\n",
      "--\n",
      "Training Step: 744  | total loss: \u001B[1m\u001B[32m0.02547\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 744 | loss: 0.02547 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 745  | total loss: \u001B[1m\u001B[32m0.02475\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 745 | loss: 0.02475 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 746  | total loss: \u001B[1m\u001B[32m0.02409\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 746 | loss: 0.02409 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 747  | total loss: \u001B[1m\u001B[32m0.02348\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 747 | loss: 0.02348 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 748  | total loss: \u001B[1m\u001B[32m0.02292\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 748 | loss: 0.02292 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 749  | total loss: \u001B[1m\u001B[32m0.02241\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 749 | loss: 0.02241 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 750  | total loss: \u001B[1m\u001B[32m0.02193\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 750 | loss: 0.02193 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 751  | total loss: \u001B[1m\u001B[32m0.02149\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 751 | loss: 0.02149 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 752  | total loss: \u001B[1m\u001B[32m0.02108\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 752 | loss: 0.02108 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 753  | total loss: \u001B[1m\u001B[32m0.02069\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 753 | loss: 0.02069 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 754  | total loss: \u001B[1m\u001B[32m0.02034\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 754 | loss: 0.02034 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 755  | total loss: \u001B[1m\u001B[32m0.02000\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 755 | loss: 0.02000 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 756  | total loss: \u001B[1m\u001B[32m0.01969\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 756 | loss: 0.01969 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 757  | total loss: \u001B[1m\u001B[32m0.01939\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 757 | loss: 0.01939 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 758  | total loss: \u001B[1m\u001B[32m0.01912\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 758 | loss: 0.01912 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 759  | total loss: \u001B[1m\u001B[32m0.01885\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 759 | loss: 0.01885 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 760  | total loss: \u001B[1m\u001B[32m0.01861\u001B[0m\u001B[0m | time: 0.012s\n",
      "| Adam | epoch: 760 | loss: 0.01861 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 761  | total loss: \u001B[1m\u001B[32m0.01815\u001B[0m\u001B[0m | time: 0.015s\n",
      "| Adam | epoch: 761 | loss: 0.01815 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 762  | total loss: \u001B[1m\u001B[32m0.01815\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 762 | loss: 0.01815 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 763  | total loss: \u001B[1m\u001B[32m0.01794\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 763 | loss: 0.01794 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 764  | total loss: \u001B[1m\u001B[32m0.01773\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 764 | loss: 0.01773 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 765  | total loss: \u001B[1m\u001B[32m0.01754\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 765 | loss: 0.01754 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 766  | total loss: \u001B[1m\u001B[32m0.01735\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 766 | loss: 0.01735 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 767  | total loss: \u001B[1m\u001B[32m0.01717\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 767 | loss: 0.01717 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 768  | total loss: \u001B[1m\u001B[32m0.01700\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 768 | loss: 0.01700 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 769  | total loss: \u001B[1m\u001B[32m0.01683\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 769 | loss: 0.01683 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 770  | total loss: \u001B[1m\u001B[32m0.01667\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 770 | loss: 0.01667 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 771  | total loss: \u001B[1m\u001B[32m0.01652\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 771 | loss: 0.01652 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 772  | total loss: \u001B[1m\u001B[32m0.01637\u001B[0m\u001B[0m | time: 0.012s\n",
      "| Adam | epoch: 772 | loss: 0.01637 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 773  | total loss: \u001B[1m\u001B[32m0.01622\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 773 | loss: 0.01622 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 774  | total loss: \u001B[1m\u001B[32m0.01608\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 774 | loss: 0.01608 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 775  | total loss: \u001B[1m\u001B[32m0.01594\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 775 | loss: 0.01594 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 776  | total loss: \u001B[1m\u001B[32m0.01580\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 776 | loss: 0.01580 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 777  | total loss: \u001B[1m\u001B[32m0.01567\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 777 | loss: 0.01567 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 778  | total loss: \u001B[1m\u001B[32m0.01554\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 778 | loss: 0.01554 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 779  | total loss: \u001B[1m\u001B[32m0.01542\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 779 | loss: 0.01542 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 780  | total loss: \u001B[1m\u001B[32m0.01530\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 780 | loss: 0.01530 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 781  | total loss: \u001B[1m\u001B[32m0.01518\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 781 | loss: 0.01518 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 782  | total loss: \u001B[1m\u001B[32m0.22628\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 782 | loss: 0.22628 - acc: 0.9500 -- iter: 8/8\n",
      "--\n",
      "Training Step: 783  | total loss: \u001B[1m\u001B[32m0.20508\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 783 | loss: 0.20508 - acc: 0.9550 -- iter: 8/8\n",
      "--\n",
      "Training Step: 784  | total loss: \u001B[1m\u001B[32m0.40416\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 784 | loss: 0.40416 - acc: 0.9095 -- iter: 8/8\n",
      "--\n",
      "Training Step: 785  | total loss: \u001B[1m\u001B[32m0.36524\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 785 | loss: 0.36524 - acc: 0.9185 -- iter: 8/8\n",
      "--\n",
      "Training Step: 786  | total loss: \u001B[1m\u001B[32m0.33026\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 786 | loss: 0.33026 - acc: 0.9267 -- iter: 8/8\n",
      "--\n",
      "Training Step: 787  | total loss: \u001B[1m\u001B[32m0.29882\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 787 | loss: 0.29882 - acc: 0.9340 -- iter: 8/8\n",
      "--\n",
      "Training Step: 788  | total loss: \u001B[1m\u001B[32m0.27056\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 788 | loss: 0.27056 - acc: 0.9406 -- iter: 8/8\n",
      "--\n",
      "Training Step: 789  | total loss: \u001B[1m\u001B[32m0.24516\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 789 | loss: 0.24516 - acc: 0.9465 -- iter: 8/8\n",
      "--\n",
      "Training Step: 790  | total loss: \u001B[1m\u001B[32m0.22233\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 790 | loss: 0.22233 - acc: 0.9519 -- iter: 8/8\n",
      "--\n",
      "Training Step: 791  | total loss: \u001B[1m\u001B[32m0.20181\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 791 | loss: 0.20181 - acc: 0.9567 -- iter: 8/8\n",
      "--\n",
      "Training Step: 792  | total loss: \u001B[1m\u001B[32m0.18336\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 792 | loss: 0.18336 - acc: 0.9610 -- iter: 8/8\n",
      "--\n",
      "Training Step: 793  | total loss: \u001B[1m\u001B[32m0.16678\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 793 | loss: 0.16678 - acc: 0.9649 -- iter: 8/8\n",
      "--\n",
      "Training Step: 794  | total loss: \u001B[1m\u001B[32m0.15187\u001B[0m\u001B[0m | time: 0.012s\n",
      "| Adam | epoch: 794 | loss: 0.15187 - acc: 0.9684 -- iter: 8/8\n",
      "--\n",
      "Training Step: 795  | total loss: \u001B[1m\u001B[32m0.13847\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 795 | loss: 0.13847 - acc: 0.9716 -- iter: 8/8\n",
      "--\n",
      "Training Step: 796  | total loss: \u001B[1m\u001B[32m0.12642\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 796 | loss: 0.12642 - acc: 0.9744 -- iter: 8/8\n",
      "--\n",
      "Training Step: 797  | total loss: \u001B[1m\u001B[32m0.11558\u001B[0m\u001B[0m | time: 0.013s\n",
      "| Adam | epoch: 797 | loss: 0.11558 - acc: 0.9770 -- iter: 8/8\n",
      "--\n",
      "Training Step: 798  | total loss: \u001B[1m\u001B[32m0.10583\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 798 | loss: 0.10583 - acc: 0.9793 -- iter: 8/8\n",
      "--\n",
      "Training Step: 799  | total loss: \u001B[1m\u001B[32m0.09706\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 799 | loss: 0.09706 - acc: 0.9814 -- iter: 8/8\n",
      "--\n",
      "Training Step: 800  | total loss: \u001B[1m\u001B[32m0.08917\u001B[0m\u001B[0m | time: 0.018s\n",
      "| Adam | epoch: 800 | loss: 0.08917 - acc: 0.9832 -- iter: 8/8\n",
      "--\n",
      "Training Step: 801  | total loss: \u001B[1m\u001B[32m0.08207\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 801 | loss: 0.08207 - acc: 0.9849 -- iter: 8/8\n",
      "--\n",
      "Training Step: 802  | total loss: \u001B[1m\u001B[32m0.07568\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 802 | loss: 0.07568 - acc: 0.9864 -- iter: 8/8\n",
      "--\n",
      "Training Step: 803  | total loss: \u001B[1m\u001B[32m0.06993\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 803 | loss: 0.06993 - acc: 0.9878 -- iter: 8/8\n",
      "--\n",
      "Training Step: 804  | total loss: \u001B[1m\u001B[32m0.06475\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 804 | loss: 0.06475 - acc: 0.9890 -- iter: 8/8\n",
      "--\n",
      "Training Step: 805  | total loss: \u001B[1m\u001B[32m0.06009\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 805 | loss: 0.06009 - acc: 0.9901 -- iter: 8/8\n",
      "--\n",
      "Training Step: 806  | total loss: \u001B[1m\u001B[32m0.05588\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 806 | loss: 0.05588 - acc: 0.9911 -- iter: 8/8\n",
      "--\n",
      "Training Step: 807  | total loss: \u001B[1m\u001B[32m0.05209\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 807 | loss: 0.05209 - acc: 0.9920 -- iter: 8/8\n",
      "--\n",
      "Training Step: 808  | total loss: \u001B[1m\u001B[32m0.04867\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 808 | loss: 0.04867 - acc: 0.9928 -- iter: 8/8\n",
      "--\n",
      "Training Step: 809  | total loss: \u001B[1m\u001B[32m0.04559\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 809 | loss: 0.04559 - acc: 0.9935 -- iter: 8/8\n",
      "--\n",
      "Training Step: 810  | total loss: \u001B[1m\u001B[32m0.04281\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 810 | loss: 0.04281 - acc: 0.9942 -- iter: 8/8\n",
      "--\n",
      "Training Step: 811  | total loss: \u001B[1m\u001B[32m0.04030\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 811 | loss: 0.04030 - acc: 0.9947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 812  | total loss: \u001B[1m\u001B[32m0.03803\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 812 | loss: 0.03803 - acc: 0.9953 -- iter: 8/8\n",
      "--\n",
      "Training Step: 813  | total loss: \u001B[1m\u001B[32m0.03597\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 813 | loss: 0.03597 - acc: 0.9957 -- iter: 8/8\n",
      "--\n",
      "Training Step: 814  | total loss: \u001B[1m\u001B[32m0.03412\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 814 | loss: 0.03412 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 815  | total loss: \u001B[1m\u001B[32m0.03244\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 815 | loss: 0.03244 - acc: 0.9965 -- iter: 8/8\n",
      "--\n",
      "Training Step: 816  | total loss: \u001B[1m\u001B[32m0.03091\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 816 | loss: 0.03091 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 817  | total loss: \u001B[1m\u001B[32m0.02953\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 817 | loss: 0.02953 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 818  | total loss: \u001B[1m\u001B[32m0.02828\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 818 | loss: 0.02828 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 819  | total loss: \u001B[1m\u001B[32m0.02714\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 819 | loss: 0.02714 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 820  | total loss: \u001B[1m\u001B[32m0.02610\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 820 | loss: 0.02610 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 821  | total loss: \u001B[1m\u001B[32m0.02516\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 821 | loss: 0.02516 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 822  | total loss: \u001B[1m\u001B[32m0.02430\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 822 | loss: 0.02430 - acc: 0.9983 -- iter: 8/8\n",
      "--\n",
      "Training Step: 823  | total loss: \u001B[1m\u001B[32m0.02352\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 823 | loss: 0.02352 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 824  | total loss: \u001B[1m\u001B[32m0.02280\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 824 | loss: 0.02280 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 825  | total loss: \u001B[1m\u001B[32m0.02214\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 825 | loss: 0.02214 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 826  | total loss: \u001B[1m\u001B[32m0.02154\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 826 | loss: 0.02154 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 827  | total loss: \u001B[1m\u001B[32m0.02099\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 827 | loss: 0.02099 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 828  | total loss: \u001B[1m\u001B[32m0.02048\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 828 | loss: 0.02048 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 829  | total loss: \u001B[1m\u001B[32m0.02001\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 829 | loss: 0.02001 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 830  | total loss: \u001B[1m\u001B[32m0.01958\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 830 | loss: 0.01958 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 831  | total loss: \u001B[1m\u001B[32m0.01918\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 831 | loss: 0.01918 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 832  | total loss: \u001B[1m\u001B[32m0.01881\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 832 | loss: 0.01881 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 833  | total loss: \u001B[1m\u001B[32m0.01846\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 833 | loss: 0.01846 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 834  | total loss: \u001B[1m\u001B[32m0.01814\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 834 | loss: 0.01814 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 835  | total loss: \u001B[1m\u001B[32m0.01784\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 835 | loss: 0.01784 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 836  | total loss: \u001B[1m\u001B[32m0.01756\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 836 | loss: 0.01756 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 837  | total loss: \u001B[1m\u001B[32m0.01730\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 837 | loss: 0.01730 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 838  | total loss: \u001B[1m\u001B[32m0.01705\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 838 | loss: 0.01705 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 839  | total loss: \u001B[1m\u001B[32m0.01682\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 839 | loss: 0.01682 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 840  | total loss: \u001B[1m\u001B[32m0.01660\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 840 | loss: 0.01660 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 841  | total loss: \u001B[1m\u001B[32m0.01639\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 841 | loss: 0.01639 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 842  | total loss: \u001B[1m\u001B[32m0.01619\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 842 | loss: 0.01619 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 843  | total loss: \u001B[1m\u001B[32m0.01601\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 843 | loss: 0.01601 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 844  | total loss: \u001B[1m\u001B[32m0.01583\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 844 | loss: 0.01583 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 845  | total loss: \u001B[1m\u001B[32m0.01566\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 845 | loss: 0.01566 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 846  | total loss: \u001B[1m\u001B[32m0.01549\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 846 | loss: 0.01549 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 847  | total loss: \u001B[1m\u001B[32m0.01534\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 847 | loss: 0.01534 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 848  | total loss: \u001B[1m\u001B[32m0.01519\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 848 | loss: 0.01519 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 849  | total loss: \u001B[1m\u001B[32m0.01504\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 849 | loss: 0.01504 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 850  | total loss: \u001B[1m\u001B[32m0.01490\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 850 | loss: 0.01490 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 851  | total loss: \u001B[1m\u001B[32m0.01477\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 851 | loss: 0.01477 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 852  | total loss: \u001B[1m\u001B[32m0.01464\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 852 | loss: 0.01464 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 853  | total loss: \u001B[1m\u001B[32m0.01451\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 853 | loss: 0.01451 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 854  | total loss: \u001B[1m\u001B[32m0.23270\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 854 | loss: 0.23270 - acc: 0.9499 -- iter: 8/8\n",
      "--\n",
      "Training Step: 855  | total loss: \u001B[1m\u001B[32m0.21078\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 855 | loss: 0.21078 - acc: 0.9549 -- iter: 8/8\n",
      "--\n",
      "Training Step: 856  | total loss: \u001B[1m\u001B[32m0.19108\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 856 | loss: 0.19108 - acc: 0.9595 -- iter: 8/8\n",
      "--\n",
      "Training Step: 857  | total loss: \u001B[1m\u001B[32m0.17336\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 857 | loss: 0.17336 - acc: 0.9635 -- iter: 8/8\n",
      "--\n",
      "Training Step: 858  | total loss: \u001B[1m\u001B[32m0.15744\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 858 | loss: 0.15744 - acc: 0.9672 -- iter: 8/8\n",
      "--\n",
      "Training Step: 859  | total loss: \u001B[1m\u001B[32m0.14312\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 859 | loss: 0.14312 - acc: 0.9704 -- iter: 8/8\n",
      "--\n",
      "Training Step: 860  | total loss: \u001B[1m\u001B[32m0.13024\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 860 | loss: 0.13024 - acc: 0.9734 -- iter: 8/8\n",
      "--\n",
      "Training Step: 861  | total loss: \u001B[1m\u001B[32m0.11866\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 861 | loss: 0.11866 - acc: 0.9761 -- iter: 8/8\n",
      "--\n",
      "Training Step: 862  | total loss: \u001B[1m\u001B[32m0.10825\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 862 | loss: 0.10825 - acc: 0.9785 -- iter: 8/8\n",
      "--\n",
      "Training Step: 863  | total loss: \u001B[1m\u001B[32m0.09888\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 863 | loss: 0.09888 - acc: 0.9806 -- iter: 8/8\n",
      "--\n",
      "Training Step: 864  | total loss: \u001B[1m\u001B[32m0.09046\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 864 | loss: 0.09046 - acc: 0.9825 -- iter: 8/8\n",
      "--\n",
      "Training Step: 865  | total loss: \u001B[1m\u001B[32m0.08288\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 865 | loss: 0.08288 - acc: 0.9843 -- iter: 8/8\n",
      "--\n",
      "Training Step: 866  | total loss: \u001B[1m\u001B[32m0.07606\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 866 | loss: 0.07606 - acc: 0.9859 -- iter: 8/8\n",
      "--\n",
      "Training Step: 867  | total loss: \u001B[1m\u001B[32m0.06993\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 867 | loss: 0.06993 - acc: 0.9873 -- iter: 8/8\n",
      "--\n",
      "Training Step: 868  | total loss: \u001B[1m\u001B[32m0.06441\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 868 | loss: 0.06441 - acc: 0.9885 -- iter: 8/8\n",
      "--\n",
      "Training Step: 869  | total loss: \u001B[1m\u001B[32m0.05943\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 869 | loss: 0.05943 - acc: 0.9897 -- iter: 8/8\n",
      "--\n",
      "Training Step: 870  | total loss: \u001B[1m\u001B[32m0.05496\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 870 | loss: 0.05496 - acc: 0.9907 -- iter: 8/8\n",
      "--\n",
      "Training Step: 871  | total loss: \u001B[1m\u001B[32m0.05093\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 871 | loss: 0.05093 - acc: 0.9917 -- iter: 8/8\n",
      "--\n",
      "Training Step: 872  | total loss: \u001B[1m\u001B[32m0.04729\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 872 | loss: 0.04729 - acc: 0.9925 -- iter: 8/8\n",
      "--\n",
      "Training Step: 873  | total loss: \u001B[1m\u001B[32m0.04402\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 873 | loss: 0.04402 - acc: 0.9932 -- iter: 8/8\n",
      "--\n",
      "Training Step: 874  | total loss: \u001B[1m\u001B[32m0.04107\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 874 | loss: 0.04107 - acc: 0.9939 -- iter: 8/8\n",
      "--\n",
      "Training Step: 875  | total loss: \u001B[1m\u001B[32m0.03841\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 875 | loss: 0.03841 - acc: 0.9945 -- iter: 8/8\n",
      "--\n",
      "Training Step: 876  | total loss: \u001B[1m\u001B[32m0.24259\u001B[0m\u001B[0m | time: 0.013s\n",
      "| Adam | epoch: 876 | loss: 0.24259 - acc: 0.9451 -- iter: 8/8\n",
      "--\n",
      "Training Step: 877  | total loss: \u001B[1m\u001B[32m0.21980\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 877 | loss: 0.21980 - acc: 0.9506 -- iter: 8/8\n",
      "--\n",
      "Training Step: 878  | total loss: \u001B[1m\u001B[32m0.19932\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 878 | loss: 0.19932 - acc: 0.9555 -- iter: 8/8\n",
      "--\n",
      "Training Step: 879  | total loss: \u001B[1m\u001B[32m0.18090\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 879 | loss: 0.18090 - acc: 0.9600 -- iter: 8/8\n",
      "--\n",
      "Training Step: 880  | total loss: \u001B[1m\u001B[32m0.16435\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 880 | loss: 0.16435 - acc: 0.9640 -- iter: 8/8\n",
      "--\n",
      "Training Step: 881  | total loss: \u001B[1m\u001B[32m0.14947\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 881 | loss: 0.14947 - acc: 0.9676 -- iter: 8/8\n",
      "--\n",
      "Training Step: 882  | total loss: \u001B[1m\u001B[32m0.13609\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 882 | loss: 0.13609 - acc: 0.9708 -- iter: 8/8\n",
      "--\n",
      "Training Step: 883  | total loss: \u001B[1m\u001B[32m0.12406\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 883 | loss: 0.12406 - acc: 0.9737 -- iter: 8/8\n",
      "--\n",
      "Training Step: 884  | total loss: \u001B[1m\u001B[32m0.11324\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 884 | loss: 0.11324 - acc: 0.9764 -- iter: 8/8\n",
      "--\n",
      "Training Step: 885  | total loss: \u001B[1m\u001B[32m0.10352\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 885 | loss: 0.10352 - acc: 0.9787 -- iter: 8/8\n",
      "--\n",
      "Training Step: 886  | total loss: \u001B[1m\u001B[32m0.09477\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 886 | loss: 0.09477 - acc: 0.9808 -- iter: 8/8\n",
      "--\n",
      "Training Step: 887  | total loss: \u001B[1m\u001B[32m0.08690\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 887 | loss: 0.08690 - acc: 0.9828 -- iter: 8/8\n",
      "--\n",
      "Training Step: 888  | total loss: \u001B[1m\u001B[32m0.07982\u001B[0m\u001B[0m | time: 0.012s\n",
      "| Adam | epoch: 888 | loss: 0.07982 - acc: 0.9845 -- iter: 8/8\n",
      "--\n",
      "Training Step: 889  | total loss: \u001B[1m\u001B[32m0.07344\u001B[0m\u001B[0m | time: 0.020s\n",
      "| Adam | epoch: 889 | loss: 0.07344 - acc: 0.9860 -- iter: 8/8\n",
      "--\n",
      "Training Step: 890  | total loss: \u001B[1m\u001B[32m0.06771\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 890 | loss: 0.06771 - acc: 0.9887 -- iter: 8/8\n",
      "--\n",
      "Training Step: 891  | total loss: \u001B[1m\u001B[32m0.06255\u001B[0m\u001B[0m | time: 0.014s\n",
      "| Adam | epoch: 891 | loss: 0.06255 - acc: 0.9887 -- iter: 8/8\n",
      "--\n",
      "Training Step: 892  | total loss: \u001B[1m\u001B[32m0.05790\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 892 | loss: 0.05790 - acc: 0.9898 -- iter: 8/8\n",
      "--\n",
      "Training Step: 893  | total loss: \u001B[1m\u001B[32m0.05371\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 893 | loss: 0.05371 - acc: 0.9908 -- iter: 8/8\n",
      "--\n",
      "Training Step: 894  | total loss: \u001B[1m\u001B[32m0.04994\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 894 | loss: 0.04994 - acc: 0.9918 -- iter: 8/8\n",
      "--\n",
      "Training Step: 895  | total loss: \u001B[1m\u001B[32m0.04654\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 895 | loss: 0.04654 - acc: 0.9926 -- iter: 8/8\n",
      "--\n",
      "Training Step: 896  | total loss: \u001B[1m\u001B[32m0.04347\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 896 | loss: 0.04347 - acc: 0.9933 -- iter: 8/8\n",
      "--\n",
      "Training Step: 897  | total loss: \u001B[1m\u001B[32m0.03821\u001B[0m\u001B[0m | time: 0.012s\n",
      "| Adam | epoch: 897 | loss: 0.03821 - acc: 0.9940 -- iter: 8/8\n",
      "--\n",
      "Training Step: 898  | total loss: \u001B[1m\u001B[32m0.03821\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 898 | loss: 0.03821 - acc: 0.9946 -- iter: 8/8\n",
      "--\n",
      "Training Step: 899  | total loss: \u001B[1m\u001B[32m0.03596\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 899 | loss: 0.03596 - acc: 0.9951 -- iter: 8/8\n",
      "--\n",
      "Training Step: 900  | total loss: \u001B[1m\u001B[32m0.14053\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 900 | loss: 0.14053 - acc: 0.9706 -- iter: 8/8\n",
      "--\n",
      "Training Step: 901  | total loss: \u001B[1m\u001B[32m0.12805\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 901 | loss: 0.12805 - acc: 0.9762 -- iter: 8/8\n",
      "--\n",
      "Training Step: 902  | total loss: \u001B[1m\u001B[32m0.11682\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 902 | loss: 0.11682 - acc: 0.9762 -- iter: 8/8\n",
      "--\n",
      "Training Step: 903  | total loss: \u001B[1m\u001B[32m0.10672\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 903 | loss: 0.10672 - acc: 0.9786 -- iter: 8/8\n",
      "--\n",
      "Training Step: 904  | total loss: \u001B[1m\u001B[32m0.20529\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 904 | loss: 0.20529 - acc: 0.9557 -- iter: 8/8\n",
      "--\n",
      "Training Step: 905  | total loss: \u001B[1m\u001B[32m0.18636\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 905 | loss: 0.18636 - acc: 0.9602 -- iter: 8/8\n",
      "--\n",
      "Training Step: 906  | total loss: \u001B[1m\u001B[32m0.16935\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 906 | loss: 0.16935 - acc: 0.9641 -- iter: 8/8\n",
      "--\n",
      "Training Step: 907  | total loss: \u001B[1m\u001B[32m0.15404\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 907 | loss: 0.15404 - acc: 0.9677 -- iter: 8/8\n",
      "--\n",
      "Training Step: 908  | total loss: \u001B[1m\u001B[32m0.14028\u001B[0m\u001B[0m | time: 0.014s\n",
      "| Adam | epoch: 908 | loss: 0.14028 - acc: 0.9709 -- iter: 8/8\n",
      "--\n",
      "Training Step: 909  | total loss: \u001B[1m\u001B[32m0.12791\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 909 | loss: 0.12791 - acc: 0.9739 -- iter: 8/8\n",
      "--\n",
      "Training Step: 910  | total loss: \u001B[1m\u001B[32m0.11678\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 910 | loss: 0.11678 - acc: 0.9765 -- iter: 8/8\n",
      "--\n",
      "Training Step: 911  | total loss: \u001B[1m\u001B[32m0.10677\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 911 | loss: 0.10677 - acc: 0.9788 -- iter: 8/8\n",
      "--\n",
      "Training Step: 912  | total loss: \u001B[1m\u001B[32m0.09776\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 912 | loss: 0.09776 - acc: 0.9809 -- iter: 8/8\n",
      "--\n",
      "Training Step: 913  | total loss: \u001B[1m\u001B[32m0.08966\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 913 | loss: 0.08966 - acc: 0.9828 -- iter: 8/8\n",
      "--\n",
      "Training Step: 914  | total loss: \u001B[1m\u001B[32m0.08236\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 914 | loss: 0.08236 - acc: 0.9846 -- iter: 8/8\n",
      "--\n",
      "Training Step: 915  | total loss: \u001B[1m\u001B[32m0.07580\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 915 | loss: 0.07580 - acc: 0.9861 -- iter: 8/8\n",
      "--\n",
      "Training Step: 916  | total loss: \u001B[1m\u001B[32m0.06989\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 916 | loss: 0.06989 - acc: 0.9875 -- iter: 8/8\n",
      "--\n",
      "Training Step: 917  | total loss: \u001B[1m\u001B[32m0.06457\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 917 | loss: 0.06457 - acc: 0.9887 -- iter: 8/8\n",
      "--\n",
      "Training Step: 918  | total loss: \u001B[1m\u001B[32m0.05977\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 918 | loss: 0.05977 - acc: 0.9899 -- iter: 8/8\n",
      "--\n",
      "Training Step: 919  | total loss: \u001B[1m\u001B[32m0.05545\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 919 | loss: 0.05545 - acc: 0.9909 -- iter: 8/8\n",
      "--\n",
      "Training Step: 920  | total loss: \u001B[1m\u001B[32m0.05156\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 920 | loss: 0.05156 - acc: 0.9918 -- iter: 8/8\n",
      "--\n",
      "Training Step: 921  | total loss: \u001B[1m\u001B[32m0.04805\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 921 | loss: 0.04805 - acc: 0.9926 -- iter: 8/8\n",
      "--\n",
      "Training Step: 922  | total loss: \u001B[1m\u001B[32m0.04489\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 922 | loss: 0.04489 - acc: 0.9934 -- iter: 8/8\n",
      "--\n",
      "Training Step: 923  | total loss: \u001B[1m\u001B[32m0.04203\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 923 | loss: 0.04203 - acc: 0.9940 -- iter: 8/8\n",
      "--\n",
      "Training Step: 924  | total loss: \u001B[1m\u001B[32m0.24353\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 924 | loss: 0.24353 - acc: 0.9446 -- iter: 8/8\n",
      "--\n",
      "Training Step: 925  | total loss: \u001B[1m\u001B[32m0.22083\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 925 | loss: 0.22083 - acc: 0.9502 -- iter: 8/8\n",
      "--\n",
      "Training Step: 926  | total loss: \u001B[1m\u001B[32m0.20043\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 926 | loss: 0.20043 - acc: 0.9551 -- iter: 8/8\n",
      "--\n",
      "Training Step: 927  | total loss: \u001B[1m\u001B[32m0.18209\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 927 | loss: 0.18209 - acc: 0.9596 -- iter: 8/8\n",
      "--\n",
      "Training Step: 928  | total loss: \u001B[1m\u001B[32m0.16561\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 928 | loss: 0.16561 - acc: 0.9637 -- iter: 8/8\n",
      "--\n",
      "Training Step: 929  | total loss: \u001B[1m\u001B[32m0.15079\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 929 | loss: 0.15079 - acc: 0.9673 -- iter: 8/8\n",
      "--\n",
      "Training Step: 930  | total loss: \u001B[1m\u001B[32m0.13747\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 930 | loss: 0.13747 - acc: 0.9706 -- iter: 8/8\n",
      "--\n",
      "Training Step: 931  | total loss: \u001B[1m\u001B[32m0.12549\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 931 | loss: 0.12549 - acc: 0.9735 -- iter: 8/8\n",
      "--\n",
      "Training Step: 932  | total loss: \u001B[1m\u001B[32m0.41506\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 932 | loss: 0.41506 - acc: 0.9012 -- iter: 8/8\n",
      "--\n",
      "Training Step: 933  | total loss: \u001B[1m\u001B[32m0.37539\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 933 | loss: 0.37539 - acc: 0.9110 -- iter: 8/8\n",
      "--\n",
      "Training Step: 934  | total loss: \u001B[1m\u001B[32m0.53355\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 934 | loss: 0.53355 - acc: 0.8699 -- iter: 8/8\n",
      "--\n",
      "Training Step: 935  | total loss: \u001B[1m\u001B[32m0.48218\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 935 | loss: 0.48218 - acc: 0.8829 -- iter: 8/8\n",
      "--\n",
      "Training Step: 936  | total loss: \u001B[1m\u001B[32m0.43603\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 936 | loss: 0.43603 - acc: 0.8947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 937  | total loss: \u001B[1m\u001B[32m0.39457\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 937 | loss: 0.39457 - acc: 0.9052 -- iter: 8/8\n",
      "--\n",
      "Training Step: 938  | total loss: \u001B[1m\u001B[32m0.35733\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 938 | loss: 0.35733 - acc: 0.9147 -- iter: 8/8\n",
      "--\n",
      "Training Step: 939  | total loss: \u001B[1m\u001B[32m0.32387\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 939 | loss: 0.32387 - acc: 0.9232 -- iter: 8/8\n",
      "--\n",
      "Training Step: 940  | total loss: \u001B[1m\u001B[32m0.47475\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 940 | loss: 0.47475 - acc: 0.8809 -- iter: 8/8\n",
      "--\n",
      "Training Step: 941  | total loss: \u001B[1m\u001B[32m0.42969\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 941 | loss: 0.42969 - acc: 0.8928 -- iter: 8/8\n",
      "--\n",
      "Training Step: 942  | total loss: \u001B[1m\u001B[32m0.38922\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 942 | loss: 0.38922 - acc: 0.9035 -- iter: 8/8\n",
      "--\n",
      "Training Step: 943  | total loss: \u001B[1m\u001B[32m0.35287\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 943 | loss: 0.35287 - acc: 0.9132 -- iter: 8/8\n",
      "--\n",
      "Training Step: 944  | total loss: \u001B[1m\u001B[32m0.32022\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 944 | loss: 0.32022 - acc: 0.9218 -- iter: 8/8\n",
      "--\n",
      "Training Step: 945  | total loss: \u001B[1m\u001B[32m0.29090\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 945 | loss: 0.29090 - acc: 0.9297 -- iter: 8/8\n",
      "--\n",
      "Training Step: 946  | total loss: \u001B[1m\u001B[32m0.26455\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 946 | loss: 0.26455 - acc: 0.9367 -- iter: 8/8\n",
      "--\n",
      "Training Step: 947  | total loss: \u001B[1m\u001B[32m0.24089\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 947 | loss: 0.24089 - acc: 0.9430 -- iter: 8/8\n",
      "--\n",
      "Training Step: 948  | total loss: \u001B[1m\u001B[32m0.21963\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 948 | loss: 0.21963 - acc: 0.9487 -- iter: 8/8\n",
      "--\n",
      "Training Step: 949  | total loss: \u001B[1m\u001B[32m0.20053\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 949 | loss: 0.20053 - acc: 0.9539 -- iter: 8/8\n",
      "--\n",
      "Training Step: 950  | total loss: \u001B[1m\u001B[32m0.36027\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 950 | loss: 0.36027 - acc: 0.9085 -- iter: 8/8\n",
      "--\n",
      "Training Step: 951  | total loss: \u001B[1m\u001B[32m0.32720\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 951 | loss: 0.32720 - acc: 0.9176 -- iter: 8/8\n",
      "--\n",
      "Training Step: 952  | total loss: \u001B[1m\u001B[32m0.29750\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 952 | loss: 0.29750 - acc: 0.9259 -- iter: 8/8\n",
      "--\n",
      "Training Step: 953  | total loss: \u001B[1m\u001B[32m0.27082\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 953 | loss: 0.27082 - acc: 0.9333 -- iter: 8/8\n",
      "--\n",
      "Training Step: 954  | total loss: \u001B[1m\u001B[32m0.24685\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 954 | loss: 0.24685 - acc: 0.9399 -- iter: 8/8\n",
      "--\n",
      "Training Step: 955  | total loss: \u001B[1m\u001B[32m0.22531\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 955 | loss: 0.22531 - acc: 0.9459 -- iter: 8/8\n",
      "--\n",
      "Training Step: 956  | total loss: \u001B[1m\u001B[32m0.20597\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 956 | loss: 0.20597 - acc: 0.9514 -- iter: 8/8\n",
      "--\n",
      "Training Step: 957  | total loss: \u001B[1m\u001B[32m0.18858\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 957 | loss: 0.18858 - acc: 0.9562 -- iter: 8/8\n",
      "--\n",
      "Training Step: 958  | total loss: \u001B[1m\u001B[32m0.17294\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 958 | loss: 0.17294 - acc: 0.9606 -- iter: 8/8\n",
      "--\n",
      "Training Step: 959  | total loss: \u001B[1m\u001B[32m0.15889\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 959 | loss: 0.15889 - acc: 0.9645 -- iter: 8/8\n",
      "--\n",
      "Training Step: 960  | total loss: \u001B[1m\u001B[32m0.14625\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 960 | loss: 0.14625 - acc: 0.9681 -- iter: 8/8\n",
      "--\n",
      "Training Step: 961  | total loss: \u001B[1m\u001B[32m0.13488\u001B[0m\u001B[0m | time: 0.014s\n",
      "| Adam | epoch: 961 | loss: 0.13488 - acc: 0.9713 -- iter: 8/8\n",
      "--\n",
      "Training Step: 962  | total loss: \u001B[1m\u001B[32m0.29259\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 962 | loss: 0.29259 - acc: 0.9241 -- iter: 8/8\n",
      "--\n",
      "Training Step: 963  | total loss: \u001B[1m\u001B[32m0.26664\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 963 | loss: 0.26664 - acc: 0.9317 -- iter: 8/8\n",
      "--\n",
      "Training Step: 964  | total loss: \u001B[1m\u001B[32m0.24333\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 964 | loss: 0.24333 - acc: 0.9386 -- iter: 8/8\n",
      "--\n",
      "Training Step: 965  | total loss: \u001B[1m\u001B[32m0.22240\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 965 | loss: 0.22240 - acc: 0.9447 -- iter: 8/8\n",
      "--\n",
      "Training Step: 966  | total loss: \u001B[1m\u001B[32m0.20358\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 966 | loss: 0.20358 - acc: 0.9502 -- iter: 8/8\n",
      "--\n",
      "Training Step: 967  | total loss: \u001B[1m\u001B[32m0.18668\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 967 | loss: 0.18668 - acc: 0.9552 -- iter: 8/8\n",
      "--\n",
      "Training Step: 968  | total loss: \u001B[1m\u001B[32m0.17148\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 968 | loss: 0.17148 - acc: 0.9597 -- iter: 8/8\n",
      "--\n",
      "Training Step: 969  | total loss: \u001B[1m\u001B[32m0.15782\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 969 | loss: 0.15782 - acc: 0.9637 -- iter: 8/8\n",
      "--\n",
      "Training Step: 970  | total loss: \u001B[1m\u001B[32m0.14553\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 970 | loss: 0.14553 - acc: 0.9673 -- iter: 8/8\n",
      "--\n",
      "Training Step: 971  | total loss: \u001B[1m\u001B[32m0.13447\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 971 | loss: 0.13447 - acc: 0.9706 -- iter: 8/8\n",
      "--\n",
      "Training Step: 972  | total loss: \u001B[1m\u001B[32m0.12452\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 972 | loss: 0.12452 - acc: 0.9736 -- iter: 8/8\n",
      "--\n",
      "Training Step: 973  | total loss: \u001B[1m\u001B[32m0.11556\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 973 | loss: 0.11556 - acc: 0.9762 -- iter: 8/8\n",
      "--\n",
      "Training Step: 974  | total loss: \u001B[1m\u001B[32m0.10749\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 974 | loss: 0.10749 - acc: 0.9786 -- iter: 8/8\n",
      "--\n",
      "Training Step: 975  | total loss: \u001B[1m\u001B[32m0.10022\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 975 | loss: 0.10022 - acc: 0.9807 -- iter: 8/8\n",
      "--\n",
      "Training Step: 976  | total loss: \u001B[1m\u001B[32m0.09366\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 976 | loss: 0.09366 - acc: 0.9826 -- iter: 8/8\n",
      "--\n",
      "Training Step: 977  | total loss: \u001B[1m\u001B[32m0.08774\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 977 | loss: 0.08774 - acc: 0.9859 -- iter: 8/8\n",
      "--\n",
      "Training Step: 978  | total loss: \u001B[1m\u001B[32m0.08239\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 978 | loss: 0.08239 - acc: 0.9859 -- iter: 8/8\n",
      "--\n",
      "Training Step: 979  | total loss: \u001B[1m\u001B[32m0.07756\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 979 | loss: 0.07756 - acc: 0.9873 -- iter: 8/8\n",
      "--\n",
      "Training Step: 980  | total loss: \u001B[1m\u001B[32m0.07319\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 980 | loss: 0.07319 - acc: 0.9886 -- iter: 8/8\n",
      "--\n",
      "Training Step: 981  | total loss: \u001B[1m\u001B[32m0.06924\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 981 | loss: 0.06924 - acc: 0.9898 -- iter: 8/8\n",
      "--\n",
      "Training Step: 982  | total loss: \u001B[1m\u001B[32m0.06566\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 982 | loss: 0.06566 - acc: 0.9908 -- iter: 8/8\n",
      "--\n",
      "Training Step: 983  | total loss: \u001B[1m\u001B[32m0.06241\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 983 | loss: 0.06241 - acc: 0.9917 -- iter: 8/8\n",
      "--\n",
      "Training Step: 984  | total loss: \u001B[1m\u001B[32m0.05946\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 984 | loss: 0.05946 - acc: 0.9925 -- iter: 8/8\n",
      "--\n",
      "Training Step: 985  | total loss: \u001B[1m\u001B[32m0.05677\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 985 | loss: 0.05677 - acc: 0.9933 -- iter: 8/8\n",
      "--\n",
      "Training Step: 986  | total loss: \u001B[1m\u001B[32m0.05433\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 986 | loss: 0.05433 - acc: 0.9939 -- iter: 8/8\n",
      "--\n",
      "Training Step: 987  | total loss: \u001B[1m\u001B[32m0.05211\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 987 | loss: 0.05211 - acc: 0.9946 -- iter: 8/8\n",
      "--\n",
      "Training Step: 988  | total loss: \u001B[1m\u001B[32m0.05008\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 988 | loss: 0.05008 - acc: 0.9951 -- iter: 8/8\n",
      "--\n",
      "Training Step: 989  | total loss: \u001B[1m\u001B[32m0.04822\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 989 | loss: 0.04822 - acc: 0.9956 -- iter: 8/8\n",
      "--\n",
      "Training Step: 990  | total loss: \u001B[1m\u001B[32m0.04652\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 990 | loss: 0.04652 - acc: 0.9960 -- iter: 8/8\n",
      "--\n",
      "Training Step: 991  | total loss: \u001B[1m\u001B[32m0.04496\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 991 | loss: 0.04496 - acc: 0.9964 -- iter: 8/8\n",
      "--\n",
      "Training Step: 992  | total loss: \u001B[1m\u001B[32m0.04353\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 992 | loss: 0.04353 - acc: 0.9968 -- iter: 8/8\n",
      "--\n",
      "Training Step: 993  | total loss: \u001B[1m\u001B[32m0.04221\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 993 | loss: 0.04221 - acc: 0.9971 -- iter: 8/8\n",
      "--\n",
      "Training Step: 994  | total loss: \u001B[1m\u001B[32m0.04100\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 994 | loss: 0.04100 - acc: 0.9974 -- iter: 8/8\n",
      "--\n",
      "Training Step: 995  | total loss: \u001B[1m\u001B[32m0.03987\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 995 | loss: 0.03987 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 996  | total loss: \u001B[1m\u001B[32m0.03884\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 996 | loss: 0.03884 - acc: 0.9979 -- iter: 8/8\n",
      "--\n",
      "Training Step: 997  | total loss: \u001B[1m\u001B[32m0.03787\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 997 | loss: 0.03787 - acc: 0.9981 -- iter: 8/8\n",
      "--\n",
      "Training Step: 998  | total loss: \u001B[1m\u001B[32m0.03697\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 998 | loss: 0.03697 - acc: 0.9983 -- iter: 8/8\n",
      "--\n",
      "Training Step: 999  | total loss: \u001B[1m\u001B[32m0.03614\u001B[0m\u001B[0m | time: 0.011s\n",
      "| Adam | epoch: 999 | loss: 0.03614 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001B[1m\u001B[32m0.03536\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 1000 | loss: 0.03536 - acc: 0.9986 -- iter: 8/8\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Daniel Krasovski\\Documents\\GitHub\\Project_Oreo\\models\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('models/model.tflearn')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save all of our data structures\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}