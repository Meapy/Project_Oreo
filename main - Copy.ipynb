{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('data/responses.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 documents\n",
      "2 classes ['goodbye', 'greeting']\n",
      "14 unique stemmed words ['anyon', 'ar', 'bye', 'day', 'good', 'goodby', 'hello', 'hi', 'how', 'is', 'lat', 'see', 'ther', 'you']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-9d149b3413ff>:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create our training data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "---------------------------------\n",
      "Run id: Y7K47A\n",
      "Log directory: tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 8\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.151s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 2  | total loss: \u001B[1m\u001B[32m0.62385\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 002 | loss: 0.62385 - acc: 0.2250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 3  | total loss: \u001B[1m\u001B[32m0.68033\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 003 | loss: 0.68033 - acc: 0.5523 -- iter: 8/8\n",
      "--\n",
      "Training Step: 4  | total loss: \u001B[1m\u001B[32m0.68953\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 004 | loss: 0.68953 - acc: 0.6068 -- iter: 8/8\n",
      "--\n",
      "Training Step: 5  | total loss: \u001B[1m\u001B[32m0.69145\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 005 | loss: 0.69145 - acc: 0.6194 -- iter: 8/8\n",
      "--\n",
      "Training Step: 6  | total loss: \u001B[1m\u001B[32m0.69182\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 006 | loss: 0.69182 - acc: 0.6230 -- iter: 8/8\n",
      "--\n",
      "Training Step: 7  | total loss: \u001B[1m\u001B[32m0.69176\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 007 | loss: 0.69176 - acc: 0.6242 -- iter: 8/8\n",
      "--\n",
      "Training Step: 8  | total loss: \u001B[1m\u001B[32m0.69158\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 008 | loss: 0.69158 - acc: 0.6247 -- iter: 8/8\n",
      "--\n",
      "Training Step: 9  | total loss: \u001B[1m\u001B[32m0.69134\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 009 | loss: 0.69134 - acc: 0.6248 -- iter: 8/8\n",
      "--\n",
      "Training Step: 10  | total loss: \u001B[1m\u001B[32m0.69108\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 010 | loss: 0.69108 - acc: 0.6249 -- iter: 8/8\n",
      "--\n",
      "Training Step: 11  | total loss: \u001B[1m\u001B[32m0.69081\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 011 | loss: 0.69081 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 12  | total loss: \u001B[1m\u001B[32m0.69053\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 012 | loss: 0.69053 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 13  | total loss: \u001B[1m\u001B[32m0.69024\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 013 | loss: 0.69024 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 14  | total loss: \u001B[1m\u001B[32m0.68994\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 014 | loss: 0.68994 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 15  | total loss: \u001B[1m\u001B[32m0.68964\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 015 | loss: 0.68964 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 16  | total loss: \u001B[1m\u001B[32m0.68932\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 016 | loss: 0.68932 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 17  | total loss: \u001B[1m\u001B[32m0.68900\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 017 | loss: 0.68900 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 18  | total loss: \u001B[1m\u001B[32m0.68866\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 018 | loss: 0.68866 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 19  | total loss: \u001B[1m\u001B[32m0.68831\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 019 | loss: 0.68831 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 20  | total loss: \u001B[1m\u001B[32m0.68795\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 020 | loss: 0.68795 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 21  | total loss: \u001B[1m\u001B[32m0.68758\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 021 | loss: 0.68758 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 22  | total loss: \u001B[1m\u001B[32m0.68719\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 022 | loss: 0.68719 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 23  | total loss: \u001B[1m\u001B[32m0.68679\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 023 | loss: 0.68679 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 24  | total loss: \u001B[1m\u001B[32m0.68636\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 024 | loss: 0.68636 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 25  | total loss: \u001B[1m\u001B[32m0.68592\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 025 | loss: 0.68592 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 26  | total loss: \u001B[1m\u001B[32m0.68546\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 026 | loss: 0.68546 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 27  | total loss: \u001B[1m\u001B[32m0.68498\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 027 | loss: 0.68498 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 28  | total loss: \u001B[1m\u001B[32m0.68448\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 028 | loss: 0.68448 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 29  | total loss: \u001B[1m\u001B[32m0.68395\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 029 | loss: 0.68395 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 30  | total loss: \u001B[1m\u001B[32m0.68339\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 030 | loss: 0.68339 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 31  | total loss: \u001B[1m\u001B[32m0.68281\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 031 | loss: 0.68281 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 32  | total loss: \u001B[1m\u001B[32m0.68219\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 032 | loss: 0.68219 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 33  | total loss: \u001B[1m\u001B[32m0.68155\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 033 | loss: 0.68155 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 34  | total loss: \u001B[1m\u001B[32m0.68087\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 034 | loss: 0.68087 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 35  | total loss: \u001B[1m\u001B[32m0.68016\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 035 | loss: 0.68016 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 36  | total loss: \u001B[1m\u001B[32m0.67941\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 036 | loss: 0.67941 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 37  | total loss: \u001B[1m\u001B[32m0.67862\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 037 | loss: 0.67862 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 38  | total loss: \u001B[1m\u001B[32m0.67779\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 038 | loss: 0.67779 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 39  | total loss: \u001B[1m\u001B[32m0.67692\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 039 | loss: 0.67692 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 40  | total loss: \u001B[1m\u001B[32m0.67600\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 040 | loss: 0.67600 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 41  | total loss: \u001B[1m\u001B[32m0.67504\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 041 | loss: 0.67504 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 42  | total loss: \u001B[1m\u001B[32m0.67403\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 042 | loss: 0.67403 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 43  | total loss: \u001B[1m\u001B[32m0.67297\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 043 | loss: 0.67297 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 44  | total loss: \u001B[1m\u001B[32m0.67186\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 044 | loss: 0.67186 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 45  | total loss: \u001B[1m\u001B[32m0.67069\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 045 | loss: 0.67069 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 46  | total loss: \u001B[1m\u001B[32m0.66947\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 046 | loss: 0.66947 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 47  | total loss: \u001B[1m\u001B[32m0.66819\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 047 | loss: 0.66819 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 48  | total loss: \u001B[1m\u001B[32m0.66685\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 048 | loss: 0.66685 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 49  | total loss: \u001B[1m\u001B[32m0.66544\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 049 | loss: 0.66544 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 50  | total loss: \u001B[1m\u001B[32m0.66711\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 050 | loss: 0.66711 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 51  | total loss: \u001B[1m\u001B[32m0.66512\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 051 | loss: 0.66512 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 52  | total loss: \u001B[1m\u001B[32m0.66318\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 052 | loss: 0.66318 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 53  | total loss: \u001B[1m\u001B[32m0.66124\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 053 | loss: 0.66124 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 54  | total loss: \u001B[1m\u001B[32m0.65930\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 054 | loss: 0.65930 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 55  | total loss: \u001B[1m\u001B[32m0.65735\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 055 | loss: 0.65735 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 56  | total loss: \u001B[1m\u001B[32m0.65536\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 056 | loss: 0.65536 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 57  | total loss: \u001B[1m\u001B[32m0.65333\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 057 | loss: 0.65333 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 58  | total loss: \u001B[1m\u001B[32m0.65125\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 058 | loss: 0.65125 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 59  | total loss: \u001B[1m\u001B[32m0.64912\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 059 | loss: 0.64912 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 60  | total loss: \u001B[1m\u001B[32m0.64693\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 060 | loss: 0.64693 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 61  | total loss: \u001B[1m\u001B[32m0.64467\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 061 | loss: 0.64467 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 62  | total loss: \u001B[1m\u001B[32m0.64235\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 062 | loss: 0.64235 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 63  | total loss: \u001B[1m\u001B[32m0.63996\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 063 | loss: 0.63996 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 64  | total loss: \u001B[1m\u001B[32m0.63749\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 064 | loss: 0.63749 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 65  | total loss: \u001B[1m\u001B[32m0.63494\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 065 | loss: 0.63494 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 66  | total loss: \u001B[1m\u001B[32m0.63232\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 066 | loss: 0.63232 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 67  | total loss: \u001B[1m\u001B[32m0.62962\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 067 | loss: 0.62962 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 68  | total loss: \u001B[1m\u001B[32m0.63179\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 068 | loss: 0.63179 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 69  | total loss: \u001B[1m\u001B[32m0.62838\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 069 | loss: 0.62838 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 70  | total loss: \u001B[1m\u001B[32m0.62499\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 070 | loss: 0.62499 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 71  | total loss: \u001B[1m\u001B[32m0.62159\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 071 | loss: 0.62159 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 72  | total loss: \u001B[1m\u001B[32m0.61818\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 072 | loss: 0.61818 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 73  | total loss: \u001B[1m\u001B[32m0.61474\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 073 | loss: 0.61474 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 74  | total loss: \u001B[1m\u001B[32m0.61128\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 074 | loss: 0.61128 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 75  | total loss: \u001B[1m\u001B[32m0.60776\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 075 | loss: 0.60776 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 76  | total loss: \u001B[1m\u001B[32m0.60421\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 076 | loss: 0.60421 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 77  | total loss: \u001B[1m\u001B[32m0.60060\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 077 | loss: 0.60060 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 78  | total loss: \u001B[1m\u001B[32m0.59693\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 078 | loss: 0.59693 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 79  | total loss: \u001B[1m\u001B[32m0.59320\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 079 | loss: 0.59320 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 80  | total loss: \u001B[1m\u001B[32m0.58942\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 080 | loss: 0.58942 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 81  | total loss: \u001B[1m\u001B[32m0.58556\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 081 | loss: 0.58556 - acc: 0.6250 -- iter: 8/8\n",
      "--\n",
      "Training Step: 82  | total loss: \u001B[1m\u001B[32m0.58165\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 082 | loss: 0.58165 - acc: 0.6375 -- iter: 8/8\n",
      "--\n",
      "Training Step: 83  | total loss: \u001B[1m\u001B[32m0.57762\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 083 | loss: 0.57762 - acc: 0.6487 -- iter: 8/8\n",
      "--\n",
      "Training Step: 84  | total loss: \u001B[1m\u001B[32m0.57349\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 084 | loss: 0.57349 - acc: 0.6589 -- iter: 8/8\n",
      "--\n",
      "Training Step: 85  | total loss: \u001B[1m\u001B[32m0.56924\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 085 | loss: 0.56924 - acc: 0.6680 -- iter: 8/8\n",
      "--\n",
      "Training Step: 86  | total loss: \u001B[1m\u001B[32m0.56490\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 086 | loss: 0.56490 - acc: 0.6762 -- iter: 8/8\n",
      "--\n",
      "Training Step: 87  | total loss: \u001B[1m\u001B[32m0.56045\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 087 | loss: 0.56045 - acc: 0.6836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 88  | total loss: \u001B[1m\u001B[32m0.55590\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 088 | loss: 0.55590 - acc: 0.6902 -- iter: 8/8\n",
      "--\n",
      "Training Step: 89  | total loss: \u001B[1m\u001B[32m0.55126\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 089 | loss: 0.55126 - acc: 0.6962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 90  | total loss: \u001B[1m\u001B[32m0.54651\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 090 | loss: 0.54651 - acc: 0.7016 -- iter: 8/8\n",
      "--\n",
      "Training Step: 91  | total loss: \u001B[1m\u001B[32m0.54167\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 091 | loss: 0.54167 - acc: 0.7064 -- iter: 8/8\n",
      "--\n",
      "Training Step: 92  | total loss: \u001B[1m\u001B[32m0.53673\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 092 | loss: 0.53673 - acc: 0.7108 -- iter: 8/8\n",
      "--\n",
      "Training Step: 93  | total loss: \u001B[1m\u001B[32m0.53170\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 093 | loss: 0.53170 - acc: 0.7147 -- iter: 8/8\n",
      "--\n",
      "Training Step: 94  | total loss: \u001B[1m\u001B[32m0.52657\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 094 | loss: 0.52657 - acc: 0.7182 -- iter: 8/8\n",
      "--\n",
      "Training Step: 95  | total loss: \u001B[1m\u001B[32m0.52136\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 095 | loss: 0.52136 - acc: 0.7214 -- iter: 8/8\n",
      "--\n",
      "Training Step: 96  | total loss: \u001B[1m\u001B[32m0.51605\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 096 | loss: 0.51605 - acc: 0.7243 -- iter: 8/8\n",
      "--\n",
      "Training Step: 97  | total loss: \u001B[1m\u001B[32m0.51064\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 097 | loss: 0.51064 - acc: 0.7268 -- iter: 8/8\n",
      "--\n",
      "Training Step: 98  | total loss: \u001B[1m\u001B[32m0.50515\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 098 | loss: 0.50515 - acc: 0.7292 -- iter: 8/8\n",
      "--\n",
      "Training Step: 99  | total loss: \u001B[1m\u001B[32m0.49958\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 099 | loss: 0.49958 - acc: 0.7312 -- iter: 8/8\n",
      "--\n",
      "Training Step: 100  | total loss: \u001B[1m\u001B[32m0.49391\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 100 | loss: 0.49391 - acc: 0.7331 -- iter: 8/8\n",
      "--\n",
      "Training Step: 101  | total loss: \u001B[1m\u001B[32m0.48817\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 101 | loss: 0.48817 - acc: 0.7348 -- iter: 8/8\n",
      "--\n",
      "Training Step: 102  | total loss: \u001B[1m\u001B[32m0.48234\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 102 | loss: 0.48234 - acc: 0.7363 -- iter: 8/8\n",
      "--\n",
      "Training Step: 103  | total loss: \u001B[1m\u001B[32m0.47643\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 103 | loss: 0.47643 - acc: 0.7377 -- iter: 8/8\n",
      "--\n",
      "Training Step: 104  | total loss: \u001B[1m\u001B[32m0.47045\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 104 | loss: 0.47045 - acc: 0.7389 -- iter: 8/8\n",
      "--\n",
      "Training Step: 105  | total loss: \u001B[1m\u001B[32m0.46440\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 105 | loss: 0.46440 - acc: 0.7400 -- iter: 8/8\n",
      "--\n",
      "Training Step: 106  | total loss: \u001B[1m\u001B[32m0.45827\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 106 | loss: 0.45827 - acc: 0.7410 -- iter: 8/8\n",
      "--\n",
      "Training Step: 107  | total loss: \u001B[1m\u001B[32m0.45209\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 107 | loss: 0.45209 - acc: 0.7419 -- iter: 8/8\n",
      "--\n",
      "Training Step: 108  | total loss: \u001B[1m\u001B[32m0.44584\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 108 | loss: 0.44584 - acc: 0.7427 -- iter: 8/8\n",
      "--\n",
      "Training Step: 109  | total loss: \u001B[1m\u001B[32m0.43954\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 109 | loss: 0.43954 - acc: 0.7435 -- iter: 8/8\n",
      "--\n",
      "Training Step: 110  | total loss: \u001B[1m\u001B[32m0.43319\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 110 | loss: 0.43319 - acc: 0.7441 -- iter: 8/8\n",
      "--\n",
      "Training Step: 111  | total loss: \u001B[1m\u001B[32m0.42679\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 111 | loss: 0.42679 - acc: 0.7447 -- iter: 8/8\n",
      "--\n",
      "Training Step: 112  | total loss: \u001B[1m\u001B[32m0.42035\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 112 | loss: 0.42035 - acc: 0.7452 -- iter: 8/8\n",
      "--\n",
      "Training Step: 113  | total loss: \u001B[1m\u001B[32m0.41387\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 113 | loss: 0.41387 - acc: 0.7582 -- iter: 8/8\n",
      "--\n",
      "Training Step: 114  | total loss: \u001B[1m\u001B[32m0.40736\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 114 | loss: 0.40736 - acc: 0.7699 -- iter: 8/8\n",
      "--\n",
      "Training Step: 115  | total loss: \u001B[1m\u001B[32m0.40083\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 115 | loss: 0.40083 - acc: 0.7929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 116  | total loss: \u001B[1m\u001B[32m0.39428\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 116 | loss: 0.39428 - acc: 0.8136 -- iter: 8/8\n",
      "--\n",
      "Training Step: 117  | total loss: \u001B[1m\u001B[32m0.38771\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 117 | loss: 0.38771 - acc: 0.8322 -- iter: 8/8\n",
      "--\n",
      "Training Step: 118  | total loss: \u001B[1m\u001B[32m0.38113\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 118 | loss: 0.38113 - acc: 0.8490 -- iter: 8/8\n",
      "--\n",
      "Training Step: 119  | total loss: \u001B[1m\u001B[32m0.37456\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 119 | loss: 0.37456 - acc: 0.8641 -- iter: 8/8\n",
      "--\n",
      "Training Step: 120  | total loss: \u001B[1m\u001B[32m0.36798\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 120 | loss: 0.36798 - acc: 0.8777 -- iter: 8/8\n",
      "--\n",
      "Training Step: 121  | total loss: \u001B[1m\u001B[32m0.36141\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 121 | loss: 0.36141 - acc: 0.8899 -- iter: 8/8\n",
      "--\n",
      "Training Step: 122  | total loss: \u001B[1m\u001B[32m0.35485\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 122 | loss: 0.35485 - acc: 0.9009 -- iter: 8/8\n",
      "--\n",
      "Training Step: 123  | total loss: \u001B[1m\u001B[32m0.34831\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 123 | loss: 0.34831 - acc: 0.9108 -- iter: 8/8\n",
      "--\n",
      "Training Step: 124  | total loss: \u001B[1m\u001B[32m0.34179\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 124 | loss: 0.34179 - acc: 0.9198 -- iter: 8/8\n",
      "--\n",
      "Training Step: 125  | total loss: \u001B[1m\u001B[32m0.33530\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 125 | loss: 0.33530 - acc: 0.9278 -- iter: 8/8\n",
      "--\n",
      "Training Step: 126  | total loss: \u001B[1m\u001B[32m0.32884\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 126 | loss: 0.32884 - acc: 0.9350 -- iter: 8/8\n",
      "--\n",
      "Training Step: 127  | total loss: \u001B[1m\u001B[32m0.32241\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 127 | loss: 0.32241 - acc: 0.9415 -- iter: 8/8\n",
      "--\n",
      "Training Step: 128  | total loss: \u001B[1m\u001B[32m0.31603\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 128 | loss: 0.31603 - acc: 0.9474 -- iter: 8/8\n",
      "--\n",
      "Training Step: 129  | total loss: \u001B[1m\u001B[32m0.30968\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 129 | loss: 0.30968 - acc: 0.9526 -- iter: 8/8\n",
      "--\n",
      "Training Step: 130  | total loss: \u001B[1m\u001B[32m0.30339\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 130 | loss: 0.30339 - acc: 0.9574 -- iter: 8/8\n",
      "--\n",
      "Training Step: 131  | total loss: \u001B[1m\u001B[32m0.29714\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 131 | loss: 0.29714 - acc: 0.9616 -- iter: 8/8\n",
      "--\n",
      "Training Step: 132  | total loss: \u001B[1m\u001B[32m0.29095\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 132 | loss: 0.29095 - acc: 0.9655 -- iter: 8/8\n",
      "--\n",
      "Training Step: 133  | total loss: \u001B[1m\u001B[32m0.28482\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 133 | loss: 0.28482 - acc: 0.9689 -- iter: 8/8\n",
      "--\n",
      "Training Step: 134  | total loss: \u001B[1m\u001B[32m0.27875\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 134 | loss: 0.27875 - acc: 0.9720 -- iter: 8/8\n",
      "--\n",
      "Training Step: 135  | total loss: \u001B[1m\u001B[32m0.27274\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 135 | loss: 0.27274 - acc: 0.9748 -- iter: 8/8\n",
      "--\n",
      "Training Step: 136  | total loss: \u001B[1m\u001B[32m0.26680\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 136 | loss: 0.26680 - acc: 0.9773 -- iter: 8/8\n",
      "--\n",
      "Training Step: 137  | total loss: \u001B[1m\u001B[32m0.26093\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 137 | loss: 0.26093 - acc: 0.9796 -- iter: 8/8\n",
      "--\n",
      "Training Step: 138  | total loss: \u001B[1m\u001B[32m0.25513\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 138 | loss: 0.25513 - acc: 0.9816 -- iter: 8/8\n",
      "--\n",
      "Training Step: 139  | total loss: \u001B[1m\u001B[32m0.24941\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 139 | loss: 0.24941 - acc: 0.9835 -- iter: 8/8\n",
      "--\n",
      "Training Step: 140  | total loss: \u001B[1m\u001B[32m0.24376\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 140 | loss: 0.24376 - acc: 0.9851 -- iter: 8/8\n",
      "--\n",
      "Training Step: 141  | total loss: \u001B[1m\u001B[32m0.23819\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 141 | loss: 0.23819 - acc: 0.9866 -- iter: 8/8\n",
      "--\n",
      "Training Step: 142  | total loss: \u001B[1m\u001B[32m0.23270\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 142 | loss: 0.23270 - acc: 0.9880 -- iter: 8/8\n",
      "--\n",
      "Training Step: 143  | total loss: \u001B[1m\u001B[32m0.22730\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 143 | loss: 0.22730 - acc: 0.9892 -- iter: 8/8\n",
      "--\n",
      "Training Step: 144  | total loss: \u001B[1m\u001B[32m0.22198\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 144 | loss: 0.22198 - acc: 0.9902 -- iter: 8/8\n",
      "--\n",
      "Training Step: 145  | total loss: \u001B[1m\u001B[32m0.21675\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 145 | loss: 0.21675 - acc: 0.9912 -- iter: 8/8\n",
      "--\n",
      "Training Step: 146  | total loss: \u001B[1m\u001B[32m0.21160\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 146 | loss: 0.21160 - acc: 0.9921 -- iter: 8/8\n",
      "--\n",
      "Training Step: 147  | total loss: \u001B[1m\u001B[32m0.20654\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 147 | loss: 0.20654 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 148  | total loss: \u001B[1m\u001B[32m0.20158\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 148 | loss: 0.20158 - acc: 0.9936 -- iter: 8/8\n",
      "--\n",
      "Training Step: 149  | total loss: \u001B[1m\u001B[32m0.19670\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 149 | loss: 0.19670 - acc: 0.9942 -- iter: 8/8\n",
      "--\n",
      "Training Step: 150  | total loss: \u001B[1m\u001B[32m0.19192\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 150 | loss: 0.19192 - acc: 0.9948 -- iter: 8/8\n",
      "--\n",
      "Training Step: 151  | total loss: \u001B[1m\u001B[32m0.18722\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 151 | loss: 0.18722 - acc: 0.9953 -- iter: 8/8\n",
      "--\n",
      "Training Step: 152  | total loss: \u001B[1m\u001B[32m0.18263\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 152 | loss: 0.18263 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 153  | total loss: \u001B[1m\u001B[32m0.17812\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 153 | loss: 0.17812 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 154  | total loss: \u001B[1m\u001B[32m0.17371\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 154 | loss: 0.17371 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 155  | total loss: \u001B[1m\u001B[32m0.16939\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 155 | loss: 0.16939 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 156  | total loss: \u001B[1m\u001B[32m0.16516\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 156 | loss: 0.16516 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 157  | total loss: \u001B[1m\u001B[32m0.16103\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 157 | loss: 0.16103 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 158  | total loss: \u001B[1m\u001B[32m0.15700\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 158 | loss: 0.15700 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 159  | total loss: \u001B[1m\u001B[32m0.15305\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 159 | loss: 0.15305 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 160  | total loss: \u001B[1m\u001B[32m0.14920\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 160 | loss: 0.14920 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 161  | total loss: \u001B[1m\u001B[32m0.14543\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 161 | loss: 0.14543 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 162  | total loss: \u001B[1m\u001B[32m0.14176\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 162 | loss: 0.14176 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 163  | total loss: \u001B[1m\u001B[32m0.13818\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 163 | loss: 0.13818 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 164  | total loss: \u001B[1m\u001B[32m0.13469\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 164 | loss: 0.13469 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 165  | total loss: \u001B[1m\u001B[32m0.13128\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 165 | loss: 0.13128 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 166  | total loss: \u001B[1m\u001B[32m0.12796\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 166 | loss: 0.12796 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 167  | total loss: \u001B[1m\u001B[32m0.12473\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 167 | loss: 0.12473 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 168  | total loss: \u001B[1m\u001B[32m0.22256\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 168 | loss: 0.22256 - acc: 0.9492 -- iter: 8/8\n",
      "--\n",
      "Training Step: 169  | total loss: \u001B[1m\u001B[32m0.20952\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 169 | loss: 0.20952 - acc: 0.9543 -- iter: 8/8\n",
      "--\n",
      "Training Step: 170  | total loss: \u001B[1m\u001B[32m0.19767\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 170 | loss: 0.19767 - acc: 0.9589 -- iter: 8/8\n",
      "--\n",
      "Training Step: 171  | total loss: \u001B[1m\u001B[32m0.18688\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 171 | loss: 0.18688 - acc: 0.9630 -- iter: 8/8\n",
      "--\n",
      "Training Step: 172  | total loss: \u001B[1m\u001B[32m0.17705\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 172 | loss: 0.17705 - acc: 0.9667 -- iter: 8/8\n",
      "--\n",
      "Training Step: 173  | total loss: \u001B[1m\u001B[32m0.16807\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 173 | loss: 0.16807 - acc: 0.9700 -- iter: 8/8\n",
      "--\n",
      "Training Step: 174  | total loss: \u001B[1m\u001B[32m0.15985\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 174 | loss: 0.15985 - acc: 0.9730 -- iter: 8/8\n",
      "--\n",
      "Training Step: 175  | total loss: \u001B[1m\u001B[32m0.15233\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 175 | loss: 0.15233 - acc: 0.9757 -- iter: 8/8\n",
      "--\n",
      "Training Step: 176  | total loss: \u001B[1m\u001B[32m0.14541\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 176 | loss: 0.14541 - acc: 0.9781 -- iter: 8/8\n",
      "--\n",
      "Training Step: 177  | total loss: \u001B[1m\u001B[32m0.13905\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 177 | loss: 0.13905 - acc: 0.9803 -- iter: 8/8\n",
      "--\n",
      "Training Step: 178  | total loss: \u001B[1m\u001B[32m0.13319\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 178 | loss: 0.13319 - acc: 0.9823 -- iter: 8/8\n",
      "--\n",
      "Training Step: 179  | total loss: \u001B[1m\u001B[32m0.12777\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 179 | loss: 0.12777 - acc: 0.9841 -- iter: 8/8\n",
      "--\n",
      "Training Step: 180  | total loss: \u001B[1m\u001B[32m0.12275\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 180 | loss: 0.12275 - acc: 0.9857 -- iter: 8/8\n",
      "--\n",
      "Training Step: 181  | total loss: \u001B[1m\u001B[32m0.11810\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 181 | loss: 0.11810 - acc: 0.9871 -- iter: 8/8\n",
      "--\n",
      "Training Step: 182  | total loss: \u001B[1m\u001B[32m0.11377\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 182 | loss: 0.11377 - acc: 0.9884 -- iter: 8/8\n",
      "--\n",
      "Training Step: 183  | total loss: \u001B[1m\u001B[32m0.10974\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 183 | loss: 0.10974 - acc: 0.9895 -- iter: 8/8\n",
      "--\n",
      "Training Step: 184  | total loss: \u001B[1m\u001B[32m0.10597\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 184 | loss: 0.10597 - acc: 0.9906 -- iter: 8/8\n",
      "--\n",
      "Training Step: 185  | total loss: \u001B[1m\u001B[32m0.10244\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 185 | loss: 0.10244 - acc: 0.9915 -- iter: 8/8\n",
      "--\n",
      "Training Step: 186  | total loss: \u001B[1m\u001B[32m0.20039\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 186 | loss: 0.20039 - acc: 0.9674 -- iter: 8/8\n",
      "--\n",
      "Training Step: 187  | total loss: \u001B[1m\u001B[32m0.18722\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 187 | loss: 0.18722 - acc: 0.9706 -- iter: 8/8\n",
      "--\n",
      "Training Step: 188  | total loss: \u001B[1m\u001B[32m0.17529\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 188 | loss: 0.17529 - acc: 0.9736 -- iter: 8/8\n",
      "--\n",
      "Training Step: 189  | total loss: \u001B[1m\u001B[32m0.16448\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 189 | loss: 0.16448 - acc: 0.9762 -- iter: 8/8\n",
      "--\n",
      "Training Step: 190  | total loss: \u001B[1m\u001B[32m0.15466\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 190 | loss: 0.15466 - acc: 0.9786 -- iter: 8/8\n",
      "--\n",
      "Training Step: 191  | total loss: \u001B[1m\u001B[32m0.14574\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 191 | loss: 0.14574 - acc: 0.9807 -- iter: 8/8\n",
      "--\n",
      "Training Step: 192  | total loss: \u001B[1m\u001B[32m0.13763\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 192 | loss: 0.13763 - acc: 0.9827 -- iter: 8/8\n",
      "--\n",
      "Training Step: 193  | total loss: \u001B[1m\u001B[32m0.13023\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 193 | loss: 0.13023 - acc: 0.9844 -- iter: 8/8\n",
      "--\n",
      "Training Step: 194  | total loss: \u001B[1m\u001B[32m0.12349\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 194 | loss: 0.12349 - acc: 0.9860 -- iter: 8/8\n",
      "--\n",
      "Training Step: 195  | total loss: \u001B[1m\u001B[32m0.11732\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 195 | loss: 0.11732 - acc: 0.9874 -- iter: 8/8\n",
      "--\n",
      "Training Step: 196  | total loss: \u001B[1m\u001B[32m0.11168\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 196 | loss: 0.11168 - acc: 0.9886 -- iter: 8/8\n",
      "--\n",
      "Training Step: 197  | total loss: \u001B[1m\u001B[32m0.10651\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 197 | loss: 0.10651 - acc: 0.9898 -- iter: 8/8\n",
      "--\n",
      "Training Step: 198  | total loss: \u001B[1m\u001B[32m0.10176\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 198 | loss: 0.10176 - acc: 0.9908 -- iter: 8/8\n",
      "--\n",
      "Training Step: 199  | total loss: \u001B[1m\u001B[32m0.09739\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 199 | loss: 0.09739 - acc: 0.9917 -- iter: 8/8\n",
      "--\n",
      "Training Step: 200  | total loss: \u001B[1m\u001B[32m0.09336\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 200 | loss: 0.09336 - acc: 0.9925 -- iter: 8/8\n",
      "--\n",
      "Training Step: 201  | total loss: \u001B[1m\u001B[32m0.08965\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 201 | loss: 0.08965 - acc: 0.9933 -- iter: 8/8\n",
      "--\n",
      "Training Step: 202  | total loss: \u001B[1m\u001B[32m0.08621\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 202 | loss: 0.08621 - acc: 0.9940 -- iter: 8/8\n",
      "--\n",
      "Training Step: 203  | total loss: \u001B[1m\u001B[32m0.08302\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 203 | loss: 0.08302 - acc: 0.9946 -- iter: 8/8\n",
      "--\n",
      "Training Step: 204  | total loss: \u001B[1m\u001B[32m0.08005\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 204 | loss: 0.08005 - acc: 0.9951 -- iter: 8/8\n",
      "--\n",
      "Training Step: 205  | total loss: \u001B[1m\u001B[32m0.07730\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 205 | loss: 0.07730 - acc: 0.9956 -- iter: 8/8\n",
      "--\n",
      "Training Step: 206  | total loss: \u001B[1m\u001B[32m0.07473\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 206 | loss: 0.07473 - acc: 0.9960 -- iter: 8/8\n",
      "--\n",
      "Training Step: 207  | total loss: \u001B[1m\u001B[32m0.07232\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 207 | loss: 0.07232 - acc: 0.9964 -- iter: 8/8\n",
      "--\n",
      "Training Step: 208  | total loss: \u001B[1m\u001B[32m0.07007\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 208 | loss: 0.07007 - acc: 0.9968 -- iter: 8/8\n",
      "--\n",
      "Training Step: 209  | total loss: \u001B[1m\u001B[32m0.06796\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 209 | loss: 0.06796 - acc: 0.9971 -- iter: 8/8\n",
      "--\n",
      "Training Step: 210  | total loss: \u001B[1m\u001B[32m0.06598\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 210 | loss: 0.06598 - acc: 0.9974 -- iter: 8/8\n",
      "--\n",
      "Training Step: 211  | total loss: \u001B[1m\u001B[32m0.06411\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 211 | loss: 0.06411 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 212  | total loss: \u001B[1m\u001B[32m0.06234\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 212 | loss: 0.06234 - acc: 0.9979 -- iter: 8/8\n",
      "--\n",
      "Training Step: 213  | total loss: \u001B[1m\u001B[32m0.06067\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 213 | loss: 0.06067 - acc: 0.9981 -- iter: 8/8\n",
      "--\n",
      "Training Step: 214  | total loss: \u001B[1m\u001B[32m0.05909\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 214 | loss: 0.05909 - acc: 0.9983 -- iter: 8/8\n",
      "--\n",
      "Training Step: 215  | total loss: \u001B[1m\u001B[32m0.05759\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 215 | loss: 0.05759 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 216  | total loss: \u001B[1m\u001B[32m0.05616\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 216 | loss: 0.05616 - acc: 0.9986 -- iter: 8/8\n",
      "--\n",
      "Training Step: 217  | total loss: \u001B[1m\u001B[32m0.05480\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 217 | loss: 0.05480 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 218  | total loss: \u001B[1m\u001B[32m0.05351\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 218 | loss: 0.05351 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 219  | total loss: \u001B[1m\u001B[32m0.05227\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 219 | loss: 0.05227 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 220  | total loss: \u001B[1m\u001B[32m0.05108\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 220 | loss: 0.05108 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 221  | total loss: \u001B[1m\u001B[32m0.04995\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 221 | loss: 0.04995 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 222  | total loss: \u001B[1m\u001B[32m0.04886\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 222 | loss: 0.04886 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 223  | total loss: \u001B[1m\u001B[32m0.04782\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 223 | loss: 0.04782 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 224  | total loss: \u001B[1m\u001B[32m0.04681\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 224 | loss: 0.04681 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 225  | total loss: \u001B[1m\u001B[32m0.04584\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 225 | loss: 0.04584 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 226  | total loss: \u001B[1m\u001B[32m0.04491\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 226 | loss: 0.04491 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 227  | total loss: \u001B[1m\u001B[32m0.04401\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 227 | loss: 0.04401 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 228  | total loss: \u001B[1m\u001B[32m0.04314\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 228 | loss: 0.04314 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 229  | total loss: \u001B[1m\u001B[32m0.04230\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 229 | loss: 0.04230 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 230  | total loss: \u001B[1m\u001B[32m0.04149\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 230 | loss: 0.04149 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 231  | total loss: \u001B[1m\u001B[32m0.04070\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 231 | loss: 0.04070 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 232  | total loss: \u001B[1m\u001B[32m0.03994\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 232 | loss: 0.03994 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 233  | total loss: \u001B[1m\u001B[32m0.03920\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 233 | loss: 0.03920 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 234  | total loss: \u001B[1m\u001B[32m0.03848\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 234 | loss: 0.03848 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 235  | total loss: \u001B[1m\u001B[32m0.03779\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 235 | loss: 0.03779 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 236  | total loss: \u001B[1m\u001B[32m0.03711\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 236 | loss: 0.03711 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 237  | total loss: \u001B[1m\u001B[32m0.03645\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 237 | loss: 0.03645 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 238  | total loss: \u001B[1m\u001B[32m0.03582\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 238 | loss: 0.03582 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 239  | total loss: \u001B[1m\u001B[32m0.03519\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 239 | loss: 0.03519 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 240  | total loss: \u001B[1m\u001B[32m0.03459\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 240 | loss: 0.03459 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 241  | total loss: \u001B[1m\u001B[32m0.03400\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 241 | loss: 0.03400 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 242  | total loss: \u001B[1m\u001B[32m0.03343\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 242 | loss: 0.03343 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 243  | total loss: \u001B[1m\u001B[32m0.03287\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 243 | loss: 0.03287 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 244  | total loss: \u001B[1m\u001B[32m0.03233\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 244 | loss: 0.03233 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 245  | total loss: \u001B[1m\u001B[32m0.03180\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 245 | loss: 0.03180 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 246  | total loss: \u001B[1m\u001B[32m0.03128\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 246 | loss: 0.03128 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 247  | total loss: \u001B[1m\u001B[32m0.03078\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 247 | loss: 0.03078 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 248  | total loss: \u001B[1m\u001B[32m0.03028\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 248 | loss: 0.03028 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 249  | total loss: \u001B[1m\u001B[32m0.02980\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 249 | loss: 0.02980 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 250  | total loss: \u001B[1m\u001B[32m0.02934\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 250 | loss: 0.02934 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 251  | total loss: \u001B[1m\u001B[32m0.02888\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 251 | loss: 0.02888 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 252  | total loss: \u001B[1m\u001B[32m0.02843\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 252 | loss: 0.02843 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 253  | total loss: \u001B[1m\u001B[32m0.02799\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 253 | loss: 0.02799 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 254  | total loss: \u001B[1m\u001B[32m0.02757\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 254 | loss: 0.02757 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 255  | total loss: \u001B[1m\u001B[32m0.02715\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 255 | loss: 0.02715 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 256  | total loss: \u001B[1m\u001B[32m0.02674\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 256 | loss: 0.02674 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 257  | total loss: \u001B[1m\u001B[32m0.02634\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 257 | loss: 0.02634 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 258  | total loss: \u001B[1m\u001B[32m0.02595\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 258 | loss: 0.02595 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 259  | total loss: \u001B[1m\u001B[32m0.02557\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 259 | loss: 0.02557 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 260  | total loss: \u001B[1m\u001B[32m0.02520\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 260 | loss: 0.02520 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 261  | total loss: \u001B[1m\u001B[32m0.02483\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 261 | loss: 0.02483 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 262  | total loss: \u001B[1m\u001B[32m0.02448\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 262 | loss: 0.02448 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 263  | total loss: \u001B[1m\u001B[32m0.02413\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 263 | loss: 0.02413 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 264  | total loss: \u001B[1m\u001B[32m0.02379\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 264 | loss: 0.02379 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 265  | total loss: \u001B[1m\u001B[32m0.02345\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 265 | loss: 0.02345 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 266  | total loss: \u001B[1m\u001B[32m0.02312\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 266 | loss: 0.02312 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 267  | total loss: \u001B[1m\u001B[32m0.02280\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 267 | loss: 0.02280 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 268  | total loss: \u001B[1m\u001B[32m0.02249\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 268 | loss: 0.02249 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 269  | total loss: \u001B[1m\u001B[32m0.02218\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 269 | loss: 0.02218 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 270  | total loss: \u001B[1m\u001B[32m0.02188\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 270 | loss: 0.02188 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 271  | total loss: \u001B[1m\u001B[32m0.02158\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 271 | loss: 0.02158 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 272  | total loss: \u001B[1m\u001B[32m0.02129\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 272 | loss: 0.02129 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 273  | total loss: \u001B[1m\u001B[32m0.02101\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 273 | loss: 0.02101 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 274  | total loss: \u001B[1m\u001B[32m0.02073\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 274 | loss: 0.02073 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 275  | total loss: \u001B[1m\u001B[32m0.02046\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 275 | loss: 0.02046 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 276  | total loss: \u001B[1m\u001B[32m0.02019\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 276 | loss: 0.02019 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 277  | total loss: \u001B[1m\u001B[32m0.01993\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 277 | loss: 0.01993 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 278  | total loss: \u001B[1m\u001B[32m0.01967\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 278 | loss: 0.01967 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 279  | total loss: \u001B[1m\u001B[32m0.01942\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 279 | loss: 0.01942 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 280  | total loss: \u001B[1m\u001B[32m0.01917\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 280 | loss: 0.01917 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 281  | total loss: \u001B[1m\u001B[32m0.01893\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 281 | loss: 0.01893 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 282  | total loss: \u001B[1m\u001B[32m0.01869\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 282 | loss: 0.01869 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 283  | total loss: \u001B[1m\u001B[32m0.01846\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 283 | loss: 0.01846 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 284  | total loss: \u001B[1m\u001B[32m0.01823\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 284 | loss: 0.01823 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 285  | total loss: \u001B[1m\u001B[32m0.01801\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 285 | loss: 0.01801 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 286  | total loss: \u001B[1m\u001B[32m0.01778\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 286 | loss: 0.01778 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 287  | total loss: \u001B[1m\u001B[32m0.01757\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 287 | loss: 0.01757 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 288  | total loss: \u001B[1m\u001B[32m0.01735\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 288 | loss: 0.01735 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 289  | total loss: \u001B[1m\u001B[32m0.01715\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 289 | loss: 0.01715 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 290  | total loss: \u001B[1m\u001B[32m0.01694\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 290 | loss: 0.01694 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 291  | total loss: \u001B[1m\u001B[32m0.01674\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 291 | loss: 0.01674 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 292  | total loss: \u001B[1m\u001B[32m0.01654\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 292 | loss: 0.01654 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 293  | total loss: \u001B[1m\u001B[32m0.01635\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 293 | loss: 0.01635 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 294  | total loss: \u001B[1m\u001B[32m0.01616\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 294 | loss: 0.01616 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 295  | total loss: \u001B[1m\u001B[32m0.01597\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 295 | loss: 0.01597 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 296  | total loss: \u001B[1m\u001B[32m0.13492\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 296 | loss: 0.13492 - acc: 0.9750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 297  | total loss: \u001B[1m\u001B[32m0.12285\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 297 | loss: 0.12285 - acc: 0.9775 -- iter: 8/8\n",
      "--\n",
      "Training Step: 298  | total loss: \u001B[1m\u001B[32m0.11199\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 298 | loss: 0.11199 - acc: 0.9797 -- iter: 8/8\n",
      "--\n",
      "Training Step: 299  | total loss: \u001B[1m\u001B[32m0.10223\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 299 | loss: 0.10223 - acc: 0.9818 -- iter: 8/8\n",
      "--\n",
      "Training Step: 300  | total loss: \u001B[1m\u001B[32m0.09344\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 300 | loss: 0.09344 - acc: 0.9836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 301  | total loss: \u001B[1m\u001B[32m0.08553\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 301 | loss: 0.08553 - acc: 0.9852 -- iter: 8/8\n",
      "--\n",
      "Training Step: 302  | total loss: \u001B[1m\u001B[32m0.07842\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 302 | loss: 0.07842 - acc: 0.9867 -- iter: 8/8\n",
      "--\n",
      "Training Step: 303  | total loss: \u001B[1m\u001B[32m0.07202\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 303 | loss: 0.07202 - acc: 0.9880 -- iter: 8/8\n",
      "--\n",
      "Training Step: 304  | total loss: \u001B[1m\u001B[32m0.06625\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 304 | loss: 0.06625 - acc: 0.9892 -- iter: 8/8\n",
      "--\n",
      "Training Step: 305  | total loss: \u001B[1m\u001B[32m0.06106\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 305 | loss: 0.06106 - acc: 0.9903 -- iter: 8/8\n",
      "--\n",
      "Training Step: 306  | total loss: \u001B[1m\u001B[32m0.05638\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 306 | loss: 0.05638 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 307  | total loss: \u001B[1m\u001B[32m0.05216\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 307 | loss: 0.05216 - acc: 0.9922 -- iter: 8/8\n",
      "--\n",
      "Training Step: 308  | total loss: \u001B[1m\u001B[32m0.04836\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 308 | loss: 0.04836 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 309  | total loss: \u001B[1m\u001B[32m0.04493\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 309 | loss: 0.04493 - acc: 0.9936 -- iter: 8/8\n",
      "--\n",
      "Training Step: 310  | total loss: \u001B[1m\u001B[32m0.04184\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 310 | loss: 0.04184 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 311  | total loss: \u001B[1m\u001B[32m0.03905\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 311 | loss: 0.03905 - acc: 0.9949 -- iter: 8/8\n",
      "--\n",
      "Training Step: 312  | total loss: \u001B[1m\u001B[32m0.03653\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 312 | loss: 0.03653 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 313  | total loss: \u001B[1m\u001B[32m0.03425\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 313 | loss: 0.03425 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 314  | total loss: \u001B[1m\u001B[32m0.03219\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 314 | loss: 0.03219 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 315  | total loss: \u001B[1m\u001B[32m0.03033\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 315 | loss: 0.03033 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 316  | total loss: \u001B[1m\u001B[32m0.02865\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 316 | loss: 0.02865 - acc: 0.9970 -- iter: 8/8\n",
      "--\n",
      "Training Step: 317  | total loss: \u001B[1m\u001B[32m0.02712\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 317 | loss: 0.02712 - acc: 0.9973 -- iter: 8/8\n",
      "--\n",
      "Training Step: 318  | total loss: \u001B[1m\u001B[32m0.02573\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 318 | loss: 0.02573 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 319  | total loss: \u001B[1m\u001B[32m0.02448\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 319 | loss: 0.02448 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 320  | total loss: \u001B[1m\u001B[32m0.02333\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 320 | loss: 0.02333 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 321  | total loss: \u001B[1m\u001B[32m0.02230\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 321 | loss: 0.02230 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 322  | total loss: \u001B[1m\u001B[32m0.02135\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 322 | loss: 0.02135 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 323  | total loss: \u001B[1m\u001B[32m0.02049\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 323 | loss: 0.02049 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 324  | total loss: \u001B[1m\u001B[32m0.01970\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 324 | loss: 0.01970 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 325  | total loss: \u001B[1m\u001B[32m0.01898\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 325 | loss: 0.01898 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 326  | total loss: \u001B[1m\u001B[32m0.01833\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 326 | loss: 0.01833 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 327  | total loss: \u001B[1m\u001B[32m0.01772\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 327 | loss: 0.01772 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 328  | total loss: \u001B[1m\u001B[32m0.01717\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 328 | loss: 0.01717 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 329  | total loss: \u001B[1m\u001B[32m0.01666\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 329 | loss: 0.01666 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 330  | total loss: \u001B[1m\u001B[32m0.01619\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 330 | loss: 0.01619 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 331  | total loss: \u001B[1m\u001B[32m0.01576\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 331 | loss: 0.01576 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 332  | total loss: \u001B[1m\u001B[32m0.01536\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 332 | loss: 0.01536 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 333  | total loss: \u001B[1m\u001B[32m0.01499\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 333 | loss: 0.01499 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 334  | total loss: \u001B[1m\u001B[32m0.01465\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 334 | loss: 0.01465 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 335  | total loss: \u001B[1m\u001B[32m0.01433\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 335 | loss: 0.01433 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 336  | total loss: \u001B[1m\u001B[32m0.01403\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 336 | loss: 0.01403 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 337  | total loss: \u001B[1m\u001B[32m0.01376\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 337 | loss: 0.01376 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 338  | total loss: \u001B[1m\u001B[32m0.01350\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 338 | loss: 0.01350 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 339  | total loss: \u001B[1m\u001B[32m0.01325\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 339 | loss: 0.01325 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 340  | total loss: \u001B[1m\u001B[32m0.01302\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 340 | loss: 0.01302 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 341  | total loss: \u001B[1m\u001B[32m0.01281\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 341 | loss: 0.01281 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 342  | total loss: \u001B[1m\u001B[32m0.01260\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 342 | loss: 0.01260 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 343  | total loss: \u001B[1m\u001B[32m0.01241\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 343 | loss: 0.01241 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 344  | total loss: \u001B[1m\u001B[32m0.01223\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 344 | loss: 0.01223 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 345  | total loss: \u001B[1m\u001B[32m0.01205\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 345 | loss: 0.01205 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 346  | total loss: \u001B[1m\u001B[32m0.01189\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 346 | loss: 0.01189 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 347  | total loss: \u001B[1m\u001B[32m0.01173\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 347 | loss: 0.01173 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 348  | total loss: \u001B[1m\u001B[32m0.01158\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 348 | loss: 0.01158 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 349  | total loss: \u001B[1m\u001B[32m0.01143\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 349 | loss: 0.01143 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 350  | total loss: \u001B[1m\u001B[32m0.01129\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 350 | loss: 0.01129 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 351  | total loss: \u001B[1m\u001B[32m0.01116\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 351 | loss: 0.01116 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 352  | total loss: \u001B[1m\u001B[32m0.01103\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 352 | loss: 0.01103 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 353  | total loss: \u001B[1m\u001B[32m0.01090\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 353 | loss: 0.01090 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 354  | total loss: \u001B[1m\u001B[32m0.01078\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 354 | loss: 0.01078 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 355  | total loss: \u001B[1m\u001B[32m0.01067\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 355 | loss: 0.01067 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 356  | total loss: \u001B[1m\u001B[32m0.01055\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 356 | loss: 0.01055 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 357  | total loss: \u001B[1m\u001B[32m0.01044\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 357 | loss: 0.01044 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 358  | total loss: \u001B[1m\u001B[32m0.01034\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 358 | loss: 0.01034 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 359  | total loss: \u001B[1m\u001B[32m0.01023\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 359 | loss: 0.01023 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 360  | total loss: \u001B[1m\u001B[32m0.01013\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 360 | loss: 0.01013 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 361  | total loss: \u001B[1m\u001B[32m0.01003\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 361 | loss: 0.01003 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 362  | total loss: \u001B[1m\u001B[32m0.00994\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 362 | loss: 0.00994 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 363  | total loss: \u001B[1m\u001B[32m0.00984\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 363 | loss: 0.00984 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 364  | total loss: \u001B[1m\u001B[32m0.00975\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 364 | loss: 0.00975 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 365  | total loss: \u001B[1m\u001B[32m0.00966\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 365 | loss: 0.00966 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 366  | total loss: \u001B[1m\u001B[32m0.00957\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 366 | loss: 0.00957 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 367  | total loss: \u001B[1m\u001B[32m0.00948\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 367 | loss: 0.00948 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 368  | total loss: \u001B[1m\u001B[32m0.00940\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 368 | loss: 0.00940 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 369  | total loss: \u001B[1m\u001B[32m0.00932\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 369 | loss: 0.00932 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 370  | total loss: \u001B[1m\u001B[32m0.00923\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 370 | loss: 0.00923 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 371  | total loss: \u001B[1m\u001B[32m0.00915\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 371 | loss: 0.00915 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 372  | total loss: \u001B[1m\u001B[32m0.00907\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 372 | loss: 0.00907 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 373  | total loss: \u001B[1m\u001B[32m0.00900\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 373 | loss: 0.00900 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 374  | total loss: \u001B[1m\u001B[32m0.00892\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 374 | loss: 0.00892 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 375  | total loss: \u001B[1m\u001B[32m0.00884\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 375 | loss: 0.00884 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 376  | total loss: \u001B[1m\u001B[32m0.00877\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 376 | loss: 0.00877 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 377  | total loss: \u001B[1m\u001B[32m0.00870\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 377 | loss: 0.00870 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 378  | total loss: \u001B[1m\u001B[32m0.00862\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 378 | loss: 0.00862 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 379  | total loss: \u001B[1m\u001B[32m0.00855\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 379 | loss: 0.00855 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 380  | total loss: \u001B[1m\u001B[32m0.00848\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 380 | loss: 0.00848 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 381  | total loss: \u001B[1m\u001B[32m0.00841\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 381 | loss: 0.00841 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 382  | total loss: \u001B[1m\u001B[32m0.00835\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 382 | loss: 0.00835 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 383  | total loss: \u001B[1m\u001B[32m0.00828\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 383 | loss: 0.00828 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 384  | total loss: \u001B[1m\u001B[32m0.00821\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 384 | loss: 0.00821 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 385  | total loss: \u001B[1m\u001B[32m0.00815\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 385 | loss: 0.00815 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 386  | total loss: \u001B[1m\u001B[32m0.00808\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 386 | loss: 0.00808 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 387  | total loss: \u001B[1m\u001B[32m0.00802\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 387 | loss: 0.00802 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 388  | total loss: \u001B[1m\u001B[32m0.00796\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 388 | loss: 0.00796 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 389  | total loss: \u001B[1m\u001B[32m0.00790\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 389 | loss: 0.00790 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 390  | total loss: \u001B[1m\u001B[32m0.00783\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 390 | loss: 0.00783 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 391  | total loss: \u001B[1m\u001B[32m0.00777\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 391 | loss: 0.00777 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 392  | total loss: \u001B[1m\u001B[32m0.00771\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 392 | loss: 0.00771 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 393  | total loss: \u001B[1m\u001B[32m0.00765\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 393 | loss: 0.00765 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 394  | total loss: \u001B[1m\u001B[32m0.00760\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 394 | loss: 0.00760 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 395  | total loss: \u001B[1m\u001B[32m0.00754\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 395 | loss: 0.00754 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 396  | total loss: \u001B[1m\u001B[32m0.00748\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 396 | loss: 0.00748 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 397  | total loss: \u001B[1m\u001B[32m0.00743\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 397 | loss: 0.00743 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 398  | total loss: \u001B[1m\u001B[32m0.00737\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 398 | loss: 0.00737 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 399  | total loss: \u001B[1m\u001B[32m0.00731\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 399 | loss: 0.00731 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 400  | total loss: \u001B[1m\u001B[32m0.00726\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 400 | loss: 0.00726 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 401  | total loss: \u001B[1m\u001B[32m0.00721\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 401 | loss: 0.00721 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 402  | total loss: \u001B[1m\u001B[32m0.00715\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 402 | loss: 0.00715 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 403  | total loss: \u001B[1m\u001B[32m0.00710\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 403 | loss: 0.00710 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 404  | total loss: \u001B[1m\u001B[32m0.00705\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 404 | loss: 0.00705 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 405  | total loss: \u001B[1m\u001B[32m0.00700\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 405 | loss: 0.00700 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 406  | total loss: \u001B[1m\u001B[32m0.00695\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 406 | loss: 0.00695 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 407  | total loss: \u001B[1m\u001B[32m0.00690\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 407 | loss: 0.00690 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 408  | total loss: \u001B[1m\u001B[32m0.00685\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 408 | loss: 0.00685 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 409  | total loss: \u001B[1m\u001B[32m0.00680\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 409 | loss: 0.00680 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 410  | total loss: \u001B[1m\u001B[32m0.27636\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 410 | loss: 0.27636 - acc: 0.9500 -- iter: 8/8\n",
      "--\n",
      "Training Step: 411  | total loss: \u001B[1m\u001B[32m0.24938\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 411 | loss: 0.24938 - acc: 0.9550 -- iter: 8/8\n",
      "--\n",
      "Training Step: 412  | total loss: \u001B[1m\u001B[32m0.22512\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 412 | loss: 0.22512 - acc: 0.9595 -- iter: 8/8\n",
      "--\n",
      "Training Step: 413  | total loss: \u001B[1m\u001B[32m0.20330\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 413 | loss: 0.20330 - acc: 0.9635 -- iter: 8/8\n",
      "--\n",
      "Training Step: 414  | total loss: \u001B[1m\u001B[32m0.18369\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 414 | loss: 0.18369 - acc: 0.9672 -- iter: 8/8\n",
      "--\n",
      "Training Step: 415  | total loss: \u001B[1m\u001B[32m0.16605\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 415 | loss: 0.16605 - acc: 0.9705 -- iter: 8/8\n",
      "--\n",
      "Training Step: 416  | total loss: \u001B[1m\u001B[32m0.15019\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 416 | loss: 0.15019 - acc: 0.9734 -- iter: 8/8\n",
      "--\n",
      "Training Step: 417  | total loss: \u001B[1m\u001B[32m0.13593\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 417 | loss: 0.13593 - acc: 0.9761 -- iter: 8/8\n",
      "--\n",
      "Training Step: 418  | total loss: \u001B[1m\u001B[32m0.12311\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 418 | loss: 0.12311 - acc: 0.9785 -- iter: 8/8\n",
      "--\n",
      "Training Step: 419  | total loss: \u001B[1m\u001B[32m0.11157\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 419 | loss: 0.11157 - acc: 0.9806 -- iter: 8/8\n",
      "--\n",
      "Training Step: 420  | total loss: \u001B[1m\u001B[32m0.10120\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 420 | loss: 0.10120 - acc: 0.9826 -- iter: 8/8\n",
      "--\n",
      "Training Step: 421  | total loss: \u001B[1m\u001B[32m0.09188\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 421 | loss: 0.09188 - acc: 0.9843 -- iter: 8/8\n",
      "--\n",
      "Training Step: 422  | total loss: \u001B[1m\u001B[32m0.08349\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 422 | loss: 0.08349 - acc: 0.9873 -- iter: 8/8\n",
      "--\n",
      "Training Step: 423  | total loss: \u001B[1m\u001B[32m0.07595\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 423 | loss: 0.07595 - acc: 0.9873 -- iter: 8/8\n",
      "--\n",
      "Training Step: 424  | total loss: \u001B[1m\u001B[32m0.06916\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 424 | loss: 0.06916 - acc: 0.9886 -- iter: 8/8\n",
      "--\n",
      "Training Step: 425  | total loss: \u001B[1m\u001B[32m0.06306\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 425 | loss: 0.06306 - acc: 0.9897 -- iter: 8/8\n",
      "--\n",
      "Training Step: 426  | total loss: \u001B[1m\u001B[32m0.16796\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 426 | loss: 0.16796 - acc: 0.9657 -- iter: 8/8\n",
      "--\n",
      "Training Step: 427  | total loss: \u001B[1m\u001B[32m0.15200\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 427 | loss: 0.15200 - acc: 0.9692 -- iter: 8/8\n",
      "--\n",
      "Training Step: 428  | total loss: \u001B[1m\u001B[32m0.13765\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 428 | loss: 0.13765 - acc: 0.9722 -- iter: 8/8\n",
      "--\n",
      "Training Step: 429  | total loss: \u001B[1m\u001B[32m0.12474\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 429 | loss: 0.12474 - acc: 0.9750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 430  | total loss: \u001B[1m\u001B[32m0.11314\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 430 | loss: 0.11314 - acc: 0.9775 -- iter: 8/8\n",
      "--\n",
      "Training Step: 431  | total loss: \u001B[1m\u001B[32m0.10270\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 431 | loss: 0.10270 - acc: 0.9798 -- iter: 8/8\n",
      "--\n",
      "Training Step: 432  | total loss: \u001B[1m\u001B[32m0.09332\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 432 | loss: 0.09332 - acc: 0.9818 -- iter: 8/8\n",
      "--\n",
      "Training Step: 433  | total loss: \u001B[1m\u001B[32m0.08488\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 433 | loss: 0.08488 - acc: 0.9836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 434  | total loss: \u001B[1m\u001B[32m0.07729\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 434 | loss: 0.07729 - acc: 0.9852 -- iter: 8/8\n",
      "--\n",
      "Training Step: 435  | total loss: \u001B[1m\u001B[32m0.07047\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 435 | loss: 0.07047 - acc: 0.9867 -- iter: 8/8\n",
      "--\n",
      "Training Step: 436  | total loss: \u001B[1m\u001B[32m0.06433\u001B[0m\u001B[0m | time: 0.013s\n",
      "| Adam | epoch: 436 | loss: 0.06433 - acc: 0.9881 -- iter: 8/8\n",
      "--\n",
      "Training Step: 437  | total loss: \u001B[1m\u001B[32m0.05881\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 437 | loss: 0.05881 - acc: 0.9892 -- iter: 8/8\n",
      "--\n",
      "Training Step: 438  | total loss: \u001B[1m\u001B[32m0.05384\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 438 | loss: 0.05384 - acc: 0.9903 -- iter: 8/8\n",
      "--\n",
      "Training Step: 439  | total loss: \u001B[1m\u001B[32m0.04937\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 439 | loss: 0.04937 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 440  | total loss: \u001B[1m\u001B[32m0.04535\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 440 | loss: 0.04535 - acc: 0.9922 -- iter: 8/8\n",
      "--\n",
      "Training Step: 441  | total loss: \u001B[1m\u001B[32m0.04173\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 441 | loss: 0.04173 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 442  | total loss: \u001B[1m\u001B[32m0.03847\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 442 | loss: 0.03847 - acc: 0.9937 -- iter: 8/8\n",
      "--\n",
      "Training Step: 443  | total loss: \u001B[1m\u001B[32m0.03553\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 443 | loss: 0.03553 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 444  | total loss: \u001B[1m\u001B[32m0.03289\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 444 | loss: 0.03289 - acc: 0.9949 -- iter: 8/8\n",
      "--\n",
      "Training Step: 445  | total loss: \u001B[1m\u001B[32m0.03050\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 445 | loss: 0.03050 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 446  | total loss: \u001B[1m\u001B[32m0.02836\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 446 | loss: 0.02836 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 447  | total loss: \u001B[1m\u001B[32m0.02642\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 447 | loss: 0.02642 - acc: 0.9963 -- iter: 8/8\n",
      "--\n",
      "Training Step: 448  | total loss: \u001B[1m\u001B[32m0.02468\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 448 | loss: 0.02468 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 449  | total loss: \u001B[1m\u001B[32m0.02310\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 449 | loss: 0.02310 - acc: 0.9970 -- iter: 8/8\n",
      "--\n",
      "Training Step: 450  | total loss: \u001B[1m\u001B[32m0.02168\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 450 | loss: 0.02168 - acc: 0.9973 -- iter: 8/8\n",
      "--\n",
      "Training Step: 451  | total loss: \u001B[1m\u001B[32m0.02040\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 451 | loss: 0.02040 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 452  | total loss: \u001B[1m\u001B[32m0.01924\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 452 | loss: 0.01924 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 453  | total loss: \u001B[1m\u001B[32m0.01820\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 453 | loss: 0.01820 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 454  | total loss: \u001B[1m\u001B[32m0.01725\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 454 | loss: 0.01725 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 455  | total loss: \u001B[1m\u001B[32m0.01639\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 455 | loss: 0.01639 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 456  | total loss: \u001B[1m\u001B[32m0.01562\u001B[0m\u001B[0m | time: 0.008s\n",
      "| Adam | epoch: 456 | loss: 0.01562 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 457  | total loss: \u001B[1m\u001B[32m0.01491\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 457 | loss: 0.01491 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 458  | total loss: \u001B[1m\u001B[32m0.01428\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 458 | loss: 0.01428 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 459  | total loss: \u001B[1m\u001B[32m0.01370\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 459 | loss: 0.01370 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 460  | total loss: \u001B[1m\u001B[32m0.01317\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 460 | loss: 0.01317 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 461  | total loss: \u001B[1m\u001B[32m0.01269\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 461 | loss: 0.01269 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 462  | total loss: \u001B[1m\u001B[32m0.01226\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 462 | loss: 0.01226 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 463  | total loss: \u001B[1m\u001B[32m0.01186\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 463 | loss: 0.01186 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 464  | total loss: \u001B[1m\u001B[32m0.01150\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 464 | loss: 0.01150 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 465  | total loss: \u001B[1m\u001B[32m0.01117\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 465 | loss: 0.01117 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 466  | total loss: \u001B[1m\u001B[32m0.01086\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 466 | loss: 0.01086 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 467  | total loss: \u001B[1m\u001B[32m0.01059\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 467 | loss: 0.01059 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 468  | total loss: \u001B[1m\u001B[32m0.01033\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 468 | loss: 0.01033 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 469  | total loss: \u001B[1m\u001B[32m0.01010\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 469 | loss: 0.01010 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 470  | total loss: \u001B[1m\u001B[32m0.00988\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 470 | loss: 0.00988 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 471  | total loss: \u001B[1m\u001B[32m0.00968\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 471 | loss: 0.00968 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 472  | total loss: \u001B[1m\u001B[32m0.00949\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 472 | loss: 0.00949 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 473  | total loss: \u001B[1m\u001B[32m0.00932\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 473 | loss: 0.00932 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 474  | total loss: \u001B[1m\u001B[32m0.00916\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 474 | loss: 0.00916 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 475  | total loss: \u001B[1m\u001B[32m0.00902\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 475 | loss: 0.00902 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 476  | total loss: \u001B[1m\u001B[32m0.00888\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 476 | loss: 0.00888 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 477  | total loss: \u001B[1m\u001B[32m0.00875\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 477 | loss: 0.00875 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 478  | total loss: \u001B[1m\u001B[32m0.00863\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 478 | loss: 0.00863 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 479  | total loss: \u001B[1m\u001B[32m0.00851\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 479 | loss: 0.00851 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 480  | total loss: \u001B[1m\u001B[32m0.00841\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 480 | loss: 0.00841 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 481  | total loss: \u001B[1m\u001B[32m0.00830\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 481 | loss: 0.00830 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 482  | total loss: \u001B[1m\u001B[32m0.00821\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 482 | loss: 0.00821 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 483  | total loss: \u001B[1m\u001B[32m0.00812\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 483 | loss: 0.00812 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 484  | total loss: \u001B[1m\u001B[32m0.00803\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 484 | loss: 0.00803 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 485  | total loss: \u001B[1m\u001B[32m0.00795\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 485 | loss: 0.00795 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 486  | total loss: \u001B[1m\u001B[32m0.00787\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 486 | loss: 0.00787 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 487  | total loss: \u001B[1m\u001B[32m0.00780\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 487 | loss: 0.00780 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 488  | total loss: \u001B[1m\u001B[32m0.00772\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 488 | loss: 0.00772 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 489  | total loss: \u001B[1m\u001B[32m0.00765\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 489 | loss: 0.00765 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 490  | total loss: \u001B[1m\u001B[32m0.11327\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 490 | loss: 0.11327 - acc: 0.9750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 491  | total loss: \u001B[1m\u001B[32m0.10265\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 491 | loss: 0.10265 - acc: 0.9775 -- iter: 8/8\n",
      "--\n",
      "Training Step: 492  | total loss: \u001B[1m\u001B[32m0.09310\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 492 | loss: 0.09310 - acc: 0.9797 -- iter: 8/8\n",
      "--\n",
      "Training Step: 493  | total loss: \u001B[1m\u001B[32m0.08452\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 493 | loss: 0.08452 - acc: 0.9817 -- iter: 8/8\n",
      "--\n",
      "Training Step: 494  | total loss: \u001B[1m\u001B[32m0.07679\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 494 | loss: 0.07679 - acc: 0.9836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 495  | total loss: \u001B[1m\u001B[32m0.06985\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 495 | loss: 0.06985 - acc: 0.9852 -- iter: 8/8\n",
      "--\n",
      "Training Step: 496  | total loss: \u001B[1m\u001B[32m0.06360\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 496 | loss: 0.06360 - acc: 0.9867 -- iter: 8/8\n",
      "--\n",
      "Training Step: 497  | total loss: \u001B[1m\u001B[32m0.05799\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 497 | loss: 0.05799 - acc: 0.9880 -- iter: 8/8\n",
      "--\n",
      "Training Step: 498  | total loss: \u001B[1m\u001B[32m0.05293\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 498 | loss: 0.05293 - acc: 0.9892 -- iter: 8/8\n",
      "--\n",
      "Training Step: 499  | total loss: \u001B[1m\u001B[32m0.04839\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 499 | loss: 0.04839 - acc: 0.9903 -- iter: 8/8\n",
      "--\n",
      "Training Step: 500  | total loss: \u001B[1m\u001B[32m0.04430\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 500 | loss: 0.04430 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 501  | total loss: \u001B[1m\u001B[32m0.04062\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 501 | loss: 0.04062 - acc: 0.9921 -- iter: 8/8\n",
      "--\n",
      "Training Step: 502  | total loss: \u001B[1m\u001B[32m0.03730\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 502 | loss: 0.03730 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 503  | total loss: \u001B[1m\u001B[32m0.03432\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 503 | loss: 0.03432 - acc: 0.9936 -- iter: 8/8\n",
      "--\n",
      "Training Step: 504  | total loss: \u001B[1m\u001B[32m0.03164\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 504 | loss: 0.03164 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 505  | total loss: \u001B[1m\u001B[32m0.02922\u001B[0m\u001B[0m | time: 0.009s\n",
      "| Adam | epoch: 505 | loss: 0.02922 - acc: 0.9948 -- iter: 8/8\n",
      "--\n",
      "Training Step: 506  | total loss: \u001B[1m\u001B[32m0.24430\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 506 | loss: 0.24430 - acc: 0.9454 -- iter: 8/8\n",
      "--\n",
      "Training Step: 507  | total loss: \u001B[1m\u001B[32m0.22064\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 507 | loss: 0.22064 - acc: 0.9508 -- iter: 8/8\n",
      "--\n",
      "Training Step: 508  | total loss: \u001B[1m\u001B[32m0.19937\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 508 | loss: 0.19937 - acc: 0.9557 -- iter: 8/8\n",
      "--\n",
      "Training Step: 509  | total loss: \u001B[1m\u001B[32m0.18025\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 509 | loss: 0.18025 - acc: 0.9602 -- iter: 8/8\n",
      "--\n",
      "Training Step: 510  | total loss: \u001B[1m\u001B[32m0.16306\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 510 | loss: 0.16306 - acc: 0.9642 -- iter: 8/8\n",
      "--\n",
      "Training Step: 511  | total loss: \u001B[1m\u001B[32m0.14760\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 511 | loss: 0.14760 - acc: 0.9677 -- iter: 8/8\n",
      "--\n",
      "Training Step: 512  | total loss: \u001B[1m\u001B[32m0.13371\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 512 | loss: 0.13371 - acc: 0.9710 -- iter: 8/8\n",
      "--\n",
      "Training Step: 513  | total loss: \u001B[1m\u001B[32m0.12122\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 513 | loss: 0.12122 - acc: 0.9739 -- iter: 8/8\n",
      "--\n",
      "Training Step: 514  | total loss: \u001B[1m\u001B[32m0.10999\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 514 | loss: 0.10999 - acc: 0.9765 -- iter: 8/8\n",
      "--\n",
      "Training Step: 515  | total loss: \u001B[1m\u001B[32m0.09989\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 515 | loss: 0.09989 - acc: 0.9788 -- iter: 8/8\n",
      "--\n",
      "Training Step: 516  | total loss: \u001B[1m\u001B[32m0.09081\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 516 | loss: 0.09081 - acc: 0.9809 -- iter: 8/8\n",
      "--\n",
      "Training Step: 517  | total loss: \u001B[1m\u001B[32m0.08264\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 517 | loss: 0.08264 - acc: 0.9829 -- iter: 8/8\n",
      "--\n",
      "Training Step: 518  | total loss: \u001B[1m\u001B[32m0.07530\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 518 | loss: 0.07530 - acc: 0.9846 -- iter: 8/8\n",
      "--\n",
      "Training Step: 519  | total loss: \u001B[1m\u001B[32m0.06869\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 519 | loss: 0.06869 - acc: 0.9861 -- iter: 8/8\n",
      "--\n",
      "Training Step: 520  | total loss: \u001B[1m\u001B[32m0.06275\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 520 | loss: 0.06275 - acc: 0.9875 -- iter: 8/8\n",
      "--\n",
      "Training Step: 521  | total loss: \u001B[1m\u001B[32m0.05741\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 521 | loss: 0.05741 - acc: 0.9888 -- iter: 8/8\n",
      "--\n",
      "Training Step: 522  | total loss: \u001B[1m\u001B[32m0.05260\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 522 | loss: 0.05260 - acc: 0.9899 -- iter: 8/8\n",
      "--\n",
      "Training Step: 523  | total loss: \u001B[1m\u001B[32m0.04828\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 523 | loss: 0.04828 - acc: 0.9909 -- iter: 8/8\n",
      "--\n",
      "Training Step: 524  | total loss: \u001B[1m\u001B[32m0.04439\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 524 | loss: 0.04439 - acc: 0.9918 -- iter: 8/8\n",
      "--\n",
      "Training Step: 525  | total loss: \u001B[1m\u001B[32m0.04088\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 525 | loss: 0.04088 - acc: 0.9926 -- iter: 8/8\n",
      "--\n",
      "Training Step: 526  | total loss: \u001B[1m\u001B[32m0.16224\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 526 | loss: 0.16224 - acc: 0.9684 -- iter: 8/8\n",
      "--\n",
      "Training Step: 527  | total loss: \u001B[1m\u001B[32m0.14696\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 527 | loss: 0.14696 - acc: 0.9715 -- iter: 8/8\n",
      "--\n",
      "Training Step: 528  | total loss: \u001B[1m\u001B[32m0.13323\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 528 | loss: 0.13323 - acc: 0.9744 -- iter: 8/8\n",
      "--\n",
      "Training Step: 529  | total loss: \u001B[1m\u001B[32m0.12088\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 529 | loss: 0.12088 - acc: 0.9769 -- iter: 8/8\n",
      "--\n",
      "Training Step: 530  | total loss: \u001B[1m\u001B[32m0.10978\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 530 | loss: 0.10978 - acc: 0.9792 -- iter: 8/8\n",
      "--\n",
      "Training Step: 531  | total loss: \u001B[1m\u001B[32m0.09979\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 531 | loss: 0.09979 - acc: 0.9813 -- iter: 8/8\n",
      "--\n",
      "Training Step: 532  | total loss: \u001B[1m\u001B[32m0.09081\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 532 | loss: 0.09081 - acc: 0.9832 -- iter: 8/8\n",
      "--\n",
      "Training Step: 533  | total loss: \u001B[1m\u001B[32m0.08273\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 533 | loss: 0.08273 - acc: 0.9849 -- iter: 8/8\n",
      "--\n",
      "Training Step: 534  | total loss: \u001B[1m\u001B[32m0.07547\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 534 | loss: 0.07547 - acc: 0.9864 -- iter: 8/8\n",
      "--\n",
      "Training Step: 535  | total loss: \u001B[1m\u001B[32m0.06894\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 535 | loss: 0.06894 - acc: 0.9877 -- iter: 8/8\n",
      "--\n",
      "Training Step: 536  | total loss: \u001B[1m\u001B[32m0.06306\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 536 | loss: 0.06306 - acc: 0.9890 -- iter: 8/8\n",
      "--\n",
      "Training Step: 537  | total loss: \u001B[1m\u001B[32m0.05777\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 537 | loss: 0.05777 - acc: 0.9901 -- iter: 8/8\n",
      "--\n",
      "Training Step: 538  | total loss: \u001B[1m\u001B[32m0.05301\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 538 | loss: 0.05301 - acc: 0.9911 -- iter: 8/8\n",
      "--\n",
      "Training Step: 539  | total loss: \u001B[1m\u001B[32m0.04873\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 539 | loss: 0.04873 - acc: 0.9920 -- iter: 8/8\n",
      "--\n",
      "Training Step: 540  | total loss: \u001B[1m\u001B[32m0.04487\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 540 | loss: 0.04487 - acc: 0.9928 -- iter: 8/8\n",
      "--\n",
      "Training Step: 541  | total loss: \u001B[1m\u001B[32m0.04140\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 541 | loss: 0.04140 - acc: 0.9935 -- iter: 8/8\n",
      "--\n",
      "Training Step: 542  | total loss: \u001B[1m\u001B[32m0.03827\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 542 | loss: 0.03827 - acc: 0.9941 -- iter: 8/8\n",
      "--\n",
      "Training Step: 543  | total loss: \u001B[1m\u001B[32m0.03546\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 543 | loss: 0.03546 - acc: 0.9947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 544  | total loss: \u001B[1m\u001B[32m0.03292\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 544 | loss: 0.03292 - acc: 0.9953 -- iter: 8/8\n",
      "--\n",
      "Training Step: 545  | total loss: \u001B[1m\u001B[32m0.03063\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 545 | loss: 0.03063 - acc: 0.9957 -- iter: 8/8\n",
      "--\n",
      "Training Step: 546  | total loss: \u001B[1m\u001B[32m0.02857\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 546 | loss: 0.02857 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 547  | total loss: \u001B[1m\u001B[32m0.02671\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 547 | loss: 0.02671 - acc: 0.9965 -- iter: 8/8\n",
      "--\n",
      "Training Step: 548  | total loss: \u001B[1m\u001B[32m0.02503\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 548 | loss: 0.02503 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 549  | total loss: \u001B[1m\u001B[32m0.02352\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 549 | loss: 0.02352 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 550  | total loss: \u001B[1m\u001B[32m0.02215\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 550 | loss: 0.02215 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 551  | total loss: \u001B[1m\u001B[32m0.02091\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 551 | loss: 0.02091 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 552  | total loss: \u001B[1m\u001B[32m0.01980\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 552 | loss: 0.01980 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 553  | total loss: \u001B[1m\u001B[32m0.01879\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 553 | loss: 0.01879 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 554  | total loss: \u001B[1m\u001B[32m0.01787\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 554 | loss: 0.01787 - acc: 0.9983 -- iter: 8/8\n",
      "--\n",
      "Training Step: 555  | total loss: \u001B[1m\u001B[32m0.01704\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 555 | loss: 0.01704 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 556  | total loss: \u001B[1m\u001B[32m0.01629\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 556 | loss: 0.01629 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 557  | total loss: \u001B[1m\u001B[32m0.01561\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 557 | loss: 0.01561 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 558  | total loss: \u001B[1m\u001B[32m0.01499\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 558 | loss: 0.01499 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 559  | total loss: \u001B[1m\u001B[32m0.01443\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 559 | loss: 0.01443 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 560  | total loss: \u001B[1m\u001B[32m0.01392\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 560 | loss: 0.01392 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 561  | total loss: \u001B[1m\u001B[32m0.01345\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 561 | loss: 0.01345 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 562  | total loss: \u001B[1m\u001B[32m0.01302\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 562 | loss: 0.01302 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 563  | total loss: \u001B[1m\u001B[32m0.01264\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 563 | loss: 0.01264 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 564  | total loss: \u001B[1m\u001B[32m0.01228\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 564 | loss: 0.01228 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 565  | total loss: \u001B[1m\u001B[32m0.01196\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 565 | loss: 0.01196 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 566  | total loss: \u001B[1m\u001B[32m0.01166\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 566 | loss: 0.01166 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 567  | total loss: \u001B[1m\u001B[32m0.01138\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 567 | loss: 0.01138 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 568  | total loss: \u001B[1m\u001B[32m0.01113\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 568 | loss: 0.01113 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 569  | total loss: \u001B[1m\u001B[32m0.01090\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 569 | loss: 0.01090 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 570  | total loss: \u001B[1m\u001B[32m0.01068\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 570 | loss: 0.01068 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 571  | total loss: \u001B[1m\u001B[32m0.01048\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 571 | loss: 0.01048 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 572  | total loss: \u001B[1m\u001B[32m0.01030\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 572 | loss: 0.01030 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 573  | total loss: \u001B[1m\u001B[32m0.01012\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 573 | loss: 0.01012 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 574  | total loss: \u001B[1m\u001B[32m0.00996\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 574 | loss: 0.00996 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 575  | total loss: \u001B[1m\u001B[32m0.00981\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 575 | loss: 0.00981 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 576  | total loss: \u001B[1m\u001B[32m0.00967\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 576 | loss: 0.00967 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 577  | total loss: \u001B[1m\u001B[32m0.00954\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 577 | loss: 0.00954 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 578  | total loss: \u001B[1m\u001B[32m0.00942\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 578 | loss: 0.00942 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 579  | total loss: \u001B[1m\u001B[32m0.00930\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 579 | loss: 0.00930 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 580  | total loss: \u001B[1m\u001B[32m0.00919\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 580 | loss: 0.00919 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 581  | total loss: \u001B[1m\u001B[32m0.00909\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 581 | loss: 0.00909 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 582  | total loss: \u001B[1m\u001B[32m0.00899\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 582 | loss: 0.00899 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 583  | total loss: \u001B[1m\u001B[32m0.00889\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 583 | loss: 0.00889 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 584  | total loss: \u001B[1m\u001B[32m0.00881\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 584 | loss: 0.00881 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 585  | total loss: \u001B[1m\u001B[32m0.00872\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 585 | loss: 0.00872 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 586  | total loss: \u001B[1m\u001B[32m0.00864\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 586 | loss: 0.00864 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 587  | total loss: \u001B[1m\u001B[32m0.00856\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 587 | loss: 0.00856 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 588  | total loss: \u001B[1m\u001B[32m0.00848\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 588 | loss: 0.00848 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 589  | total loss: \u001B[1m\u001B[32m0.00841\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 589 | loss: 0.00841 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 590  | total loss: \u001B[1m\u001B[32m0.00834\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 590 | loss: 0.00834 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 591  | total loss: \u001B[1m\u001B[32m0.00827\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 591 | loss: 0.00827 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 592  | total loss: \u001B[1m\u001B[32m0.00820\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 592 | loss: 0.00820 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 593  | total loss: \u001B[1m\u001B[32m0.00814\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 593 | loss: 0.00814 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 594  | total loss: \u001B[1m\u001B[32m0.00808\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 594 | loss: 0.00808 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 595  | total loss: \u001B[1m\u001B[32m0.00801\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 595 | loss: 0.00801 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 596  | total loss: \u001B[1m\u001B[32m0.00796\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 596 | loss: 0.00796 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 597  | total loss: \u001B[1m\u001B[32m0.00790\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 597 | loss: 0.00790 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 598  | total loss: \u001B[1m\u001B[32m0.00784\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 598 | loss: 0.00784 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 599  | total loss: \u001B[1m\u001B[32m0.00779\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 599 | loss: 0.00779 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 600  | total loss: \u001B[1m\u001B[32m0.00773\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 600 | loss: 0.00773 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 601  | total loss: \u001B[1m\u001B[32m0.00768\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 601 | loss: 0.00768 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 602  | total loss: \u001B[1m\u001B[32m0.00763\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 602 | loss: 0.00763 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 603  | total loss: \u001B[1m\u001B[32m0.00757\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 603 | loss: 0.00757 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 604  | total loss: \u001B[1m\u001B[32m0.00752\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 604 | loss: 0.00752 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 605  | total loss: \u001B[1m\u001B[32m0.00747\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 605 | loss: 0.00747 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 606  | total loss: \u001B[1m\u001B[32m0.00743\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 606 | loss: 0.00743 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 607  | total loss: \u001B[1m\u001B[32m0.00738\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 607 | loss: 0.00738 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 608  | total loss: \u001B[1m\u001B[32m0.27227\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 608 | loss: 0.27227 - acc: 0.9500 -- iter: 8/8\n",
      "--\n",
      "Training Step: 609  | total loss: \u001B[1m\u001B[32m0.24575\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 609 | loss: 0.24575 - acc: 0.9550 -- iter: 8/8\n",
      "--\n",
      "Training Step: 610  | total loss: \u001B[1m\u001B[32m0.22191\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 610 | loss: 0.22191 - acc: 0.9595 -- iter: 8/8\n",
      "--\n",
      "Training Step: 611  | total loss: \u001B[1m\u001B[32m0.20048\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 611 | loss: 0.20048 - acc: 0.9635 -- iter: 8/8\n",
      "--\n",
      "Training Step: 612  | total loss: \u001B[1m\u001B[32m0.29564\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 612 | loss: 0.29564 - acc: 0.9422 -- iter: 8/8\n",
      "--\n",
      "Training Step: 613  | total loss: \u001B[1m\u001B[32m0.26687\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 613 | loss: 0.26687 - acc: 0.9480 -- iter: 8/8\n",
      "--\n",
      "Training Step: 614  | total loss: \u001B[1m\u001B[32m0.24101\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 614 | loss: 0.24101 - acc: 0.9532 -- iter: 8/8\n",
      "--\n",
      "Training Step: 615  | total loss: \u001B[1m\u001B[32m0.21776\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 615 | loss: 0.21776 - acc: 0.9579 -- iter: 8/8\n",
      "--\n",
      "Training Step: 616  | total loss: \u001B[1m\u001B[32m0.19685\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 616 | loss: 0.19685 - acc: 0.9621 -- iter: 8/8\n",
      "--\n",
      "Training Step: 617  | total loss: \u001B[1m\u001B[32m0.17806\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 617 | loss: 0.17806 - acc: 0.9659 -- iter: 8/8\n",
      "--\n",
      "Training Step: 618  | total loss: \u001B[1m\u001B[32m0.16116\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 618 | loss: 0.16116 - acc: 0.9693 -- iter: 8/8\n",
      "--\n",
      "Training Step: 619  | total loss: \u001B[1m\u001B[32m0.14596\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 619 | loss: 0.14596 - acc: 0.9724 -- iter: 8/8\n",
      "--\n",
      "Training Step: 620  | total loss: \u001B[1m\u001B[32m0.13230\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 620 | loss: 0.13230 - acc: 0.9751 -- iter: 8/8\n",
      "--\n",
      "Training Step: 621  | total loss: \u001B[1m\u001B[32m0.12002\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 621 | loss: 0.12002 - acc: 0.9776 -- iter: 8/8\n",
      "--\n",
      "Training Step: 622  | total loss: \u001B[1m\u001B[32m0.09904\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 622 | loss: 0.09904 - acc: 0.9798 -- iter: 8/8\n",
      "--\n",
      "Training Step: 623  | total loss: \u001B[1m\u001B[32m0.09904\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 623 | loss: 0.09904 - acc: 0.9819 -- iter: 8/8\n",
      "--\n",
      "Training Step: 624  | total loss: \u001B[1m\u001B[32m0.09011\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 624 | loss: 0.09011 - acc: 0.9837 -- iter: 8/8\n",
      "--\n",
      "Training Step: 625  | total loss: \u001B[1m\u001B[32m0.08207\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 625 | loss: 0.08207 - acc: 0.9853 -- iter: 8/8\n",
      "--\n",
      "Training Step: 626  | total loss: \u001B[1m\u001B[32m0.07485\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 626 | loss: 0.07485 - acc: 0.9868 -- iter: 8/8\n",
      "--\n",
      "Training Step: 627  | total loss: \u001B[1m\u001B[32m0.06835\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 627 | loss: 0.06835 - acc: 0.9881 -- iter: 8/8\n",
      "--\n",
      "Training Step: 628  | total loss: \u001B[1m\u001B[32m0.06250\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 628 | loss: 0.06250 - acc: 0.9893 -- iter: 8/8\n",
      "--\n",
      "Training Step: 629  | total loss: \u001B[1m\u001B[32m0.05724\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 629 | loss: 0.05724 - acc: 0.9904 -- iter: 8/8\n",
      "--\n",
      "Training Step: 630  | total loss: \u001B[1m\u001B[32m0.05251\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 630 | loss: 0.05251 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 631  | total loss: \u001B[1m\u001B[32m0.04825\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 631 | loss: 0.04825 - acc: 0.9922 -- iter: 8/8\n",
      "--\n",
      "Training Step: 632  | total loss: \u001B[1m\u001B[32m0.04441\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 632 | loss: 0.04441 - acc: 0.9930 -- iter: 8/8\n",
      "--\n",
      "Training Step: 633  | total loss: \u001B[1m\u001B[32m0.04096\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 633 | loss: 0.04096 - acc: 0.9937 -- iter: 8/8\n",
      "--\n",
      "Training Step: 634  | total loss: \u001B[1m\u001B[32m0.03785\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 634 | loss: 0.03785 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 635  | total loss: \u001B[1m\u001B[32m0.03505\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 635 | loss: 0.03505 - acc: 0.9949 -- iter: 8/8\n",
      "--\n",
      "Training Step: 636  | total loss: \u001B[1m\u001B[32m0.03253\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 636 | loss: 0.03253 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 637  | total loss: \u001B[1m\u001B[32m0.03026\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 637 | loss: 0.03026 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 638  | total loss: \u001B[1m\u001B[32m0.02821\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 638 | loss: 0.02821 - acc: 0.9963 -- iter: 8/8\n",
      "--\n",
      "Training Step: 639  | total loss: \u001B[1m\u001B[32m0.02636\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 639 | loss: 0.02636 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 640  | total loss: \u001B[1m\u001B[32m0.02469\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 640 | loss: 0.02469 - acc: 0.9970 -- iter: 8/8\n",
      "--\n",
      "Training Step: 641  | total loss: \u001B[1m\u001B[32m0.02319\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 641 | loss: 0.02319 - acc: 0.9973 -- iter: 8/8\n",
      "--\n",
      "Training Step: 642  | total loss: \u001B[1m\u001B[32m0.02183\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 642 | loss: 0.02183 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 643  | total loss: \u001B[1m\u001B[32m0.02060\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 643 | loss: 0.02060 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 644  | total loss: \u001B[1m\u001B[32m0.01949\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 644 | loss: 0.01949 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 645  | total loss: \u001B[1m\u001B[32m0.01849\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 645 | loss: 0.01849 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 646  | total loss: \u001B[1m\u001B[32m0.01758\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 646 | loss: 0.01758 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 647  | total loss: \u001B[1m\u001B[32m0.01676\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 647 | loss: 0.01676 - acc: 0.9986 -- iter: 8/8\n",
      "--\n",
      "Training Step: 648  | total loss: \u001B[1m\u001B[32m0.01602\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 648 | loss: 0.01602 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 649  | total loss: \u001B[1m\u001B[32m0.01534\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 649 | loss: 0.01534 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 650  | total loss: \u001B[1m\u001B[32m0.01473\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 650 | loss: 0.01473 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 651  | total loss: \u001B[1m\u001B[32m0.01417\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 651 | loss: 0.01417 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 652  | total loss: \u001B[1m\u001B[32m0.01367\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 652 | loss: 0.01367 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 653  | total loss: \u001B[1m\u001B[32m0.01321\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 653 | loss: 0.01321 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 654  | total loss: \u001B[1m\u001B[32m0.01279\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 654 | loss: 0.01279 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 655  | total loss: \u001B[1m\u001B[32m0.01240\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 655 | loss: 0.01240 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 656  | total loss: \u001B[1m\u001B[32m0.01205\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 656 | loss: 0.01205 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 657  | total loss: \u001B[1m\u001B[32m0.01173\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 657 | loss: 0.01173 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 658  | total loss: \u001B[1m\u001B[32m0.01144\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 658 | loss: 0.01144 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 659  | total loss: \u001B[1m\u001B[32m0.01117\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 659 | loss: 0.01117 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 660  | total loss: \u001B[1m\u001B[32m0.13468\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 660 | loss: 0.13468 - acc: 0.9746 -- iter: 8/8\n",
      "--\n",
      "Training Step: 661  | total loss: \u001B[1m\u001B[32m0.12209\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 661 | loss: 0.12209 - acc: 0.9772 -- iter: 8/8\n",
      "--\n",
      "Training Step: 662  | total loss: \u001B[1m\u001B[32m0.11077\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 662 | loss: 0.11077 - acc: 0.9795 -- iter: 8/8\n",
      "--\n",
      "Training Step: 663  | total loss: \u001B[1m\u001B[32m0.10059\u001B[0m\u001B[0m | time: 0.010s\n",
      "| Adam | epoch: 663 | loss: 0.10059 - acc: 0.9815 -- iter: 8/8\n",
      "--\n",
      "Training Step: 664  | total loss: \u001B[1m\u001B[32m0.09144\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 664 | loss: 0.09144 - acc: 0.9834 -- iter: 8/8\n",
      "--\n",
      "Training Step: 665  | total loss: \u001B[1m\u001B[32m0.08320\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 665 | loss: 0.08320 - acc: 0.9850 -- iter: 8/8\n",
      "--\n",
      "Training Step: 666  | total loss: \u001B[1m\u001B[32m0.07579\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 666 | loss: 0.07579 - acc: 0.9865 -- iter: 8/8\n",
      "--\n",
      "Training Step: 667  | total loss: \u001B[1m\u001B[32m0.06913\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 667 | loss: 0.06913 - acc: 0.9879 -- iter: 8/8\n",
      "--\n",
      "Training Step: 668  | total loss: \u001B[1m\u001B[32m0.06313\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 668 | loss: 0.06313 - acc: 0.9891 -- iter: 8/8\n",
      "--\n",
      "Training Step: 669  | total loss: \u001B[1m\u001B[32m0.05774\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 669 | loss: 0.05774 - acc: 0.9902 -- iter: 8/8\n",
      "--\n",
      "Training Step: 670  | total loss: \u001B[1m\u001B[32m0.05289\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 670 | loss: 0.05289 - acc: 0.9912 -- iter: 8/8\n",
      "--\n",
      "Training Step: 671  | total loss: \u001B[1m\u001B[32m0.04852\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 671 | loss: 0.04852 - acc: 0.9920 -- iter: 8/8\n",
      "--\n",
      "Training Step: 672  | total loss: \u001B[1m\u001B[32m0.43499\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 672 | loss: 0.43499 - acc: 0.9178 -- iter: 8/8\n",
      "--\n",
      "Training Step: 673  | total loss: \u001B[1m\u001B[32m0.39246\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 673 | loss: 0.39246 - acc: 0.9261 -- iter: 8/8\n",
      "--\n",
      "Training Step: 674  | total loss: \u001B[1m\u001B[32m0.35422\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 674 | loss: 0.35422 - acc: 0.9334 -- iter: 8/8\n",
      "--\n",
      "Training Step: 675  | total loss: \u001B[1m\u001B[32m0.31984\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 675 | loss: 0.31984 - acc: 0.9401 -- iter: 8/8\n",
      "--\n",
      "Training Step: 676  | total loss: \u001B[1m\u001B[32m0.28893\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 676 | loss: 0.28893 - acc: 0.9461 -- iter: 8/8\n",
      "--\n",
      "Training Step: 677  | total loss: \u001B[1m\u001B[32m0.26114\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 677 | loss: 0.26114 - acc: 0.9515 -- iter: 8/8\n",
      "--\n",
      "Training Step: 678  | total loss: \u001B[1m\u001B[32m0.23616\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 678 | loss: 0.23616 - acc: 0.9563 -- iter: 8/8\n",
      "--\n",
      "Training Step: 679  | total loss: \u001B[1m\u001B[32m0.21370\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 679 | loss: 0.21370 - acc: 0.9607 -- iter: 8/8\n",
      "--\n",
      "Training Step: 680  | total loss: \u001B[1m\u001B[32m0.19352\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 680 | loss: 0.19352 - acc: 0.9646 -- iter: 8/8\n",
      "--\n",
      "Training Step: 681  | total loss: \u001B[1m\u001B[32m0.17537\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 681 | loss: 0.17537 - acc: 0.9682 -- iter: 8/8\n",
      "--\n",
      "Training Step: 682  | total loss: \u001B[1m\u001B[32m0.15905\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 682 | loss: 0.15905 - acc: 0.9714 -- iter: 8/8\n",
      "--\n",
      "Training Step: 683  | total loss: \u001B[1m\u001B[32m0.14438\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 683 | loss: 0.14438 - acc: 0.9742 -- iter: 8/8\n",
      "--\n",
      "Training Step: 684  | total loss: \u001B[1m\u001B[32m0.13118\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 684 | loss: 0.13118 - acc: 0.9768 -- iter: 8/8\n",
      "--\n",
      "Training Step: 685  | total loss: \u001B[1m\u001B[32m0.11932\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 685 | loss: 0.11932 - acc: 0.9791 -- iter: 8/8\n",
      "--\n",
      "Training Step: 686  | total loss: \u001B[1m\u001B[32m0.10866\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 686 | loss: 0.10866 - acc: 0.9812 -- iter: 8/8\n",
      "--\n",
      "Training Step: 687  | total loss: \u001B[1m\u001B[32m0.09906\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 687 | loss: 0.09906 - acc: 0.9831 -- iter: 8/8\n",
      "--\n",
      "Training Step: 688  | total loss: \u001B[1m\u001B[32m0.09044\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 688 | loss: 0.09044 - acc: 0.9848 -- iter: 8/8\n",
      "--\n",
      "Training Step: 689  | total loss: \u001B[1m\u001B[32m0.08267\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 689 | loss: 0.08267 - acc: 0.9863 -- iter: 8/8\n",
      "--\n",
      "Training Step: 690  | total loss: \u001B[1m\u001B[32m0.07569\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 690 | loss: 0.07569 - acc: 0.9877 -- iter: 8/8\n",
      "--\n",
      "Training Step: 691  | total loss: \u001B[1m\u001B[32m0.06941\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 691 | loss: 0.06941 - acc: 0.9889 -- iter: 8/8\n",
      "--\n",
      "Training Step: 692  | total loss: \u001B[1m\u001B[32m0.06376\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 692 | loss: 0.06376 - acc: 0.9900 -- iter: 8/8\n",
      "--\n",
      "Training Step: 693  | total loss: \u001B[1m\u001B[32m0.05867\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 693 | loss: 0.05867 - acc: 0.9910 -- iter: 8/8\n",
      "--\n",
      "Training Step: 694  | total loss: \u001B[1m\u001B[32m0.05409\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 694 | loss: 0.05409 - acc: 0.9919 -- iter: 8/8\n",
      "--\n",
      "Training Step: 695  | total loss: \u001B[1m\u001B[32m0.04997\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 695 | loss: 0.04997 - acc: 0.9927 -- iter: 8/8\n",
      "--\n",
      "Training Step: 696  | total loss: \u001B[1m\u001B[32m0.04625\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 696 | loss: 0.04625 - acc: 0.9934 -- iter: 8/8\n",
      "--\n",
      "Training Step: 697  | total loss: \u001B[1m\u001B[32m0.04290\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 697 | loss: 0.04290 - acc: 0.9941 -- iter: 8/8\n",
      "--\n",
      "Training Step: 698  | total loss: \u001B[1m\u001B[32m0.03989\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 698 | loss: 0.03989 - acc: 0.9947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 699  | total loss: \u001B[1m\u001B[32m0.03717\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 699 | loss: 0.03717 - acc: 0.9952 -- iter: 8/8\n",
      "--\n",
      "Training Step: 700  | total loss: \u001B[1m\u001B[32m0.03472\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 700 | loss: 0.03472 - acc: 0.9957 -- iter: 8/8\n",
      "--\n",
      "Training Step: 701  | total loss: \u001B[1m\u001B[32m0.03251\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 701 | loss: 0.03251 - acc: 0.9961 -- iter: 8/8\n",
      "--\n",
      "Training Step: 702  | total loss: \u001B[1m\u001B[32m0.03051\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 702 | loss: 0.03051 - acc: 0.9965 -- iter: 8/8\n",
      "--\n",
      "Training Step: 703  | total loss: \u001B[1m\u001B[32m0.02871\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 703 | loss: 0.02871 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 704  | total loss: \u001B[1m\u001B[32m0.12564\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 704 | loss: 0.12564 - acc: 0.9722 -- iter: 8/8\n",
      "--\n",
      "Training Step: 705  | total loss: \u001B[1m\u001B[32m0.11433\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 705 | loss: 0.11433 - acc: 0.9750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 706  | total loss: \u001B[1m\u001B[32m0.10416\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 706 | loss: 0.10416 - acc: 0.9775 -- iter: 8/8\n",
      "--\n",
      "Training Step: 707  | total loss: \u001B[1m\u001B[32m0.09501\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 707 | loss: 0.09501 - acc: 0.9797 -- iter: 8/8\n",
      "--\n",
      "Training Step: 708  | total loss: \u001B[1m\u001B[32m0.08678\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 708 | loss: 0.08678 - acc: 0.9817 -- iter: 8/8\n",
      "--\n",
      "Training Step: 709  | total loss: \u001B[1m\u001B[32m0.07938\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 709 | loss: 0.07938 - acc: 0.9836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 710  | total loss: \u001B[1m\u001B[32m0.07272\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 710 | loss: 0.07272 - acc: 0.9852 -- iter: 8/8\n",
      "--\n",
      "Training Step: 711  | total loss: \u001B[1m\u001B[32m0.06673\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 711 | loss: 0.06673 - acc: 0.9867 -- iter: 8/8\n",
      "--\n",
      "Training Step: 712  | total loss: \u001B[1m\u001B[32m0.06134\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 712 | loss: 0.06134 - acc: 0.9880 -- iter: 8/8\n",
      "--\n",
      "Training Step: 713  | total loss: \u001B[1m\u001B[32m0.05649\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 713 | loss: 0.05649 - acc: 0.9892 -- iter: 8/8\n",
      "--\n",
      "Training Step: 714  | total loss: \u001B[1m\u001B[32m0.05212\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 714 | loss: 0.05212 - acc: 0.9903 -- iter: 8/8\n",
      "--\n",
      "Training Step: 715  | total loss: \u001B[1m\u001B[32m0.04818\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 715 | loss: 0.04818 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 716  | total loss: \u001B[1m\u001B[32m0.04464\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 716 | loss: 0.04464 - acc: 0.9921 -- iter: 8/8\n",
      "--\n",
      "Training Step: 717  | total loss: \u001B[1m\u001B[32m0.04145\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 717 | loss: 0.04145 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 718  | total loss: \u001B[1m\u001B[32m0.03857\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 718 | loss: 0.03857 - acc: 0.9936 -- iter: 8/8\n",
      "--\n",
      "Training Step: 719  | total loss: \u001B[1m\u001B[32m0.03597\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 719 | loss: 0.03597 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 720  | total loss: \u001B[1m\u001B[32m0.03363\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 720 | loss: 0.03363 - acc: 0.9948 -- iter: 8/8\n",
      "--\n",
      "Training Step: 721  | total loss: \u001B[1m\u001B[32m0.03152\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 721 | loss: 0.03152 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 722  | total loss: \u001B[1m\u001B[32m0.02962\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 722 | loss: 0.02962 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 723  | total loss: \u001B[1m\u001B[32m0.02790\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 723 | loss: 0.02790 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 724  | total loss: \u001B[1m\u001B[32m0.02634\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 724 | loss: 0.02634 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 725  | total loss: \u001B[1m\u001B[32m0.02494\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 725 | loss: 0.02494 - acc: 0.9970 -- iter: 8/8\n",
      "--\n",
      "Training Step: 726  | total loss: \u001B[1m\u001B[32m0.02367\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 726 | loss: 0.02367 - acc: 0.9973 -- iter: 8/8\n",
      "--\n",
      "Training Step: 727  | total loss: \u001B[1m\u001B[32m0.02251\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 727 | loss: 0.02251 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 728  | total loss: \u001B[1m\u001B[32m0.02147\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 728 | loss: 0.02147 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 729  | total loss: \u001B[1m\u001B[32m0.02053\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 729 | loss: 0.02053 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 730  | total loss: \u001B[1m\u001B[32m0.01967\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 730 | loss: 0.01967 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 731  | total loss: \u001B[1m\u001B[32m0.01889\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 731 | loss: 0.01889 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 732  | total loss: \u001B[1m\u001B[32m0.01818\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 732 | loss: 0.01818 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 733  | total loss: \u001B[1m\u001B[32m0.01754\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 733 | loss: 0.01754 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 734  | total loss: \u001B[1m\u001B[32m0.01695\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 734 | loss: 0.01695 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 735  | total loss: \u001B[1m\u001B[32m0.01641\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 735 | loss: 0.01641 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 736  | total loss: \u001B[1m\u001B[32m0.01592\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 736 | loss: 0.01592 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 737  | total loss: \u001B[1m\u001B[32m0.01547\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 737 | loss: 0.01547 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 738  | total loss: \u001B[1m\u001B[32m0.22425\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 738 | loss: 0.22425 - acc: 0.9492 -- iter: 8/8\n",
      "--\n",
      "Training Step: 739  | total loss: \u001B[1m\u001B[32m0.20299\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 739 | loss: 0.20299 - acc: 0.9543 -- iter: 8/8\n",
      "--\n",
      "Training Step: 740  | total loss: \u001B[1m\u001B[32m0.18388\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 740 | loss: 0.18388 - acc: 0.9589 -- iter: 8/8\n",
      "--\n",
      "Training Step: 741  | total loss: \u001B[1m\u001B[32m0.16669\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 741 | loss: 0.16669 - acc: 0.9630 -- iter: 8/8\n",
      "--\n",
      "Training Step: 742  | total loss: \u001B[1m\u001B[32m0.15125\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 742 | loss: 0.15125 - acc: 0.9667 -- iter: 8/8\n",
      "--\n",
      "Training Step: 743  | total loss: \u001B[1m\u001B[32m0.13736\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 743 | loss: 0.13736 - acc: 0.9700 -- iter: 8/8\n",
      "--\n",
      "Training Step: 744  | total loss: \u001B[1m\u001B[32m0.12487\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 744 | loss: 0.12487 - acc: 0.9730 -- iter: 8/8\n",
      "--\n",
      "Training Step: 745  | total loss: \u001B[1m\u001B[32m0.11365\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 745 | loss: 0.11365 - acc: 0.9757 -- iter: 8/8\n",
      "--\n",
      "Training Step: 746  | total loss: \u001B[1m\u001B[32m0.10355\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 746 | loss: 0.10355 - acc: 0.9781 -- iter: 8/8\n",
      "--\n",
      "Training Step: 747  | total loss: \u001B[1m\u001B[32m0.09448\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 747 | loss: 0.09448 - acc: 0.9803 -- iter: 8/8\n",
      "--\n",
      "Training Step: 748  | total loss: \u001B[1m\u001B[32m0.08631\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 748 | loss: 0.08631 - acc: 0.9823 -- iter: 8/8\n",
      "--\n",
      "Training Step: 749  | total loss: \u001B[1m\u001B[32m0.07897\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 749 | loss: 0.07897 - acc: 0.9841 -- iter: 8/8\n",
      "--\n",
      "Training Step: 750  | total loss: \u001B[1m\u001B[32m0.07236\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 750 | loss: 0.07236 - acc: 0.9857 -- iter: 8/8\n",
      "--\n",
      "Training Step: 751  | total loss: \u001B[1m\u001B[32m0.06642\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 751 | loss: 0.06642 - acc: 0.9871 -- iter: 8/8\n",
      "--\n",
      "Training Step: 752  | total loss: \u001B[1m\u001B[32m0.06107\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 752 | loss: 0.06107 - acc: 0.9884 -- iter: 8/8\n",
      "--\n",
      "Training Step: 753  | total loss: \u001B[1m\u001B[32m0.05626\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 753 | loss: 0.05626 - acc: 0.9895 -- iter: 8/8\n",
      "--\n",
      "Training Step: 754  | total loss: \u001B[1m\u001B[32m0.05192\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 754 | loss: 0.05192 - acc: 0.9906 -- iter: 8/8\n",
      "--\n",
      "Training Step: 755  | total loss: \u001B[1m\u001B[32m0.04802\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 755 | loss: 0.04802 - acc: 0.9915 -- iter: 8/8\n",
      "--\n",
      "Training Step: 756  | total loss: \u001B[1m\u001B[32m0.04451\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 756 | loss: 0.04451 - acc: 0.9924 -- iter: 8/8\n",
      "--\n",
      "Training Step: 757  | total loss: \u001B[1m\u001B[32m0.04134\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 757 | loss: 0.04134 - acc: 0.9931 -- iter: 8/8\n",
      "--\n",
      "Training Step: 758  | total loss: \u001B[1m\u001B[32m0.03849\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 758 | loss: 0.03849 - acc: 0.9938 -- iter: 8/8\n",
      "--\n",
      "Training Step: 759  | total loss: \u001B[1m\u001B[32m0.03592\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 759 | loss: 0.03592 - acc: 0.9944 -- iter: 8/8\n",
      "--\n",
      "Training Step: 760  | total loss: \u001B[1m\u001B[32m0.25747\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 760 | loss: 0.25747 - acc: 0.9450 -- iter: 8/8\n",
      "--\n",
      "Training Step: 761  | total loss: \u001B[1m\u001B[32m0.23302\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 761 | loss: 0.23302 - acc: 0.9505 -- iter: 8/8\n",
      "--\n",
      "Training Step: 762  | total loss: \u001B[1m\u001B[32m0.21105\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 762 | loss: 0.21105 - acc: 0.9554 -- iter: 8/8\n",
      "--\n",
      "Training Step: 763  | total loss: \u001B[1m\u001B[32m0.19129\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 763 | loss: 0.19129 - acc: 0.9599 -- iter: 8/8\n",
      "--\n",
      "Training Step: 764  | total loss: \u001B[1m\u001B[32m0.17353\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 764 | loss: 0.17353 - acc: 0.9639 -- iter: 8/8\n",
      "--\n",
      "Training Step: 765  | total loss: \u001B[1m\u001B[32m0.15757\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 765 | loss: 0.15757 - acc: 0.9675 -- iter: 8/8\n",
      "--\n",
      "Training Step: 766  | total loss: \u001B[1m\u001B[32m0.14321\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 766 | loss: 0.14321 - acc: 0.9708 -- iter: 8/8\n",
      "--\n",
      "Training Step: 767  | total loss: \u001B[1m\u001B[32m0.13031\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 767 | loss: 0.13031 - acc: 0.9737 -- iter: 8/8\n",
      "--\n",
      "Training Step: 768  | total loss: \u001B[1m\u001B[32m0.11870\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 768 | loss: 0.11870 - acc: 0.9763 -- iter: 8/8\n",
      "--\n",
      "Training Step: 769  | total loss: \u001B[1m\u001B[32m0.10827\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 769 | loss: 0.10827 - acc: 0.9787 -- iter: 8/8\n",
      "--\n",
      "Training Step: 770  | total loss: \u001B[1m\u001B[32m0.09888\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 770 | loss: 0.09888 - acc: 0.9808 -- iter: 8/8\n",
      "--\n",
      "Training Step: 771  | total loss: \u001B[1m\u001B[32m0.09044\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 771 | loss: 0.09044 - acc: 0.9827 -- iter: 8/8\n",
      "--\n",
      "Training Step: 772  | total loss: \u001B[1m\u001B[32m0.08284\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 772 | loss: 0.08284 - acc: 0.9845 -- iter: 8/8\n",
      "--\n",
      "Training Step: 773  | total loss: \u001B[1m\u001B[32m0.07601\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 773 | loss: 0.07601 - acc: 0.9860 -- iter: 8/8\n",
      "--\n",
      "Training Step: 774  | total loss: \u001B[1m\u001B[32m0.06986\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 774 | loss: 0.06986 - acc: 0.9874 -- iter: 8/8\n",
      "--\n",
      "Training Step: 775  | total loss: \u001B[1m\u001B[32m0.06432\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 775 | loss: 0.06432 - acc: 0.9887 -- iter: 8/8\n",
      "--\n",
      "Training Step: 776  | total loss: \u001B[1m\u001B[32m0.05934\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 776 | loss: 0.05934 - acc: 0.9898 -- iter: 8/8\n",
      "--\n",
      "Training Step: 777  | total loss: \u001B[1m\u001B[32m0.05485\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 777 | loss: 0.05485 - acc: 0.9908 -- iter: 8/8\n",
      "--\n",
      "Training Step: 778  | total loss: \u001B[1m\u001B[32m0.05081\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 778 | loss: 0.05081 - acc: 0.9917 -- iter: 8/8\n",
      "--\n",
      "Training Step: 779  | total loss: \u001B[1m\u001B[32m0.04717\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 779 | loss: 0.04717 - acc: 0.9926 -- iter: 8/8\n",
      "--\n",
      "Training Step: 780  | total loss: \u001B[1m\u001B[32m0.04389\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 780 | loss: 0.04389 - acc: 0.9933 -- iter: 8/8\n",
      "--\n",
      "Training Step: 781  | total loss: \u001B[1m\u001B[32m0.04093\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 781 | loss: 0.04093 - acc: 0.9940 -- iter: 8/8\n",
      "--\n",
      "Training Step: 782  | total loss: \u001B[1m\u001B[32m0.03826\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 782 | loss: 0.03826 - acc: 0.9946 -- iter: 8/8\n",
      "--\n",
      "Training Step: 783  | total loss: \u001B[1m\u001B[32m0.03585\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 783 | loss: 0.03585 - acc: 0.9951 -- iter: 8/8\n",
      "--\n",
      "Training Step: 784  | total loss: \u001B[1m\u001B[32m0.03368\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 784 | loss: 0.03368 - acc: 0.9956 -- iter: 8/8\n",
      "--\n",
      "Training Step: 785  | total loss: \u001B[1m\u001B[32m0.03172\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 785 | loss: 0.03172 - acc: 0.9961 -- iter: 8/8\n",
      "--\n",
      "Training Step: 786  | total loss: \u001B[1m\u001B[32m0.02994\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 786 | loss: 0.02994 - acc: 0.9964 -- iter: 8/8\n",
      "--\n",
      "Training Step: 787  | total loss: \u001B[1m\u001B[32m0.02834\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 787 | loss: 0.02834 - acc: 0.9968 -- iter: 8/8\n",
      "--\n",
      "Training Step: 788  | total loss: \u001B[1m\u001B[32m0.25675\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 788 | loss: 0.25675 - acc: 0.9471 -- iter: 8/8\n",
      "--\n",
      "Training Step: 789  | total loss: \u001B[1m\u001B[32m0.23249\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 789 | loss: 0.23249 - acc: 0.9524 -- iter: 8/8\n",
      "--\n",
      "Training Step: 790  | total loss: \u001B[1m\u001B[32m0.21068\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 790 | loss: 0.21068 - acc: 0.9572 -- iter: 8/8\n",
      "--\n",
      "Training Step: 791  | total loss: \u001B[1m\u001B[32m0.19107\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 791 | loss: 0.19107 - acc: 0.9615 -- iter: 8/8\n",
      "--\n",
      "Training Step: 792  | total loss: \u001B[1m\u001B[32m0.17345\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 792 | loss: 0.17345 - acc: 0.9653 -- iter: 8/8\n",
      "--\n",
      "Training Step: 793  | total loss: \u001B[1m\u001B[32m0.15761\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 793 | loss: 0.15761 - acc: 0.9688 -- iter: 8/8\n",
      "--\n",
      "Training Step: 794  | total loss: \u001B[1m\u001B[32m0.14336\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 794 | loss: 0.14336 - acc: 0.9719 -- iter: 8/8\n",
      "--\n",
      "Training Step: 795  | total loss: \u001B[1m\u001B[32m0.13056\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 795 | loss: 0.13056 - acc: 0.9747 -- iter: 8/8\n",
      "--\n",
      "Training Step: 796  | total loss: \u001B[1m\u001B[32m0.11904\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 796 | loss: 0.11904 - acc: 0.9772 -- iter: 8/8\n",
      "--\n",
      "Training Step: 797  | total loss: \u001B[1m\u001B[32m0.10869\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 797 | loss: 0.10869 - acc: 0.9795 -- iter: 8/8\n",
      "--\n",
      "Training Step: 798  | total loss: \u001B[1m\u001B[32m0.09937\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 798 | loss: 0.09937 - acc: 0.9816 -- iter: 8/8\n",
      "--\n",
      "Training Step: 799  | total loss: \u001B[1m\u001B[32m0.09100\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 799 | loss: 0.09100 - acc: 0.9834 -- iter: 8/8\n",
      "--\n",
      "Training Step: 800  | total loss: \u001B[1m\u001B[32m0.08346\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 800 | loss: 0.08346 - acc: 0.9851 -- iter: 8/8\n",
      "--\n",
      "Training Step: 801  | total loss: \u001B[1m\u001B[32m0.07668\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 801 | loss: 0.07668 - acc: 0.9866 -- iter: 8/8\n",
      "--\n",
      "Training Step: 802  | total loss: \u001B[1m\u001B[32m0.07057\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 802 | loss: 0.07057 - acc: 0.9879 -- iter: 8/8\n",
      "--\n",
      "Training Step: 803  | total loss: \u001B[1m\u001B[32m0.06508\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 803 | loss: 0.06508 - acc: 0.9891 -- iter: 8/8\n",
      "--\n",
      "Training Step: 804  | total loss: \u001B[1m\u001B[32m0.06013\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 804 | loss: 0.06013 - acc: 0.9902 -- iter: 8/8\n",
      "--\n",
      "Training Step: 805  | total loss: \u001B[1m\u001B[32m0.05568\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 805 | loss: 0.05568 - acc: 0.9912 -- iter: 8/8\n",
      "--\n",
      "Training Step: 806  | total loss: \u001B[1m\u001B[32m0.05167\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 806 | loss: 0.05167 - acc: 0.9921 -- iter: 8/8\n",
      "--\n",
      "Training Step: 807  | total loss: \u001B[1m\u001B[32m0.04805\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 807 | loss: 0.04805 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 808  | total loss: \u001B[1m\u001B[32m0.04479\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 808 | loss: 0.04479 - acc: 0.9936 -- iter: 8/8\n",
      "--\n",
      "Training Step: 809  | total loss: \u001B[1m\u001B[32m0.04185\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 809 | loss: 0.04185 - acc: 0.9942 -- iter: 8/8\n",
      "--\n",
      "Training Step: 810  | total loss: \u001B[1m\u001B[32m0.03920\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 810 | loss: 0.03920 - acc: 0.9948 -- iter: 8/8\n",
      "--\n",
      "Training Step: 811  | total loss: \u001B[1m\u001B[32m0.03680\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 811 | loss: 0.03680 - acc: 0.9953 -- iter: 8/8\n",
      "--\n",
      "Training Step: 812  | total loss: \u001B[1m\u001B[32m0.03464\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 812 | loss: 0.03464 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 813  | total loss: \u001B[1m\u001B[32m0.03268\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 813 | loss: 0.03268 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 814  | total loss: \u001B[1m\u001B[32m0.03092\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 814 | loss: 0.03092 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 815  | total loss: \u001B[1m\u001B[32m0.02932\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 815 | loss: 0.02932 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 816  | total loss: \u001B[1m\u001B[32m0.02787\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 816 | loss: 0.02787 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 817  | total loss: \u001B[1m\u001B[32m0.02656\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 817 | loss: 0.02656 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 818  | total loss: \u001B[1m\u001B[32m0.02537\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 818 | loss: 0.02537 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 819  | total loss: \u001B[1m\u001B[32m0.02429\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 819 | loss: 0.02429 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 820  | total loss: \u001B[1m\u001B[32m0.02331\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 820 | loss: 0.02331 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 821  | total loss: \u001B[1m\u001B[32m0.02241\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 821 | loss: 0.02241 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 822  | total loss: \u001B[1m\u001B[32m0.02160\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 822 | loss: 0.02160 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 823  | total loss: \u001B[1m\u001B[32m0.02086\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 823 | loss: 0.02086 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 824  | total loss: \u001B[1m\u001B[32m0.02018\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 824 | loss: 0.02018 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 825  | total loss: \u001B[1m\u001B[32m0.01956\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 825 | loss: 0.01956 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 826  | total loss: \u001B[1m\u001B[32m0.01900\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 826 | loss: 0.01900 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 827  | total loss: \u001B[1m\u001B[32m0.01848\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 827 | loss: 0.01848 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 828  | total loss: \u001B[1m\u001B[32m0.01800\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 828 | loss: 0.01800 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 829  | total loss: \u001B[1m\u001B[32m0.01756\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 829 | loss: 0.01756 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 830  | total loss: \u001B[1m\u001B[32m0.01716\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 830 | loss: 0.01716 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 831  | total loss: \u001B[1m\u001B[32m0.01679\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 831 | loss: 0.01679 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 832  | total loss: \u001B[1m\u001B[32m0.01644\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 832 | loss: 0.01644 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 833  | total loss: \u001B[1m\u001B[32m0.01612\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 833 | loss: 0.01612 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 834  | total loss: \u001B[1m\u001B[32m0.01582\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 834 | loss: 0.01582 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 835  | total loss: \u001B[1m\u001B[32m0.01554\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 835 | loss: 0.01554 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 836  | total loss: \u001B[1m\u001B[32m0.01528\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 836 | loss: 0.01528 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 837  | total loss: \u001B[1m\u001B[32m0.01504\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 837 | loss: 0.01504 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 838  | total loss: \u001B[1m\u001B[32m0.01482\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 838 | loss: 0.01482 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 839  | total loss: \u001B[1m\u001B[32m0.01460\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 839 | loss: 0.01460 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 840  | total loss: \u001B[1m\u001B[32m0.01440\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 840 | loss: 0.01440 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 841  | total loss: \u001B[1m\u001B[32m0.01421\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 841 | loss: 0.01421 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 842  | total loss: \u001B[1m\u001B[32m0.01403\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 842 | loss: 0.01403 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 843  | total loss: \u001B[1m\u001B[32m0.01386\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 843 | loss: 0.01386 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 844  | total loss: \u001B[1m\u001B[32m0.01370\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 844 | loss: 0.01370 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 845  | total loss: \u001B[1m\u001B[32m0.01355\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 845 | loss: 0.01355 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 846  | total loss: \u001B[1m\u001B[32m0.01340\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 846 | loss: 0.01340 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 847  | total loss: \u001B[1m\u001B[32m0.01326\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 847 | loss: 0.01326 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 848  | total loss: \u001B[1m\u001B[32m0.01312\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 848 | loss: 0.01312 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 849  | total loss: \u001B[1m\u001B[32m0.01299\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 849 | loss: 0.01299 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 850  | total loss: \u001B[1m\u001B[32m0.01287\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 850 | loss: 0.01287 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 851  | total loss: \u001B[1m\u001B[32m0.01275\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 851 | loss: 0.01275 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 852  | total loss: \u001B[1m\u001B[32m0.01263\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 852 | loss: 0.01263 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 853  | total loss: \u001B[1m\u001B[32m0.01252\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 853 | loss: 0.01252 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 854  | total loss: \u001B[1m\u001B[32m0.01241\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 854 | loss: 0.01241 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 855  | total loss: \u001B[1m\u001B[32m0.01230\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 855 | loss: 0.01230 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 856  | total loss: \u001B[1m\u001B[32m0.01220\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 856 | loss: 0.01220 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 857  | total loss: \u001B[1m\u001B[32m0.01210\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 857 | loss: 0.01210 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 858  | total loss: \u001B[1m\u001B[32m0.01200\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 858 | loss: 0.01200 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 859  | total loss: \u001B[1m\u001B[32m0.01191\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 859 | loss: 0.01191 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 860  | total loss: \u001B[1m\u001B[32m0.01181\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 860 | loss: 0.01181 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 861  | total loss: \u001B[1m\u001B[32m0.01172\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 861 | loss: 0.01172 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 862  | total loss: \u001B[1m\u001B[32m0.13962\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 862 | loss: 0.13962 - acc: 0.9750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 863  | total loss: \u001B[1m\u001B[32m0.12676\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 863 | loss: 0.12676 - acc: 0.9775 -- iter: 8/8\n",
      "--\n",
      "Training Step: 864  | total loss: \u001B[1m\u001B[32m0.11518\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 864 | loss: 0.11518 - acc: 0.9797 -- iter: 8/8\n",
      "--\n",
      "Training Step: 865  | total loss: \u001B[1m\u001B[32m0.10478\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 865 | loss: 0.10478 - acc: 0.9818 -- iter: 8/8\n",
      "--\n",
      "Training Step: 866  | total loss: \u001B[1m\u001B[32m0.09542\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 866 | loss: 0.09542 - acc: 0.9836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 867  | total loss: \u001B[1m\u001B[32m0.08700\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 867 | loss: 0.08700 - acc: 0.9852 -- iter: 8/8\n",
      "--\n",
      "Training Step: 868  | total loss: \u001B[1m\u001B[32m0.07943\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 868 | loss: 0.07943 - acc: 0.9867 -- iter: 8/8\n",
      "--\n",
      "Training Step: 869  | total loss: \u001B[1m\u001B[32m0.07262\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 869 | loss: 0.07262 - acc: 0.9880 -- iter: 8/8\n",
      "--\n",
      "Training Step: 870  | total loss: \u001B[1m\u001B[32m0.06649\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 870 | loss: 0.06649 - acc: 0.9892 -- iter: 8/8\n",
      "--\n",
      "Training Step: 871  | total loss: \u001B[1m\u001B[32m0.06098\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 871 | loss: 0.06098 - acc: 0.9903 -- iter: 8/8\n",
      "--\n",
      "Training Step: 872  | total loss: \u001B[1m\u001B[32m0.28816\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 872 | loss: 0.28816 - acc: 0.9413 -- iter: 8/8\n",
      "--\n",
      "Training Step: 873  | total loss: \u001B[1m\u001B[32m0.26052\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 873 | loss: 0.26052 - acc: 0.9471 -- iter: 8/8\n",
      "--\n",
      "Training Step: 874  | total loss: \u001B[1m\u001B[32m0.23567\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 874 | loss: 0.23567 - acc: 0.9524 -- iter: 8/8\n",
      "--\n",
      "Training Step: 875  | total loss: \u001B[1m\u001B[32m0.21333\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 875 | loss: 0.21333 - acc: 0.9572 -- iter: 8/8\n",
      "--\n",
      "Training Step: 876  | total loss: \u001B[1m\u001B[32m0.19325\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 876 | loss: 0.19325 - acc: 0.9615 -- iter: 8/8\n",
      "--\n",
      "Training Step: 877  | total loss: \u001B[1m\u001B[32m0.17520\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 877 | loss: 0.17520 - acc: 0.9653 -- iter: 8/8\n",
      "--\n",
      "Training Step: 878  | total loss: \u001B[1m\u001B[32m0.15897\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 878 | loss: 0.15897 - acc: 0.9688 -- iter: 8/8\n",
      "--\n",
      "Training Step: 879  | total loss: \u001B[1m\u001B[32m0.14438\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 879 | loss: 0.14438 - acc: 0.9719 -- iter: 8/8\n",
      "--\n",
      "Training Step: 880  | total loss: \u001B[1m\u001B[32m0.13127\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 880 | loss: 0.13127 - acc: 0.9747 -- iter: 8/8\n",
      "--\n",
      "Training Step: 881  | total loss: \u001B[1m\u001B[32m0.11948\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 881 | loss: 0.11948 - acc: 0.9772 -- iter: 8/8\n",
      "--\n",
      "Training Step: 882  | total loss: \u001B[1m\u001B[32m0.10888\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 882 | loss: 0.10888 - acc: 0.9795 -- iter: 8/8\n",
      "--\n",
      "Training Step: 883  | total loss: \u001B[1m\u001B[32m0.09935\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 883 | loss: 0.09935 - acc: 0.9816 -- iter: 8/8\n",
      "--\n",
      "Training Step: 884  | total loss: \u001B[1m\u001B[32m0.09077\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 884 | loss: 0.09077 - acc: 0.9834 -- iter: 8/8\n",
      "--\n",
      "Training Step: 885  | total loss: \u001B[1m\u001B[32m0.08306\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 885 | loss: 0.08306 - acc: 0.9851 -- iter: 8/8\n",
      "--\n",
      "Training Step: 886  | total loss: \u001B[1m\u001B[32m0.07612\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 886 | loss: 0.07612 - acc: 0.9866 -- iter: 8/8\n",
      "--\n",
      "Training Step: 887  | total loss: \u001B[1m\u001B[32m0.06988\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 887 | loss: 0.06988 - acc: 0.9879 -- iter: 8/8\n",
      "--\n",
      "Training Step: 888  | total loss: \u001B[1m\u001B[32m0.06427\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 888 | loss: 0.06427 - acc: 0.9891 -- iter: 8/8\n",
      "--\n",
      "Training Step: 889  | total loss: \u001B[1m\u001B[32m0.05921\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 889 | loss: 0.05921 - acc: 0.9902 -- iter: 8/8\n",
      "--\n",
      "Training Step: 890  | total loss: \u001B[1m\u001B[32m0.05466\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 890 | loss: 0.05466 - acc: 0.9912 -- iter: 8/8\n",
      "--\n",
      "Training Step: 891  | total loss: \u001B[1m\u001B[32m0.05056\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 891 | loss: 0.05056 - acc: 0.9921 -- iter: 8/8\n",
      "--\n",
      "Training Step: 892  | total loss: \u001B[1m\u001B[32m0.04687\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 892 | loss: 0.04687 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 893  | total loss: \u001B[1m\u001B[32m0.04355\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 893 | loss: 0.04355 - acc: 0.9936 -- iter: 8/8\n",
      "--\n",
      "Training Step: 894  | total loss: \u001B[1m\u001B[32m0.04055\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 894 | loss: 0.04055 - acc: 0.9942 -- iter: 8/8\n",
      "--\n",
      "Training Step: 895  | total loss: \u001B[1m\u001B[32m0.03785\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 895 | loss: 0.03785 - acc: 0.9948 -- iter: 8/8\n",
      "--\n",
      "Training Step: 896  | total loss: \u001B[1m\u001B[32m0.16198\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 896 | loss: 0.16198 - acc: 0.9703 -- iter: 8/8\n",
      "--\n",
      "Training Step: 897  | total loss: \u001B[1m\u001B[32m0.14714\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 897 | loss: 0.14714 - acc: 0.9733 -- iter: 8/8\n",
      "--\n",
      "Training Step: 898  | total loss: \u001B[1m\u001B[32m0.13379\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 898 | loss: 0.13379 - acc: 0.9760 -- iter: 8/8\n",
      "--\n",
      "Training Step: 899  | total loss: \u001B[1m\u001B[32m0.12179\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 899 | loss: 0.12179 - acc: 0.9784 -- iter: 8/8\n",
      "--\n",
      "Training Step: 900  | total loss: \u001B[1m\u001B[32m0.11099\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 900 | loss: 0.11099 - acc: 0.9805 -- iter: 8/8\n",
      "--\n",
      "Training Step: 901  | total loss: \u001B[1m\u001B[32m0.10128\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 901 | loss: 0.10128 - acc: 0.9825 -- iter: 8/8\n",
      "--\n",
      "Training Step: 902  | total loss: \u001B[1m\u001B[32m0.09254\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 902 | loss: 0.09254 - acc: 0.9842 -- iter: 8/8\n",
      "--\n",
      "Training Step: 903  | total loss: \u001B[1m\u001B[32m0.08468\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 903 | loss: 0.08468 - acc: 0.9858 -- iter: 8/8\n",
      "--\n",
      "Training Step: 904  | total loss: \u001B[1m\u001B[32m0.07760\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 904 | loss: 0.07760 - acc: 0.9872 -- iter: 8/8\n",
      "--\n",
      "Training Step: 905  | total loss: \u001B[1m\u001B[32m0.07123\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 905 | loss: 0.07123 - acc: 0.9885 -- iter: 8/8\n",
      "--\n",
      "Training Step: 906  | total loss: \u001B[1m\u001B[32m0.06550\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 906 | loss: 0.06550 - acc: 0.9896 -- iter: 8/8\n",
      "--\n",
      "Training Step: 907  | total loss: \u001B[1m\u001B[32m0.06033\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 907 | loss: 0.06033 - acc: 0.9907 -- iter: 8/8\n",
      "--\n",
      "Training Step: 908  | total loss: \u001B[1m\u001B[32m0.05568\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 908 | loss: 0.05568 - acc: 0.9916 -- iter: 8/8\n",
      "--\n",
      "Training Step: 909  | total loss: \u001B[1m\u001B[32m0.05150\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 909 | loss: 0.05150 - acc: 0.9925 -- iter: 8/8\n",
      "--\n",
      "Training Step: 910  | total loss: \u001B[1m\u001B[32m0.04772\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 910 | loss: 0.04772 - acc: 0.9932 -- iter: 8/8\n",
      "--\n",
      "Training Step: 911  | total loss: \u001B[1m\u001B[32m0.04432\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 911 | loss: 0.04432 - acc: 0.9939 -- iter: 8/8\n",
      "--\n",
      "Training Step: 912  | total loss: \u001B[1m\u001B[32m0.04125\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 912 | loss: 0.04125 - acc: 0.9945 -- iter: 8/8\n",
      "--\n",
      "Training Step: 913  | total loss: \u001B[1m\u001B[32m0.03849\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 913 | loss: 0.03849 - acc: 0.9950 -- iter: 8/8\n",
      "--\n",
      "Training Step: 914  | total loss: \u001B[1m\u001B[32m0.14774\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 914 | loss: 0.14774 - acc: 0.9705 -- iter: 8/8\n",
      "--\n",
      "Training Step: 915  | total loss: \u001B[1m\u001B[32m0.13433\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 915 | loss: 0.13433 - acc: 0.9735 -- iter: 8/8\n",
      "--\n",
      "Training Step: 916  | total loss: \u001B[1m\u001B[32m0.21995\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 916 | loss: 0.21995 - acc: 0.9511 -- iter: 8/8\n",
      "--\n",
      "Training Step: 917  | total loss: \u001B[1m\u001B[32m0.19935\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 917 | loss: 0.19935 - acc: 0.9560 -- iter: 8/8\n",
      "--\n",
      "Training Step: 918  | total loss: \u001B[1m\u001B[32m0.18084\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 918 | loss: 0.18084 - acc: 0.9604 -- iter: 8/8\n",
      "--\n",
      "Training Step: 919  | total loss: \u001B[1m\u001B[32m0.16419\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 919 | loss: 0.16419 - acc: 0.9644 -- iter: 8/8\n",
      "--\n",
      "Training Step: 920  | total loss: \u001B[1m\u001B[32m0.14923\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 920 | loss: 0.14923 - acc: 0.9679 -- iter: 8/8\n",
      "--\n",
      "Training Step: 921  | total loss: \u001B[1m\u001B[32m0.13578\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 921 | loss: 0.13578 - acc: 0.9711 -- iter: 8/8\n",
      "--\n",
      "Training Step: 922  | total loss: \u001B[1m\u001B[32m0.12368\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 922 | loss: 0.12368 - acc: 0.9740 -- iter: 8/8\n",
      "--\n",
      "Training Step: 923  | total loss: \u001B[1m\u001B[32m0.11280\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 923 | loss: 0.11280 - acc: 0.9766 -- iter: 8/8\n",
      "--\n",
      "Training Step: 924  | total loss: \u001B[1m\u001B[32m0.10301\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 924 | loss: 0.10301 - acc: 0.9790 -- iter: 8/8\n",
      "--\n",
      "Training Step: 925  | total loss: \u001B[1m\u001B[32m0.09421\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 925 | loss: 0.09421 - acc: 0.9811 -- iter: 8/8\n",
      "--\n",
      "Training Step: 926  | total loss: \u001B[1m\u001B[32m0.08629\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 926 | loss: 0.08629 - acc: 0.9830 -- iter: 8/8\n",
      "--\n",
      "Training Step: 927  | total loss: \u001B[1m\u001B[32m0.07917\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 927 | loss: 0.07917 - acc: 0.9847 -- iter: 8/8\n",
      "--\n",
      "Training Step: 928  | total loss: \u001B[1m\u001B[32m0.07276\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 928 | loss: 0.07276 - acc: 0.9862 -- iter: 8/8\n",
      "--\n",
      "Training Step: 929  | total loss: \u001B[1m\u001B[32m0.06699\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 929 | loss: 0.06699 - acc: 0.9876 -- iter: 8/8\n",
      "--\n",
      "Training Step: 930  | total loss: \u001B[1m\u001B[32m0.06179\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 930 | loss: 0.06179 - acc: 0.9888 -- iter: 8/8\n",
      "--\n",
      "Training Step: 931  | total loss: \u001B[1m\u001B[32m0.05712\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 931 | loss: 0.05712 - acc: 0.9899 -- iter: 8/8\n",
      "--\n",
      "Training Step: 932  | total loss: \u001B[1m\u001B[32m0.05290\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 932 | loss: 0.05290 - acc: 0.9909 -- iter: 8/8\n",
      "--\n",
      "Training Step: 933  | total loss: \u001B[1m\u001B[32m0.04911\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 933 | loss: 0.04911 - acc: 0.9919 -- iter: 8/8\n",
      "--\n",
      "Training Step: 934  | total loss: \u001B[1m\u001B[32m0.04568\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 934 | loss: 0.04568 - acc: 0.9927 -- iter: 8/8\n",
      "--\n",
      "Training Step: 935  | total loss: \u001B[1m\u001B[32m0.04260\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 935 | loss: 0.04260 - acc: 0.9934 -- iter: 8/8\n",
      "--\n",
      "Training Step: 936  | total loss: \u001B[1m\u001B[32m0.13721\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 936 | loss: 0.13721 - acc: 0.9691 -- iter: 8/8\n",
      "--\n",
      "Training Step: 937  | total loss: \u001B[1m\u001B[32m0.12497\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 937 | loss: 0.12497 - acc: 0.9722 -- iter: 8/8\n",
      "--\n",
      "Training Step: 938  | total loss: \u001B[1m\u001B[32m0.11397\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 938 | loss: 0.11397 - acc: 0.9749 -- iter: 8/8\n",
      "--\n",
      "Training Step: 939  | total loss: \u001B[1m\u001B[32m0.10408\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 939 | loss: 0.10408 - acc: 0.9774 -- iter: 8/8\n",
      "--\n",
      "Training Step: 940  | total loss: \u001B[1m\u001B[32m0.09518\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 940 | loss: 0.09518 - acc: 0.9797 -- iter: 8/8\n",
      "--\n",
      "Training Step: 941  | total loss: \u001B[1m\u001B[32m0.08718\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 941 | loss: 0.08718 - acc: 0.9817 -- iter: 8/8\n",
      "--\n",
      "Training Step: 942  | total loss: \u001B[1m\u001B[32m0.07997\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 942 | loss: 0.07997 - acc: 0.9836 -- iter: 8/8\n",
      "--\n",
      "Training Step: 943  | total loss: \u001B[1m\u001B[32m0.07349\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 943 | loss: 0.07349 - acc: 0.9852 -- iter: 8/8\n",
      "--\n",
      "Training Step: 944  | total loss: \u001B[1m\u001B[32m0.06766\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 944 | loss: 0.06766 - acc: 0.9867 -- iter: 8/8\n",
      "--\n",
      "Training Step: 945  | total loss: \u001B[1m\u001B[32m0.06240\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 945 | loss: 0.06240 - acc: 0.9880 -- iter: 8/8\n",
      "--\n",
      "Training Step: 946  | total loss: \u001B[1m\u001B[32m0.05767\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 946 | loss: 0.05767 - acc: 0.9892 -- iter: 8/8\n",
      "--\n",
      "Training Step: 947  | total loss: \u001B[1m\u001B[32m0.05341\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 947 | loss: 0.05341 - acc: 0.9903 -- iter: 8/8\n",
      "--\n",
      "Training Step: 948  | total loss: \u001B[1m\u001B[32m0.04958\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 948 | loss: 0.04958 - acc: 0.9913 -- iter: 8/8\n",
      "--\n",
      "Training Step: 949  | total loss: \u001B[1m\u001B[32m0.04612\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 949 | loss: 0.04612 - acc: 0.9921 -- iter: 8/8\n",
      "--\n",
      "Training Step: 950  | total loss: \u001B[1m\u001B[32m0.04300\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 950 | loss: 0.04300 - acc: 0.9929 -- iter: 8/8\n",
      "--\n",
      "Training Step: 951  | total loss: \u001B[1m\u001B[32m0.04018\u001B[0m\u001B[0m | time: 0.006s\n",
      "| Adam | epoch: 951 | loss: 0.04018 - acc: 0.9936 -- iter: 8/8\n",
      "--\n",
      "Training Step: 952  | total loss: \u001B[1m\u001B[32m0.03764\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 952 | loss: 0.03764 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 953  | total loss: \u001B[1m\u001B[32m0.03535\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 953 | loss: 0.03535 - acc: 0.9948 -- iter: 8/8\n",
      "--\n",
      "Training Step: 954  | total loss: \u001B[1m\u001B[32m0.03328\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 954 | loss: 0.03328 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 955  | total loss: \u001B[1m\u001B[32m0.03141\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 955 | loss: 0.03141 - acc: 0.9958 -- iter: 8/8\n",
      "--\n",
      "Training Step: 956  | total loss: \u001B[1m\u001B[32m0.02972\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 956 | loss: 0.02972 - acc: 0.9962 -- iter: 8/8\n",
      "--\n",
      "Training Step: 957  | total loss: \u001B[1m\u001B[32m0.02818\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 957 | loss: 0.02818 - acc: 0.9966 -- iter: 8/8\n",
      "--\n",
      "Training Step: 958  | total loss: \u001B[1m\u001B[32m0.02680\u001B[0m\u001B[0m | time: 0.007s\n",
      "| Adam | epoch: 958 | loss: 0.02680 - acc: 0.9970 -- iter: 8/8\n",
      "--\n",
      "Training Step: 959  | total loss: \u001B[1m\u001B[32m0.02554\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 959 | loss: 0.02554 - acc: 0.9973 -- iter: 8/8\n",
      "--\n",
      "Training Step: 960  | total loss: \u001B[1m\u001B[32m0.02440\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 960 | loss: 0.02440 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 961  | total loss: \u001B[1m\u001B[32m0.02337\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 961 | loss: 0.02337 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 962  | total loss: \u001B[1m\u001B[32m0.02242\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 962 | loss: 0.02242 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 963  | total loss: \u001B[1m\u001B[32m0.02157\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 963 | loss: 0.02157 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 964  | total loss: \u001B[1m\u001B[32m0.02079\u001B[0m\u001B[0m | time: 0.014s\n",
      "| Adam | epoch: 964 | loss: 0.02079 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 965  | total loss: \u001B[1m\u001B[32m0.02008\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 965 | loss: 0.02008 - acc: 0.9985 -- iter: 8/8\n",
      "--\n",
      "Training Step: 966  | total loss: \u001B[1m\u001B[32m0.01943\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 966 | loss: 0.01943 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 967  | total loss: \u001B[1m\u001B[32m0.01883\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 967 | loss: 0.01883 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 968  | total loss: \u001B[1m\u001B[32m0.01829\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 968 | loss: 0.01829 - acc: 0.9989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 969  | total loss: \u001B[1m\u001B[32m0.01779\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 969 | loss: 0.01779 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 970  | total loss: \u001B[1m\u001B[32m0.01733\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 970 | loss: 0.01733 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 971  | total loss: \u001B[1m\u001B[32m0.01691\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 971 | loss: 0.01691 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 972  | total loss: \u001B[1m\u001B[32m0.01652\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 972 | loss: 0.01652 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 973  | total loss: \u001B[1m\u001B[32m0.01617\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 973 | loss: 0.01617 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 974  | total loss: \u001B[1m\u001B[32m0.01583\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 974 | loss: 0.01583 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 975  | total loss: \u001B[1m\u001B[32m0.01553\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 975 | loss: 0.01553 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 976  | total loss: \u001B[1m\u001B[32m0.01524\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 976 | loss: 0.01524 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 977  | total loss: \u001B[1m\u001B[32m0.01497\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 977 | loss: 0.01497 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 978  | total loss: \u001B[1m\u001B[32m0.01472\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 978 | loss: 0.01472 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 979  | total loss: \u001B[1m\u001B[32m0.01449\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 979 | loss: 0.01449 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 980  | total loss: \u001B[1m\u001B[32m0.01427\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 980 | loss: 0.01427 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 981  | total loss: \u001B[1m\u001B[32m0.01407\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 981 | loss: 0.01407 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 982  | total loss: \u001B[1m\u001B[32m0.01388\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 982 | loss: 0.01388 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 983  | total loss: \u001B[1m\u001B[32m0.01369\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 983 | loss: 0.01369 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 984  | total loss: \u001B[1m\u001B[32m0.01352\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 984 | loss: 0.01352 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 985  | total loss: \u001B[1m\u001B[32m0.01336\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 985 | loss: 0.01336 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 986  | total loss: \u001B[1m\u001B[32m0.01320\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 986 | loss: 0.01320 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 987  | total loss: \u001B[1m\u001B[32m0.01305\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 987 | loss: 0.01305 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 988  | total loss: \u001B[1m\u001B[32m0.01291\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 988 | loss: 0.01291 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 989  | total loss: \u001B[1m\u001B[32m0.01278\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 989 | loss: 0.01278 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 990  | total loss: \u001B[1m\u001B[32m0.01265\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 990 | loss: 0.01265 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 991  | total loss: \u001B[1m\u001B[32m0.01252\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 991 | loss: 0.01252 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 992  | total loss: \u001B[1m\u001B[32m0.01240\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 992 | loss: 0.01240 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 993  | total loss: \u001B[1m\u001B[32m0.01229\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 993 | loss: 0.01229 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 994  | total loss: \u001B[1m\u001B[32m0.01217\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 994 | loss: 0.01217 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 995  | total loss: \u001B[1m\u001B[32m0.01207\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 995 | loss: 0.01207 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 996  | total loss: \u001B[1m\u001B[32m0.01196\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 996 | loss: 0.01196 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 997  | total loss: \u001B[1m\u001B[32m0.01186\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 997 | loss: 0.01186 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 998  | total loss: \u001B[1m\u001B[32m0.01176\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 998 | loss: 0.01176 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 999  | total loss: \u001B[1m\u001B[32m0.01166\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 999 | loss: 0.01166 - acc: 1.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001B[1m\u001B[32m0.01157\u001B[0m\u001B[0m | time: 0.004s\n",
      "| Adam | epoch: 1000 | loss: 0.01157 - acc: 1.0000 -- iter: 8/8\n",
      "--\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to rename: C:\\Users\\Daniel Krasovski\\Documents\\GitHub\\Project_Oreo\\models\\model.tflearn.index.tempstate5008535237807874328 to: C:\\Users\\Daniel Krasovski\\Documents\\GitHub\\Project_Oreo\\models\\model.tflearn.index : Access is denied.\r\n; Input/output error\n\t [[node save/SaveV2 (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:134) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/SaveV2:\n val_loss (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:666)\t\n FullyConnected_1/b (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\variables.py:57)\t\n FullyConnected_2/b/Adam_1 (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:713)\t\n Global_Step (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:102)\t\n Training_step (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:621)\t\n Accuracy/Mean/moving_avg (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:686)\t\n Crossentropy/Mean/moving_avg (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\summaries.py:244)\t\n val_acc (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:667)\n\nOriginal stack trace for 'save/SaveV2':\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-ad4d62996ec2>\", line 11, in <module>\n    model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\models\\dnn.py\", line 57, in __init__\n    self.trainer = Trainer(self.train_ops,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py\", line 134, in __init__\n    self.saver = tf.train.Saver(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 836, in __init__\n    self.build()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 848, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 876, in _build\n    self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 513, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 206, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 121, in save_op\n    return io_ops.save_v2(filename_tensor, tensor_names, tensor_slices,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1734, in save_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnknownError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1364\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1365\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1366\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOpError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run_fn\u001B[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[0;32m   1348\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_extend_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1349\u001B[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001B[0m\u001B[0;32m   1350\u001B[0m                                       target_list, run_metadata)\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_call_tf_sessionrun\u001B[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[0;32m   1440\u001B[0m                           run_metadata):\n\u001B[1;32m-> 1441\u001B[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001B[0m\u001B[0;32m   1442\u001B[0m                                             \u001B[0mfetch_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_list\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnknownError\u001B[0m: Failed to rename: C:\\Users\\Daniel Krasovski\\Documents\\GitHub\\Project_Oreo\\models\\model.tflearn.index.tempstate5008535237807874328 to: C:\\Users\\Daniel Krasovski\\Documents\\GitHub\\Project_Oreo\\models\\model.tflearn.index : Access is denied.\r\n; Input/output error\n\t [[{{node save/SaveV2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mUnknownError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-ad4d62996ec2>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;31m# Start training (apply gradient descent algorithm)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_x\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_y\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_epoch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1000\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m8\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshow_metric\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'models/model.tflearn'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\models\\dnn.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self, model_file)\u001B[0m\n\u001B[0;32m    282\u001B[0m         \"\"\"\n\u001B[0;32m    283\u001B[0m         \u001B[1;31m#with self.graph.as_default():\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 284\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    285\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    286\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweights_only\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0moptargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self, model_file, global_step, use_val_saver)\u001B[0m\n\u001B[0;32m    422\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mval_saver\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msession\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_step\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mglobal_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    423\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 424\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaver\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msession\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_step\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mglobal_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    425\u001B[0m         \u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfix_saver\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj_lists\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    426\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001B[0m\n\u001B[0;32m   1181\u001B[0m           \u001B[0mmodel_checkpoint_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaver_def\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_tensor_name\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1182\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1183\u001B[1;33m           model_checkpoint_path = sess.run(\n\u001B[0m\u001B[0;32m   1184\u001B[0m               \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaver_def\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_tensor_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1185\u001B[0m               {self.saver_def.filename_tensor_name: checkpoint_file})\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    955\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    956\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 957\u001B[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[0m\u001B[0;32m    958\u001B[0m                          run_metadata_ptr)\n\u001B[0;32m    959\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1178\u001B[0m     \u001B[1;31m# or if the call is a partial run that specifies feeds.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1179\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfinal_fetches\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mfinal_targets\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mfeed_dict_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1180\u001B[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001B[0m\u001B[0;32m   1181\u001B[0m                              feed_dict_tensor, options, run_metadata)\n\u001B[0;32m   1182\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_run\u001B[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1356\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1357\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhandle\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1358\u001B[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001B[0m\u001B[0;32m   1359\u001B[0m                            run_metadata)\n\u001B[0;32m   1360\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1382\u001B[0m                     \u001B[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1383\u001B[0m                     'disable_meta_optimizer = True')\n\u001B[1;32m-> 1384\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnode_def\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1385\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1386\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_extend_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnknownError\u001B[0m: Failed to rename: C:\\Users\\Daniel Krasovski\\Documents\\GitHub\\Project_Oreo\\models\\model.tflearn.index.tempstate5008535237807874328 to: C:\\Users\\Daniel Krasovski\\Documents\\GitHub\\Project_Oreo\\models\\model.tflearn.index : Access is denied.\r\n; Input/output error\n\t [[node save/SaveV2 (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:134) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/SaveV2:\n val_loss (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:666)\t\n FullyConnected_1/b (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\variables.py:57)\t\n FullyConnected_2/b/Adam_1 (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:713)\t\n Global_Step (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:102)\t\n Training_step (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:621)\t\n Accuracy/Mean/moving_avg (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:686)\t\n Crossentropy/Mean/moving_avg (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\summaries.py:244)\t\n val_acc (defined at C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py:667)\n\nOriginal stack trace for 'save/SaveV2':\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-ad4d62996ec2>\", line 11, in <module>\n    model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\models\\dnn.py\", line 57, in __init__\n    self.trainer = Trainer(self.train_ops,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tflearn\\helpers\\trainer.py\", line 134, in __init__\n    self.saver = tf.train.Saver(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 836, in __init__\n    self.build()\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 848, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 876, in _build\n    self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 513, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 206, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 121, in save_op\n    return io_ops.save_v2(filename_tensor, tensor_names, tensor_slices,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1734, in save_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"C:\\Users\\Daniel Krasovski\\anaconda3\\envs\\Project_Oreo\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('models/model.tflearn')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save all of our data structures\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}